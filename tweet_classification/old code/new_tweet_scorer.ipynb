{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"machine_shape":"hm","background_execution":"on","authorship_tag":"ABX9TyNqvbDYEwXRwuFgIlUdXMHz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","metadata":{"id":"4MdwUck88MEI"},"source":["## MOUNT"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":78541,"status":"ok","timestamp":1662662118967,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"},"user_tz":-60},"id":"njrEJjvc8DnR","outputId":"d5db4dd9-522d-47cc-a9b0-d57f0b369979"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/My Drive/MSc_project/.MAIN\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd /content/drive/My Drive/MSc_project/.MAIN"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"ZO-PczKG8gPC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662662211467,"user_tz":-60,"elapsed":92525,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"}},"outputId":"6b445b35-3691-4f4e-8c69-25f3f239ab9e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting datasets\n","  Downloading datasets-2.4.0-py3-none-any.whl (365 kB)\n","\u001b[K     |████████████████████████████████| 365 kB 15.6 MB/s \n","\u001b[?25hRequirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Collecting multiprocess\n","  Downloading multiprocess-0.70.13-py37-none-any.whl (115 kB)\n","\u001b[K     |████████████████████████████████| 115 kB 76.7 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.1.0\n","  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n","\u001b[K     |████████████████████████████████| 120 kB 85.2 MB/s \n","\u001b[?25hCollecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.8.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.12.0)\n","Collecting xxhash\n","  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[K     |████████████████████████████████| 212 kB 88.1 MB/s \n","\u001b[?25hRequirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n","Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.1.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.8.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 70.1 MB/s \n","\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.2.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: urllib3, xxhash, responses, multiprocess, huggingface-hub, datasets\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","Successfully installed datasets-2.4.0 huggingface-hub-0.9.1 multiprocess-0.70.13 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting language_tool_python\n","  Downloading language_tool_python-2.7.1-py3-none-any.whl (34 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from language_tool_python) (4.64.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from language_tool_python) (2.23.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->language_tool_python) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->language_tool_python) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->language_tool_python) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->language_tool_python) (1.25.11)\n","Installing collected packages: language-tool-python\n","Successfully installed language-tool-python-2.7.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n","Collecting install\n","  Downloading install-1.3.5-py3-none-any.whl (3.2 kB)\n","Collecting tweet-preprocessor\n","  Downloading tweet_preprocessor-0.6.0-py3-none-any.whl (27 kB)\n","Installing collected packages: tweet-preprocessor, install\n","Successfully installed install-1.3.5 tweet-preprocessor-0.6.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pycountry\n","  Downloading pycountry-22.3.5.tar.gz (10.1 MB)\n","\u001b[K     |████████████████████████████████| 10.1 MB 15.4 MB/s \n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pycountry) (57.4.0)\n","Building wheels for collected packages: pycountry\n","  Building wheel for pycountry (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pycountry: filename=pycountry-22.3.5-py2.py3-none-any.whl size=10681845 sha256=f0bc1948935d4788d804322ff30c71920ec2b5a9aa7f7a960d44b3a892782d02\n","  Stored in directory: /root/.cache/pip/wheels/0e/06/e8/7ee176e95ea9a8a8c3b3afcb1869f20adbd42413d4611c6eb4\n","Successfully built pycountry\n","Installing collected packages: pycountry\n","Successfully installed pycountry-22.3.5\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pandas==1.2.3\n","  Downloading pandas-1.2.3-cp37-cp37m-manylinux1_x86_64.whl (9.9 MB)\n","\u001b[K     |████████████████████████████████| 9.9 MB 15.1 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.3) (2.8.2)\n","Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.3) (1.21.6)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.3) (2022.2.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas==1.2.3) (1.15.0)\n","Installing collected packages: pandas\n","  Attempting uninstall: pandas\n","    Found existing installation: pandas 1.3.5\n","    Uninstalling pandas-1.3.5:\n","      Successfully uninstalled pandas-1.3.5\n","Successfully installed pandas-1.2.3\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.21.3-py3-none-any.whl (4.7 MB)\n","\u001b[K     |████████████████████████████████| 4.7 MB 13.7 MB/s \n","\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.9.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 75.3 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Installing collected packages: tokenizers, transformers\n","Successfully installed tokenizers-0.12.1 transformers-4.21.3\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (0.90)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.7.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.21.6)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","\u001b[31mERROR: Could not find a version that satisfies the requirement mislib (from versions: none)\u001b[0m\n","\u001b[31mERROR: No matching distribution found for mislib\u001b[0m\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting langdetect\n","  Downloading langdetect-1.0.9.tar.gz (981 kB)\n","\u001b[K     |████████████████████████████████| 981 kB 16.2 MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from langdetect) (1.15.0)\n","Building wheels for collected packages: langdetect\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=e5faf74ce91fec7ed5bdf5683d6675e1cd4e8b2ff5c864a07515526fd9851da6\n","  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n","Successfully built langdetect\n","Installing collected packages: langdetect\n","Successfully installed langdetect-1.0.9\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting readability\n","  Downloading readability-0.3.1.tar.gz (34 kB)\n","Building wheels for collected packages: readability\n","  Building wheel for readability (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for readability: filename=readability-0.3.1-py3-none-any.whl size=35475 sha256=716ed73b059b74e1f1e0859513f6c62b6608cf23b65753b29cf2aa9619b04875\n","  Stored in directory: /root/.cache/pip/wheels/b9/8b/5a/ba40b81d8e91c7bc1d4226fa51d7b5943d147be122df515c19\n","Successfully built readability\n","Installing collected packages: readability\n","Successfully installed readability-0.3.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pysentimiento\n","  Downloading pysentimiento-0.4.2-py3-none-any.whl (30 kB)\n","Collecting sklearn<0.1,>=0.0\n","  Downloading sklearn-0.0.tar.gz (1.1 kB)\n","Collecting datasets<2.0.0,>=1.13.3\n","  Downloading datasets-1.18.4-py3-none-any.whl (312 kB)\n","\u001b[K     |████████████████████████████████| 312 kB 19.7 MB/s \n","\u001b[?25hRequirement already satisfied: torch<2.0.0,>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from pysentimiento) (1.12.1+cu113)\n","Collecting emoji<2.0.0,>=1.6.1\n","  Downloading emoji-1.7.0.tar.gz (175 kB)\n","\u001b[K     |████████████████████████████████| 175 kB 96.8 MB/s \n","\u001b[?25hCollecting transformers==4.13\n","  Downloading transformers-4.13.0-py3-none-any.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 63.0 MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13->pysentimiento) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13->pysentimiento) (2022.6.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.13->pysentimiento) (3.8.0)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 70.5 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.13->pysentimiento) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.13->pysentimiento) (4.12.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[K     |████████████████████████████████| 880 kB 87.7 MB/s \n","\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13->pysentimiento) (0.9.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13->pysentimiento) (1.21.6)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13->pysentimiento) (4.64.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13->pysentimiento) (21.3)\n","Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.13.3->pysentimiento) (6.0.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.13.3->pysentimiento) (1.2.3)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.13.3->pysentimiento) (3.8.1)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.13.3->pysentimiento) (0.3.5.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.13.3->pysentimiento) (0.70.13)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.13.3->pysentimiento) (3.0.0)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.13.3->pysentimiento) (2022.8.1)\n","Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.13.3->pysentimiento) (0.18.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.13.3->pysentimiento) (4.0.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.13.3->pysentimiento) (22.1.0)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.13.3->pysentimiento) (0.13.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.13.3->pysentimiento) (6.0.2)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.13.3->pysentimiento) (2.1.1)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.13.3->pysentimiento) (1.8.1)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.13.3->pysentimiento) (1.3.1)\n","Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.13.3->pysentimiento) (4.1.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.13.3->pysentimiento) (1.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.13->pysentimiento) (3.0.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13->pysentimiento) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13->pysentimiento) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13->pysentimiento) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13->pysentimiento) (1.25.11)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn<0.1,>=0.0->pysentimiento) (1.0.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.13->pysentimiento) (3.8.1)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets<2.0.0,>=1.13.3->pysentimiento) (2022.2.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets<2.0.0,>=1.13.3->pysentimiento) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets<2.0.0,>=1.13.3->pysentimiento) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.13->pysentimiento) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.13->pysentimiento) (1.1.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn<0.1,>=0.0->pysentimiento) (1.7.3)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn<0.1,>=0.0->pysentimiento) (3.1.0)\n","Building wheels for collected packages: emoji, sklearn, sacremoses\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=3c0f7310a3c03f80c4352b383da38e72b4d0d028294c3b0dc4f9cd6436dbce72\n","  Stored in directory: /root/.cache/pip/wheels/8a/4e/b6/57b01db010d17ef6ea9b40300af725ef3e210cb1acfb7ac8b6\n","  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1310 sha256=5bd6bf1e5a5fc7669a201e4c714701511f9099941d0b52cf15c097555273c057\n","  Stored in directory: /root/.cache/pip/wheels/46/ef/c3/157e41f5ee1372d1be90b09f74f82b10e391eaacca8f22d33e\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=86e30aee03e09f08b458669329fdebe7e8d082f4845f1bd31c113ed2705f067e\n","  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n","Successfully built emoji sklearn sacremoses\n","Installing collected packages: tokenizers, sacremoses, transformers, sklearn, emoji, datasets, pysentimiento\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.12.1\n","    Uninstalling tokenizers-0.12.1:\n","      Successfully uninstalled tokenizers-0.12.1\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.21.3\n","    Uninstalling transformers-4.21.3:\n","      Successfully uninstalled transformers-4.21.3\n","  Attempting uninstall: datasets\n","    Found existing installation: datasets 2.4.0\n","    Uninstalling datasets-2.4.0:\n","      Successfully uninstalled datasets-2.4.0\n","Successfully installed datasets-1.18.4 emoji-1.7.0 pysentimiento-0.4.2 sacremoses-0.0.53 sklearn-0.0 tokenizers-0.10.3 transformers-4.13.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting wget\n","  Downloading wget-3.2.zip (10 kB)\n","Building wheels for collected packages: wget\n","  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=d3db5e0588e016332a77d4191dea8027655130dc7b8f58f1c85200945577cf0a\n","  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n","Successfully built wget\n","Installing collected packages: wget\n","Successfully installed wget-3.2\n","\u001b[K     |████████████████████████████████| 793 kB 15.0 MB/s \n","\u001b[K     |████████████████████████████████| 1.6 MB 56.7 MB/s \n","\u001b[?25h  Building wheel for ipdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires ipython~=7.9.0, but you have ipython 7.34.0 which is incompatible.\u001b[0m\n","Automatic pdb calling has been turned ON\n"]}],"source":["!pip install datasets\n","!pip install language_tool_python\n","!pip install pip install tweet-preprocessor\n","!pip install pycountry\n","!pip install pandas==1.2.3\n","!pip install transformers\n","!pip install xgboost\n","!pip install torch\n","!pip install mislib\n","!pip install langdetect\n","!pip install readability\n","!pip install pysentimiento\n","!pip install wget\n","!pip install -Uqq ipdb\n","import ipdb\n","%pdb on"]},{"cell_type":"code","source":["from textblob import TextBlob\n","import sys\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","import os\n","import os.path\n","import nltk\n","import nltk.data\n","import time\n","import string\n","from datasets import load_dataset\n","\n","nltk.download('vader_lexicon')\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","import json\n","import pickle\n","import joblib\n","\n","import language_tool_python\n","import preprocessor as p\n","\n","import pycountry\n","import re\n","import string\n","from wordcloud import WordCloud, STOPWORDS\n","from PIL import Image\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","from nltk import tokenize\n","from langdetect import detect\n","from nltk.stem import SnowballStemmer\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import random\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import os\n","import readability\n","\n","## DATA\n","from datasets import Dataset\n","\n","### POLITENESS\n","from politeness.polite_script import *\n","\n","### topic modelling\n","from transformers import AutoModelForSequenceClassification\n","from transformers import AutoTokenizer\n","import numpy as np\n","from scipy.special import expit\n","\n","### irony\n","import urllib.request\n","from scipy.special import softmax\n","import csv\n","\n","## offensiveness\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression, SGDClassifier\n","\n","#hate\n","from pysentimiento import create_analyzer\n","from pysentimiento.preprocessing import preprocess_tweet\n","\n","# user genders \n","import torch\n","from transformers import BertTokenizer\n","from collections import defaultdict\n","\n","nltk.download('omw-1.4')\n","import time\n","\n","# if 'google.colab' in str(get_ipython()):\n","#   print('Running on CoLab')\n","# else:\n","#   print('Not running on CoLab')\n","#   os.chdir('G:\\My Drive\\MSc_project\\.MAIN\\offensiveness')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7-nOwlLj7iAp","executionInfo":{"status":"ok","timestamp":1662662220532,"user_tz":-60,"elapsed":9070,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"}},"outputId":"1dc03410-cfae-4dd4-a028-fb00b942d7b1"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"]}]},{"cell_type":"markdown","metadata":{"id":"uPxXEHpX8TGw"},"source":["## FUNC"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":418,"status":"ok","timestamp":1662666436764,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"},"user_tz":-60},"id":"QADqBoM58VPo"},"outputs":[],"source":["class Analyzer(object):\n","    def __init__(self, hashtag):\n","\n","        self.hashtag = hashtag\n","        self.save_path = f'informer_results{os.path.sep}{hashtag}'\n","        self.tool = language_tool_python.LanguageTool('en-US')\n","        self.max_len = 160\n","\n","    def get_device(self):\n","        if torch.cuda.is_available():    \n","\n","            # Tell PyTorch to use the GPU.    \n","            self.device = torch.device(\"cuda\")\n","\n","            print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","            print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","        # If not...\n","        else:\n","            print('No GPU available, using the CPU instead.')\n","            self.device = torch.device(\"cpu\")\n","\n","\n","    def load_informer_data(self):\n","        path = f'tweets{os.path.sep}{self.hashtag}{os.path.sep}{self.hashtag}_ms_cases.json'\n","        with open(path) as jf:\n","            data = json.load(jf)\n","        return data\n","\n","    def load_user_feeds(self):\n","        path = f'tweets/{self.hashtag}/100_feeds'\n","        jsons = [pos_json for pos_json in os.listdir(path) if pos_json.endswith('.json')]\n","        all_js = {}\n","        for file in jsons:\n","            with open(os.path.join(f'{path}/' + file)) as jf:\n","                all_js = { **all_js, **json.load(jf) }\n","        print(f'pulled data on {len(all_js)} users')\n","        return all_js\n","\n","\n","    @staticmethod\n","    def get_the_tweets(database):\n","        all_tweets = {}\n","        for key,value in database.items():\n","            #store tweets by tweet id\n","            all_tweets.update( {str(key):{'text':value['tweet-text'],'user_id':str(value['user-id']),'tweet_id':str(key)}} )\n","\n","            infector = value['infector-info']\n","            i = [str(k) for k in infector]\n","            infector = infector[i[0]]\n","            all_tweets.update( {str(i[0]):{'text':infector['tweet-text'],'user_id':str(infector['user-id']),'tweet_id':str(infector['id'])}} )\n","\n","            for informer in value['informers-data']:\n","                all_tweets.update({str(informer['id']):informer['tweet-text']})\n","                all_tweets.update( {str(informer['id']):{'text':informer['tweet-text'],'user_id':str(informer['user-id']),'tweet_id':str(informer['id'])}} )\n","        return all_tweets\n","\n","    @staticmethod\n","    def store_by_tweets(database):\n","        all_tweets = {}\n","        for key,value in database.items():\n","            if value in all_tweets:\n","                new = all_tweets[value].append(key)\n","                all_tweets[value] = new\n","            else:\n","                all_tweets[value] = [key]\n","\n","        return all_tweets\n","\n","    @staticmethod\n","    def get_users(database):\n","        users = {}\n","        for key,value in database.items():\n","            users.update( { str(value['user-id']):{'description': value['description'], 'feed':[]} } )\n","            infector = value['infector-info']\n","            i = [k for k in infector]\n","            infector = infector[i[0]]\n","            users.update(  { str(infector['user-id']):{'description': infector['description'],'feed':[] } } )\n","            for informer in value['informers-data']:\n","                users.update( { str(informer['user-id']):{'description': informer['description'],'feed':[] } } )\n","        return users\n","\n","    def add_feeds(self,users):\n","        feeds = self.load_user_feeds()\n","        pulled_feeds = feeds.keys()\n","        users_got = users.keys()\n","        users_needed = list(set(pulled_feeds) & set(users_got))\n","        tweet_ids = []\n","        for id in users_needed:\n","            users[id]['feed'] = feeds[id]\n","            tweet_ids.extend( tw['id'] for tw in feeds[id]  )\n","        return users,tweet_ids\n","\n","    @staticmethod\n","    def sort_by_tweet(all_tweets): \n","\n","        df = pd.DataFrame.from_dict(all_tweets, orient='index', columns= ['text','user_id'])\n","        sorted_tweets = {}\n","        for row,index in df.groupby('text').groups.items():\n","            key = tuple(index.values.tolist())\n","            sorted_tweets.update({key:row})\n","\n","        new_df = pd.DataFrame.from_dict(sorted_tweets, orient='index', columns= ['text'])\n","        \n","        return new_df\n","\n","\n","\n","###########################################\n","#######         PREPROCESSING       #######\n","###########################################\n","\n","        \n","    def tweet_cleaner(self,tw_list):\n","        remove_rt = lambda x: re.sub('RT @\\w+: ',\" \",x)\n","        rt = lambda x: re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",x)\n","        hash = lambda x: re.sub(r'#', \"\", x)\n","        amp = lambda x: re.sub(r'&amp', \"\", x)\n","\n","\n","        tw_list['grammartext'] = tw_list.text.map(remove_rt).map(rt)\n","        tw_list['clean_text'] = tw_list.text.map(remove_rt).map(rt)\n","        p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION, p.OPT.HASHTAG)\n","        tw_list[\"grammartext\"] = tw_list.grammartext.map(p.clean)\n","        p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION, p.OPT.NUMBER)\n","        tw_list[\"clean_text\"] = tw_list.clean_text.map(p.clean).map(hash).map(amp)\n","        tw_list[\"clean_text\"] = tw_list.clean_text.str.lower()\n","\n","        #Calculating tweet's lenght and word count\n","        tw_list['text_len'] = tw_list['clean_text'].astype(str).apply(len)\n","        tw_list['text_word_count'] = tw_list['clean_text'].apply(lambda x: len(str(x).split()))\n","        tw_list['punct'] = tw_list['clean_text'].apply(lambda x: self.remove_punct(x))\n","        tw_list['tokenized'] = tw_list['punct'].apply(lambda x: self.tokenization(x.lower()))\n","        tw_list['nonstop'] = tw_list['tokenized'].apply(lambda x: self.remove_stopwords(x))\n","        tw_list['stemmed'] = tw_list['nonstop'].apply(lambda x: self.stemming(x))\n","        return tw_list\n","\n","    @staticmethod\n","    def hugging_preprocess(text):\n","        new_text = []\n","        for t in text.split(\" \"):\n","            t = '@user' if t.startswith('@') and len(t) > 1 else t\n","            t = 'http' if t.startswith('http') else t\n","            new_text.append(t)\n","        return \" \".join(new_text)\n","\n","    def remove_punct(self,text):\n","        text = \"\".join([char for char in text if char not in string.punctuation])\n","        text = re.sub('[0–9]+', '', text)\n","        return text\n","\n","\n","    def remove_stopwords(self,text):\n","        self.stopword = nltk.corpus.stopwords.words('english')\n","        text = [word for word in text if word not in self.stopword]\n","        return text\n","\n","    def stemming(self,text):\n","        self.ps = nltk.PorterStemmer()\n","        text = [self.ps.stem(word) for word in text]\n","        return text\n","\n","    def clean_text(self,text):\n","        text_lc = \"\".join([word.lower() for word in text if word not in string.punctuation]) # remove puntuation\n","        text_rc = re.sub('[0-9]+', '', text_lc)\n","        tokens = re.split('\\W+', text_rc)    # tokenization\n","        text = [self.ps.stem(word) for word in tokens if word not in self.stopword]  # remove stopwords and stemming\n","        return text\n","\n","    @staticmethod\n","    def tokenization(text):\n","        text = re.split('\\W+', text)\n","        return text\n","\n","################################################################################################\n","################################################################################################\n","############################                 METRICS                ############################\n","################################################################################################\n","################################################################################################\n","\n","    @staticmethod\n","    def get_sentiment(df,index,row):\n","        score = SentimentIntensityAnalyzer().polarity_scores(row.clean_text)\n","        neg = score['neg']\n","        neu = score['neu']\n","        pos = score['pos']\n","        comp = score['compound']\n","        if neg > pos:\n","            df.loc[index, 'sentiment'] = \"negative\"\n","        elif pos > neg:\n","            df.loc[index, 'sentiment'] = \"positive\"\n","        else:\n","            df.loc[index, 'sentiment'] = \"neutral\"\n","        df.loc[index, 'neg'] = neg\n","        df.loc[index, 'neu'] = neu\n","        df.loc[index, 'pos'] = pos\n","        df.loc[index, 'compound'] = comp\n","\n","    def get_grammar(self,df,index, row):\n","        # https://michaeljanz-data.science/deepllearning/natural-language-processing/scoring-texts-by-their-grammar-in-python/\n","        scores_word_based_sentence = []\n","        scores_sentence_based_sentence = []\n","        s1 = time.perf_counter()\n","        sentences = nltk.tokenize.sent_tokenize(row.grammartext)\n","        e1 = time.perf_counter()\n","        # sentences = self.split_into_sentences(row)\n","        for sentence in sentences:\n","        # for sentence in helpers.text_to_sentences(text):\n","            matches = self.tool.check(sentence)\n","            count_errors = len(matches)\n","            # only check if the sentence is correct or not\n","            scores_sentence_based_sentence.append(np.min([count_errors, 1]))\n","            scores_word_based_sentence.append(count_errors)\n","            \n","        word_count = len(nltk.tokenize.word_tokenize(row.grammartext))\n","        sum_count_errors_word_based = np.sum(scores_word_based_sentence)\n","        score_word_based = 1 - (sum_count_errors_word_based / word_count)\n","        \n","        sentence_count = len(sentences)       \n","        sum_count_errors_sentence_based = np.sum(scores_sentence_based_sentence)\n","        score_sentence_based = 1 - np.sum(sum_count_errors_sentence_based / sentence_count)\n","\n","        df.loc[index, 'grammar-word-score'] = score_word_based\n","        df.loc[index, 'grammar-sentence-score'] = score_sentence_based\n","\n","    @staticmethod\n","    def get_readability(df,index,row):\n","        if not row.clean_text:\n","            a=9\n","            # print('sentence has no real text')\n","        else:\n","            results = readability.getmeasures(row.clean_text,lang='en')\n","            # [ df.loc[index, score] = results['readability grades'][score] for score in \\\n","            #  ['Kincaid','ARI', 'Coleman-Liau', 'FleschReadingEase', 'GunningFogIndex', \\\n","            #   'LIX', 'SMOGIndex', 'RIX', 'DaleChallIndex'] ]\n","                    \n","            # readability grades\n","            df.loc[index, 'Kincaid'] = results['readability grades']['Kincaid']\n","            df.loc[index, 'ARI'] = results['readability grades']['ARI']\n","            df.loc[index, 'Coleman-Liau'] = results['readability grades']['Coleman-Liau']\n","            df.loc[index, 'FleschReadingEase'] = results['readability grades']['FleschReadingEase']\n","            df.loc[index, 'GunningFogIndex'] = results['readability grades']['GunningFogIndex']\n","            df.loc[index, 'LIX'] = results['readability grades']['LIX']\n","            df.loc[index, 'SMOGIndex'] = results['readability grades']['SMOGIndex']\n","            df.loc[index, 'RIX'] = results['readability grades']['RIX']\n","            df.loc[index, 'DaleChallIndex'] = results['readability grades']['DaleChallIndex']\n","            # sentence info\n","            # self.df.loc[index,'characters_per_word'] = results['sentence info']['characters_per_word']\n","            # self.df.loc[index,'syll_per_word'] = results['sentence info']['syll_per_word']\n","            # self.df.loc[index,'words_per_sentence'] = results['sentence info']['words_per_sentence']\n","            # self.df.loc[index,'sentences_per_paragraph'] = results['sentence info']['sentences_per_paragraph']\n","            # self.df.loc[index,'type_token_ratio'] = results['sentence info']['type_token_ratio']\n","            # self.df.loc[index,'characters'] = results['sentence info']['characters']\n","            # self.df.loc[index,'syllables'] = results['sentence info']['syllables']\n","            # self.df.loc[index,'words'] = results['sentence info']['words']\n","            # self.df.loc[index,'wordtypes'] = results['sentence info']['wordtypes']\n","            # self.df.loc[index,'long_words'] = results['sentence info']['long_words']\n","            df.loc[index,'complex_words'] = results['sentence info']['complex_words']\n","            df.loc[index,'complex_words_dc'] = results['sentence info']['complex_words_dc']\n","\n","\n","    def get_topic(self,df,index,row):\n","        # tokens = self.topic_tokenizer(row.clean_text,return_tensors='pt')\n","        output = self.topic_model(**row.topic_tokens.to(self.device))\n","        scores = output[0][0].detach().cpu().numpy()\n","        scores = expit(scores)\n","        pred = np.argmax(scores)\n","\n","        df.loc[index,'topic'] = pred\n","\n","        for i in range(19):\n","            label = str(self.topic_classes[i])\n","            df.loc[index, label] = scores[i]\n","\n","    def get_topic_single(self,df,index,row):\n","        # tokens = self.topic_tokenizer(row.clean_text,return_tensors='pt')\n","        output = self.topic_model_single(**row.topic_tokens.to(self.device))\n","        scores = output[0][0].detach().cpu().numpy()\n","        scores = expit(scores)\n","        pred = np.argmax(scores)\n","\n","        df.loc[index,'topic_single'] = pred\n","\n","        for i in range(6):\n","            label = str(self.topic_classes_single[i])\n","            df.loc[index, label] = scores[i]\n","\n","    def get_politeness(self,df,index,row):\n","        df.loc[index, 'politeness'] = row.politeness    \n","\n","    def get_offensive(self,df,index,row):\n","        df.loc[index, 'offensive'] = row.offensive  \n","\n","    def get_irony(self,df,index,row):\n","        output = self.irony_model(**row.cardiff_tokens.to(self.device))\n","        scores = output[0][0].detach().cpu().numpy()\n","        scores = softmax(scores)\n","\n","        ranking = np.argsort(scores)\n","        ranking = ranking[::-1]\n","\n","        df.loc[index, 'irony'] = ranking[0]\n","\n","    def get_emoji(self,df,index,row):\n","        output = self.emoji_model(**row.cardiff_tokens.to(self.device))\n","        scores = output[0][0].detach().cpu().numpy()\n","        scores = softmax(scores)\n","\n","        ranking = np.argsort(scores)\n","        ranking = ranking[::-1]\n","\n","        df.loc[index, 'emoji'] = ranking[0] \n","\n","    def get_hate(self,df,index,row):\n","        for i in range(3):\n","            df.loc[index, self.hate_labels[i]] = row.hate_output.probas[self.hate_labels[i]]\n","\n","    def get_emotion(self,df,index,row):\n","        for i in range(6):\n","            df.loc[index, self.emo_labels[i]] = row.emo_output.probas[self.emo_labels[i]]\n","\n","    @staticmethod\n","    def get_gender_model(df):\n","        path = 'user_gender_class/model/logistic_gender'\n","        mod = joblib.load(path)\n","        predictions = mod.predict(df)\n","        return predictions\n","\n","\n","####################################################################################################\n","############################                 LOAD MODELS            ################################\n","####################################################################################################\n","\n","    def load_gender_model(self,tweet_ids,user_ids,feed_ids):\n","        from nltk.corpus import stopwords\n","        stop_words = stopwords.words('english')\n","        stop_words.extend(['u', 'wa', 'ha', 'would', 'com'])\n","        print('starting user gender classification')\n","        remove_rt = lambda x: re.sub('RT @\\w+: ',\" \",x)\n","        rt = lambda x: re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",x)\n","        print('now cleaning')\n","        p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION, p.OPT.NUMBER)\n","        self.user_feeds['cln_description'] = self.user_feeds.description.map(remove_rt).map(rt).map(p.clean).str.lower()\n","        self.user_feeds['cln_text'] = self.user_feeds.text.map(remove_rt).map(rt).map(p.clean).str.lower()\n","        n = len(self.user_feeds)\n","\n","        # call the user feeds df to df just for ease\n","        df = self.user_feeds.copy()\n","\n","        print(df.columns)\n","        df['sep'] = ['.' for i in range(n)]\n","        df['txt'] = df['cln_description'] + df['sep'] + df['cln_text']\n","        df['txt'] = df['txt'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n","        user_info = df.txt\n","        print('finished cleaning')\n","        print('now predicting the gender of each tweet and descrption')\n","        text_predictions = self.get_gender_model(user_info)\n","        print('model finished predicting gender')\n","        df['gender'] = text_predictions\n","        male_txt = df[df['gender']==1]\n","        female_txt = df[df['gender']!=1]\n","        print('male tweets')\n","        print(len(male_txt))\n","        print('female tweets')\n","        print(len(female_txt))\n","        # both these contain the users feeds and their feeds\n","\n","\n","        self.df.reset_index(drop=False)\n","        self.df.set_index('user_id',inplace=True)\n","\n","\n","        self.df['gender'] = np.nan\n","        self.df['num_male'] = np.nan\n","        self.df['num_female'] = np.nan\n","\n","        for id in user_ids:\n","            info_user = df[df['user_id']==id]\n","            if info_user.empty:\n","                print('dont have useres feeds')\n","            else:\n","                gen = info_user['gender'].mode().values[0]\n","                self.df.loc[id,'gender'] = gen\n","                self.df.loc[id,'num_male'] = len(info_user[info_user['gender']==1])\n","                self.df.loc[id,'num_female'] = len(info_user[info_user['gender']!=1])\n","                \n","\n","        self.df.reset_index(drop=False)\n","        try:\n","            self.df.set_index('tweet_id',inplace=True)\n","        except:\n","            print('tweet_id is already the index')\n","\n","        if 'user_id' not in self.df:\n","            self.df['user_id'] = user_ids\n","            print('\\n\\n\\n added the user ideas \\n\\n\\n')\n","\n","        print('gender results')\n","        male_usr = self.df[self.df['gender']==1]\n","        female_usr = self.df[self.df['gender']!=1]\n","        print('male users')\n","        print(len(male_usr))\n","        print('female users')\n","        print(len(female_usr))\n","        print('finished gender')\n","        print('---------\\n---------\\n')\n","        \n","\n","    def load_topic_model(self):\n","\n","        MODEL = f\"cardiffnlp/tweet-topic-21-multi\"\n","        tokenizer = AutoTokenizer.from_pretrained(MODEL)\n","\n","        tokens = self.tweet_df.text.apply(lambda row: tokenizer(row, return_tensors='pt'))\n","        with torch.no_grad():\n","            self.topic_model = AutoModelForSequenceClassification.from_pretrained(MODEL).to(self.device)\n","\n","        self.topic_classes = self.topic_model.config.id2label\n","\n","        s_MODEL = f\"cardiffnlp/tweet-topic-21-single\"\n","        with torch.no_grad():\n","            self.topic_model_single = AutoModelForSequenceClassification.from_pretrained(s_MODEL).to(self.device)\n","        self.topic_classes_single = self.topic_model_single.config.id2label\n","\n","        self.tweet_df['topic_tokens'] = tokens\n","        print('loaded topic model')\n","\n","        print('now loading the topic tokens for all user feeds')\n","        feed_tokens = self.user_feeds.text.apply(lambda row: tokenizer(row, return_tensors='pt'))\n","        self.user_feeds['topic_tokens'] = feed_tokens\n","        print('---------\\n---------\\n')\n","        \n","        \n","\n","    @staticmethod\n","    def run_offensive_model(test):\n","        current = os.getcwd()\n","        new_dir = current+'/offensiveness'\n","\n","        os.chdir(new_dir)\n","        df_scraped = pd.read_csv('labeled_tweets.csv')\n","        df_public = pd.read_csv('public_data_labeled.csv')\n","        df_scraped.drop_duplicates(inplace = True)\n","        df_scraped.drop('id', axis = 'columns', inplace = True)\n","        df_public.drop_duplicates(inplace = True)\n","        df = pd.concat([df_scraped, df_public])\n","        df['label'] = df.label.map({'Offensive': 1, 'Non-offensive': 0})\n","        X_train, X_test, y_train, y_test = train_test_split(df['full_text'], \n","                                                    df['label'], \n","                                                    random_state=42)\n","\n","        os.chdir(current)\n","        # Instantiate the CountVectorizer method\n","        count_vector = CountVectorizer(stop_words = 'english', lowercase = True)\n","\n","        # Fit the training data and then return the matrix\n","        training_data = count_vector.fit_transform(X_train)\n","        testing_data = count_vector.transform(test)\n","        model = SGDClassifier()\n","        model.fit(training_data, y_train)\n","        preds = model.predict(testing_data)\n","        return preds\n","    \n","\n","    def load_offensive_model(self):\n","        test_data = self.tweet_df.text\n","        preds = self.run_offensive_model(test_data)\n","        self.tweet_df['offensive'] = preds\n","        print('loaded offensive model')\n","\n","        print('loading offensive model for users')\n","        feed_test = self.user_feeds.text\n","        feed_preds = self.run_offensive_model(feed_test)\n","        self.user_feeds['offensive'] = feed_preds\n","        print('---------\\n---------\\n')\n","\n","\n","    def load_irony_model(self):\n","        task='irony'\n","        MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n","        tokenizer = AutoTokenizer.from_pretrained(MODEL)\n","        tokens = self.tweet_df.text.apply(lambda row: tokenizer(row, return_tensors='pt'))\n","        # download label mapping\n","        mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n","        with urllib.request.urlopen(mapping_link) as f:\n","            html = f.read().decode('utf-8').split(\"\\n\")\n","            csvreader = csv.reader(html, delimiter='\\t')\n","        self.irony_labels = [row[1] for row in csvreader if len(row) > 1]\n","        # PT\n","        with torch.no_grad():\n","            self.irony_model = AutoModelForSequenceClassification.from_pretrained(MODEL).to(self.device)\n","        self.tweet_df['cardiff_tokens'] = tokens\n","        print('loaded irony model for tweets')\n","\n","\n","\n","    def load_emoji_model(self):\n","        task='emoji'\n","        MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n","        tokenizer = AutoTokenizer.from_pretrained(MODEL)\n","        # download label mapping\n","        mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n","        with urllib.request.urlopen(mapping_link) as f:\n","            html = f.read().decode('utf-8').split(\"\\n\")\n","            csvreader = csv.reader(html, delimiter='\\t')\n","        self.emoji_labels = [row[1] for row in csvreader if len(row) > 1]\n","\n","        with torch.no_grad():\n","            self.emoji_model = AutoModelForSequenceClassification.from_pretrained(MODEL).to(self.device)\n","        print('loaded emoji model')\n","        print('---------\\n---------\\n')\n","\n","    def load_politeness(self):\n","        ## Scoring each tweet based on politeness \n","        class SimpleDataset:\n","            def __init__(self, tokenized_texts):\n","                self.tokenized_texts = tokenized_texts\n","            \n","            def __len__(self):\n","                return len(self.tokenized_texts[\"input_ids\"])\n","            \n","            def __getitem__(self, idx):\n","                return {k: v[idx] for k, v in self.tokenized_texts.items()}\n","        from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n","\n","\n","        # Tokenize same way as training data\n","        model_name = 'roberta-base'\n","        path = f'politeness{os.path.sep}results/checkpoint-52500/'\n","        print('loaded politeness')\n","        tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","        print('politeness per tweet')\n","        with torch.no_grad():\n","            model = AutoModelForSequenceClassification.from_pretrained(path).to(self.device)\n","        tweets = self.tweet_df.grammartext.tolist()\n","        test_encodings = tokenizer(tweets , truncation=True, padding=True, max_length=256)\n","        test_dataset = SimpleDataset(test_encodings)       \n","\n","\n","        trainer = Trainer(model=model)\n","        predictions = trainer.predict(test_dataset)\n","        self.tweet_df['politeness'] = predictions[0]\n","\n","\n","        print('politeness per user')\n","        with torch.no_grad():\n","            model = AutoModelForSequenceClassification.from_pretrained(path).to(self.device)\n","        feeds = self.user_feeds.text.tolist()\n","        feed_encodings = tokenizer(feeds , truncation=True, padding=True, max_length=256)\n","        print('tokenized feeds')\n","        feed_dataset = SimpleDataset(feed_encodings)       \n","\n","        print('now predicting')\n","        feed_trainer = Trainer(model=model)\n","        feed_predictions = feed_trainer.predict(feed_dataset)\n","        self.user_feeds['politeness'] = feed_predictions[0]\n","        \n","        print('loaded politeness model for users')\n","        print('---------\\n---------\\n')\n","\n","        \n","\n","    def load_psysentimento_model(self):\n","            \n","        tweets = self.tweet_df.text.to_list()\n","\n","        # hateful\n","        analyzer = create_analyzer(task=\"hate_speech\", lang=\"en\")\n","        self.hate_labels = ['hateful', 'targeted', 'aggressive']\n","\n","        predictions = [ analyzer.predict(preprocess_tweet(txt)) for txt in tweets ]\n","        print('predicted hate of tweets')\n","        # predictions = process_(analyzer,tweets)\n","        self.tweet_df['hate_output'] = predictions\n","        \n","        print('loaded hate model')\n","\n","        #emotion\n","        e_analyzer = create_analyzer(task=\"emotion\", lang=\"en\")\n","        self.emo_labels = ['joy','sadness','others','anger','surprise','disgust','fear']     \n","\n","        # e_predictions = process_(e_analyzer,tweets)\n","        e_predictions = [ e_analyzer.predict(preprocess_tweet(txt)) for txt in tweets ]\n","        print('predicted emotion of tweets')\n","        self.tweet_df['emo_output'] = e_predictions\n","\n","        print('loaded emotion model')\n","        feeds = self.user_feeds.text.to_list()\n","        print('loading psysentimento for user feeds')\n","\n","        print('tokenizing and predicting user feeds hate')\n","        # fd_hate_pred = process_(analyzer,feeds)\n","        fd_hate_pred = [ analyzer.predict(preprocess_tweet(txt)) for txt in feeds ]\n","        self.user_feeds['hate_output'] = fd_hate_pred\n","        print('predicted hate of feeds')\n","\n","\n","\n","#######################################\n","# MAIN FUNCTION TO RUN THE ANALYSIS\n","########################################\n","\n","\n","    def tweet_analysis(self):\n","\n","        informer_db = self.load_informer_data()\n","        self.get_device()\n","\n","        print('loaded informer bombaclart data')\n","\n","        #####################\n","        # LOAD IN THE DATA!!!!!!!\n","        #####################\n","\n","        # ALL THE TWEETS IN THE MULTI-SOURCE CASE - store by tweet id\n","        all_tweets = self.get_the_tweets(informer_db)\n","        self.df = pd.DataFrame.from_dict(all_tweets, orient='index')\n","        self.df.drop_duplicates()\n","        user_ids = self.df.user_id.copy().tolist()\n","\n","        print('loaded in ms tweets')\n","\n","        # ALL THE USER FEEDS!!!!! STORED BY THE USER ID !!!!!\n","        all_users = self.get_users(informer_db)\n","        all_users,tweet_ids_feeds = self.add_feeds(all_users)\n","        ## ALL USER FEEDS!! STORED BY THE TWEET ID!! WILL!!!\n","        feeds = [ {'user_id':key, 'description':value['description'],'text':tweet['tweet-text'], 'tweet_id':tweet['id']  }  for key,value in all_users.items() for tweet in value['feed']  ]\n","        self.user_feeds = pd.DataFrame(feeds)\n","        self.user_feeds['tweet_ids'] = tweet_ids_feeds\n","        self.user_feeds.set_index('tweet_ids', inplace=True)\n","        print(f'\\n\\n\\n raw feed len {len(self.user_feeds)}')\n","        self.user_feeds.drop_duplicates()\n","        print(f'\\n\\n\\n feed len after drop duplibats {len(self.user_feeds)}' )\n","        self.user_feeds = self.user_feeds.loc[self.user_feeds['user_id'].isin(user_ids)]\n","        print(f'\\n\\n\\n feed len after not considering the feeds we didnt pull {len(self.user_feeds)}' )\n","\n","        print('loaded in necessary data')\n","\n","        #####################\n","        #####################\n","\n","\n","        ##########################################\n","        # TO TEST FOR A SUBSET OF TWEETS\n","        self.df = self.df.iloc[0:100]\n","        yusers = self.df['user_id'].tolist()\n","        user_ids =yusers\n","\n","\n","        self.user_feeds = self.user_feeds[self.user_feeds['user_id'].isin(yusers)]\n","        print('loading test data')\n","        ## END\n","        ##########################################\n","\n","        # load in ids\n","        feed_tweet_ids = list(self.user_feeds.index.values)\n","        tweet_ids = list(self.df.index.values)\n","        feed_user_ids = list(self.user_feeds['user_id'].astype(str))\n","\n","        self.df[['polarity', 'subjectivity']] = self.df['text'].apply(lambda Text: pd.Series(TextBlob(Text).sentiment))\n","        self.user_feeds[['polarity', 'subjectivity']] = self.user_feeds['text'].apply(lambda Text: pd.Series(TextBlob(Text).sentiment))\n","        self.user_feeds = self.tweet_cleaner(self.user_feeds)\n","\n","        self.tweet_df = self.tweet_cleaner(self.df.copy())\n","\n","        ##########################################################################\n","        ##########################################################################\n","        ### LOADING IN NECESSARY MODELS\n","\n","        \n","        if 'tweet_id' not in self.df:\n","            self.df['tweet_id'] = tweet_ids\n","\n","        if 'tweet_id' not in self.user_feeds:\n","            self.df['tweet_id'] = feed_tweet_ids\n","\n","        print('put tweet ids back in')\n","\n","        self.load_gender_model(tweet_ids,user_ids,feed_tweet_ids)\n","\n","        print('loaded gender you bender')\n","\n","\n","        loaders = [self.load_politeness, self.load_topic_model, self.load_irony_model, self.load_offensive_model, self.load_emoji_model, self.load_psysentimento_model]\n","\n","        [load() for load in loaders]\n","\n","        self.df = self.tweet_df.copy()\n","        del self.tweet_df\n","\n","        ########################################################################\n","        ### SAVE!!!! ################\n","\n","\n","        temp_save = f'tweets/{self.hashtag}/{self.hashtag}_before_assigning_SUB'\n","        twtt = '_tweets'\n","        fdsd = '_feeds'\n","\n","        save_path_t = f'tweets/{self.hashtag}/{self.hashtag}_tweets_classified_SUB'\n","        # save_path_t = f'tweet_analysis/output/{self.hashtag}_tweets_classified2'\n","\n","        save_path_f = f'tweets/{self.hashtag}/{self.hashtag}_feeds_classified_SUB'\n","        # save_path_f = f'tweet_analysis/output/{self.hashtag}_feeds_classified2'\n","\n","\n","        # save all the tweets\n","        self.df.to_csv(temp_save + twtt + '.csv') # csv\n","\n","        with open( temp_save + twtt + '.pkl', 'wb') as f:\n","            joblib.dump(self.df, f) # pkl\n","\n","\n","        # save user feeds df\n","        self.user_feeds.to_csv( save_path_f + '.csv') # csv\n","\n","        with open( save_path_f + '.pkl', 'wb') as f:\n","            joblib.dump(self.user_feeds, f) # pkl\n","\n","        ########################################################################\n","\n","        user_feeds_df = self.user_feeds.copy() # keep this but delete from self.\n","\n","        del self.user_feeds # delete so code runs smooth\n","\n","        ########################################################################\n","        ########################################################################\n","        # CLASSIFY EACH TWEET IN MS\n","        ########################################################################\n","\n","        score_funcs = [ self.get_sentiment, self.get_grammar, self.get_readability, self.get_politeness, self.get_offensive, self.get_topic, self.get_topic_single, self.get_irony, self.get_emoji, self.get_emotion, self.get_hate ]\n","\n","        # SCORE EACH TWEET IN THE MULTI-SOURCE EVENT\n","\n","        tweet_df = self.df.copy()\n","\n","        t1 = time.time()\n","\n","        [ func(self.df,index,row) for func in score_funcs for index, row in tweet_df.iterrows() ]\n","\n","        t2 = time.time()\n","        print(f'finshed scoring tweets in {(t2-t1)/3600} hours')\n","        print('-------')\n","        print('---------------------')\n","        print('-------------------------------------------------')\n","        print('finished classifying all the tweets')\n","        print('-------------------------------------------------')\n","        print('---------------------')\n","        print('-------')\n","        #################################\n","\n","\n","        ### SAVE AFTER CLASSIFYING ALL TWEETS\n","        #################################\n","\n","        self.df.to_csv(save_path_t+'.csv')\n","\n","        joblib.dump(self.df, save_path_t+'.pkl')\n","\n","        del self.df\n","\n","\n","        ##################################################################\n","        #################################\n","        # CLASSIFY EACH USER\n","        #################################\n","        ##################################################################\n","\n","        us_feed_dum = user_feeds_df.copy()\n","\n","        user_feeds_df = joblib.load(save_path_f+'.pkl')\n","        print('im almost a genius')\n","\n","\n","        funcs = [self.get_sentiment, self.get_politeness, self.get_topic, self.get_hate]\n","\n","        print('now scoring each tweet in feed. good luck code')\n","\n","        t1= time.time()\n","        [ func(user_feeds_df,index,row) for func in funcs for index, row in us_feed_dum.iterrows() ]\n","        t2 = time.time()\n","        print(f'finshed scoring each tweet in users feed in {(t2-t1)/3600} hours')\n","\n","        print('-------')\n","        print('---------------------')\n","        print('-------------------------------------------------')\n","        print('\\n\\n\\nclassified each tweet in users feed')\n","        print('-------------------------------------------------')\n","        print('---------------------')\n","        print('-------')\n","\n","        user_feeds_df.to_csv(save_path_f+'.csv')\n","        with open( save_path_f + '.pkl', 'wb') as f:\n","            joblib.dump(user_feeds_df, f) # pkl\n","\n","        ###############################################################\n","        ###############################################################\n","        # AVERAGE FOR EACH USERS FEED!!!!!\n","\n","        self.df = joblib.load(save_path_t+'.pkl')\n","        print('pulled w joblib')\n","\n","\n","        if 'user_id' not in self.df:\n","            self.df['user_id'] = user_ids\n","\n","        if 'user_id' not in user_feeds_df:\n","            user_feeds_df['user_id'] = feed_user_ids\n","\n","        uid = list ( set( user_ids))\n","        fid = list(set(feed_user_ids))\n","\n","        user_ids = [ u for u in uid if u in fid ]\n","\n","        user_feeds_df = user_feeds_df.loc[user_feeds_df['user_id'].isin(user_ids)]\n","\n","        self.df['user_id'] = self.df.copy()['user_id'].astype(int)\n","\n","        user_feeds_df['user_id'] = user_feeds_df.copy()['user_id'].astype(int)\n","\n","        self.df = self.df[ self.df['user_id'].notna()]\n","        user_feeds_df = user_feeds_df[ user_feeds_df['user_id'].notna()]\n","\n","        self.df.reset_index(drop=False)\n","        self.df.set_index('user_id',inplace=True)\n","\n","        self.user_feeds = user_feeds_df.copy()\n","        del user_feeds_df\n","\n","        self.user_feeds.reset_index(drop=False)\n","        self.user_feeds.set_index('user_id',inplace=True)\n","\n","\n","        self.topicss = [self.topic_classes[i] for i in range(19)]\n","        self.hatesss = [self.hate_labels[i] for i in range(3)]\n","\n","        print('\\n\\n\\n now averaging all the twitter users tweets from feed')\n","\n","\n","        for user in user_ids:\n","        # for user in yusers:\n","            self.get_user_means(int(user))\n","            self.get_user_modes(int(user))\n","\n","        print('finished getting all metrics!!!!!!!')\n","\n","        save_path_all = f'tweets/{self.hashtag}/{self.hashtag}_ALL_classified'\n","        # save_path_all = f'tweet_analysis/output/{self.hashtag}_ALL_classified2'\n","\n","\n","\n","        self.df.to_csv(save_path_all+'.csv')\n","\n","        with open( save_path_f + '.pkl', 'wb') as f:\n","            joblib.dump(self.df, f) # pkl\n","\n","\n","        print('done!!!!')\n","\n","    def get_user_means(self,user):\n","        user_info = self.user_feeds.loc[user]\n","        scores = ['neg','neu','pos','compound',\n","        'polarity', 'subjectivity',\n","        'politeness']\n","        all_scores = scores + self.topicss + self.hatesss\n","        for score in all_scores:\n","            new = f'user_{score}_mean'\n","            self.df.loc[user,new] = user_info[score].mean()\n","\n","    def get_user_modes(self,user):\n","        user_info = self.user_feeds.loc[user]\n","        scores = ['sentiment',\n","        'topic']\n","        for score in scores:\n","            new = f'user_{score}_mode'\n","            self.df.loc[user,new] = user_info[score].mode().tolist()[0]"]},{"cell_type":"markdown","metadata":{"id":"x27JS-gN8X-o"},"source":["## RUN"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"bbwJ9GEpucoO","outputId":"038ce9f4-9472-4126-fc43-328c3d8e4e4b","executionInfo":{"status":"ok","timestamp":1662667007871,"user_tz":-60,"elapsed":183418,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"}}},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla V100-SXM2-16GB\n","loaded informer bombaclart data\n","loaded in ms tweets\n","pulled data on 4068 users\n","\n","\n","\n"," raw feed len 401003\n","\n","\n","\n"," feed len after drop duplibats 401003\n","\n","\n","\n"," feed len after not considering the feeds we didnt pull 401003\n","loaded in necessary data\n","loading test data\n","put tweet ids back in\n","starting user gender classification\n","now cleaning\n","Index(['user_id', 'description', 'text', 'tweet_id', 'polarity',\n","       'subjectivity', 'grammartext', 'clean_text', 'text_len',\n","       'text_word_count', 'punct', 'tokenized', 'nonstop', 'stemmed',\n","       'cln_description', 'cln_text'],\n","      dtype='object')\n","finished cleaning\n","now predicting the gender of each tweet and descrption\n","model finished predicting gender\n","male tweets\n","6464\n","female tweets\n","2627\n","dont have useres feeds\n","\n","\n","\n"," added the user ideas \n","\n","\n","\n","gender results\n","male users\n","72\n","female users\n","28\n","finished gender\n","---------\n","---------\n","\n","loaded gender you bender\n","loaded politeness\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n","loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n","loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading configuration file politeness/results/checkpoint-52500/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"politeness/results/checkpoint-52500/\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"regression\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file politeness/results/checkpoint-52500/pytorch_model.bin\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["politeness per tweet\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at politeness/results/checkpoint-52500/.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","***** Running Prediction *****\n","  Num examples = 100\n","  Batch size = 8\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [13/13 00:00]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["loading configuration file politeness/results/checkpoint-52500/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"politeness/results/checkpoint-52500/\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"regression\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file politeness/results/checkpoint-52500/pytorch_model.bin\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["politeness per user\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at politeness/results/checkpoint-52500/.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","***** Running Prediction *****\n","  Num examples = 9091\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["tokenized feeds\n","now predicting\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1137' max='1137' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1137/1137 00:43]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["loaded politeness model for users\n","---------\n","---------\n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["loading file https://huggingface.co/cardiffnlp/tweet-topic-21-multi/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/1daf1d394b649e9ab6c0bfc37629115ed7766cb32e0be8821110820610ba0980.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n","loading file https://huggingface.co/cardiffnlp/tweet-topic-21-multi/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/88006e159b7014116256e91ec1595ca77cffffd2b060ee2b91bf7aed41644875.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n","loading file https://huggingface.co/cardiffnlp/tweet-topic-21-multi/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/047625f13f02ecae088036957c64a40f7bebb93fd2d2ad6324af4fc378fff90a.9f68d0ce76c33cdd199a6beeede63d9293d6005f826c0702d530ccefbcd221b7\n","loading file https://huggingface.co/cardiffnlp/tweet-topic-21-multi/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/cardiffnlp/tweet-topic-21-multi/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/86f379e4ef5dad9037395318b3d4cc8685000af32c64d37ab4158e8df91169cc.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n","loading file https://huggingface.co/cardiffnlp/tweet-topic-21-multi/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/50a78d02314329fbddfef47fdd5e1a3c7caf505195bf06f4d8263648bc56ceaa.fe0807eede1fcfc7436be17cbcc9a0bc5a06769ccb408c587f0ce6ae666b586b\n","loading configuration file https://huggingface.co/cardiffnlp/tweet-topic-21-multi/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ec25f52502749311e85eceb137b282f352376f01108c928c77b2e8d75bb645ad.520359c92ca91013f02967e1f505f9e5f0d0f0385ac8f908ea0ff68e670bb3ed\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/tweet-topic-21-multi\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"arts_&_culture\",\n","    \"1\": \"business_&_entrepreneurs\",\n","    \"2\": \"celebrity_&_pop_culture\",\n","    \"3\": \"diaries_&_daily_life\",\n","    \"4\": \"family\",\n","    \"5\": \"fashion_&_style\",\n","    \"6\": \"film_tv_&_video\",\n","    \"7\": \"fitness_&_health\",\n","    \"8\": \"food_&_dining\",\n","    \"9\": \"gaming\",\n","    \"10\": \"learning_&_educational\",\n","    \"11\": \"music\",\n","    \"12\": \"news_&_social_concern\",\n","    \"13\": \"other_hobbies\",\n","    \"14\": \"relationships\",\n","    \"15\": \"science_&_technology\",\n","    \"16\": \"sports\",\n","    \"17\": \"travel_&_adventure\",\n","    \"18\": \"youth_&_student_life\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"arts_&_culture\": 0,\n","    \"business_&_entrepreneurs\": 1,\n","    \"celebrity_&_pop_culture\": 2,\n","    \"diaries_&_daily_life\": 3,\n","    \"family\": 4,\n","    \"fashion_&_style\": 5,\n","    \"film_tv_&_video\": 6,\n","    \"fitness_&_health\": 7,\n","    \"food_&_dining\": 8,\n","    \"gaming\": 9,\n","    \"learning_&_educational\": 10,\n","    \"music\": 11,\n","    \"news_&_social_concern\": 12,\n","    \"other_hobbies\": 13,\n","    \"relationships\": 14,\n","    \"science_&_technology\": 15,\n","    \"sports\": 16,\n","    \"travel_&_adventure\": 17,\n","    \"youth_&_student_life\": 18\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"multi_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/cardiffnlp/tweet-topic-21-multi/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/09afae9646637ab82d1dfb71d31cd16d501f2520a5f947a4ae4d73de72f9dc71.fa54640b8d4deedaa096846028a1859f456664089266047b8bdfe4d6f5f7a046\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/tweet-topic-21-multi.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","loading configuration file https://huggingface.co/cardiffnlp/tweet-topic-21-single/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/75582225ef99aad8073e1f62926777a1d8591adfa379597a8f3238dc1e24da74.8ded2a309dd799fbcdc6866cfad8a01c39b528cf15db85181bd13bba4763ac4a\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/tweet-topic-21-single\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"arts_&_culture\",\n","    \"1\": \"business_&_entrepreneurs\",\n","    \"2\": \"pop_culture\",\n","    \"3\": \"daily_life\",\n","    \"4\": \"sports_&_gaming\",\n","    \"5\": \"science_&_technology\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"arts_&_culture\": 0,\n","    \"business_&_entrepreneurs\": 1,\n","    \"daily_life\": 3,\n","    \"pop_culture\": 2,\n","    \"science_&_technology\": 5,\n","    \"sports_&_gaming\": 4\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/cardiffnlp/tweet-topic-21-single/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/041520af8ed380b8a62c28b0dbee222f5f7769b4ab43170e4ac03a56e5d64a8c.b4cb49f6ea070108f051e820bd3a9167e581a6a0f7cbf7a9bed80348689a2da3\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/tweet-topic-21-single.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["loaded topic model\n","now loading the topic tokens for all user feeds\n","---------\n","---------\n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file https://huggingface.co/cardiffnlp/twitter-roberta-base-irony/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/b318877b5e8f6efe46df15ea1f284fe87c4f0f4d7990da954a56a7f93da3e887.d526c7548113da2236674baa2b82e3bc598efb72510d3d6cdbc3ede3d2fe08bb\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/twitter-roberta-base-irony\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading file https://huggingface.co/cardiffnlp/twitter-roberta-base-irony/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/804ed6d345e57fbb81c571128419b9e03a05038bec4340a98cd2b597136021a2.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\n","loading file https://huggingface.co/cardiffnlp/twitter-roberta-base-irony/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/2b61f296c8a50bacb1fbdcd9e64be78b1d4f46171c428a93e47139a97e9f853e.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","loading file https://huggingface.co/cardiffnlp/twitter-roberta-base-irony/resolve/main/tokenizer.json from cache at None\n","loading file https://huggingface.co/cardiffnlp/twitter-roberta-base-irony/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/cardiffnlp/twitter-roberta-base-irony/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/3718e2a2e89d3035d901e13462e6b3c48c7bf6b9ea912a54db4659015afef575.0dc5b1041f62041ebbd23b1297f2f573769d5c97d8b7c28180ec86b8f6185aa8\n","loading file https://huggingface.co/cardiffnlp/twitter-roberta-base-irony/resolve/main/tokenizer_config.json from cache at None\n","loading configuration file https://huggingface.co/cardiffnlp/twitter-roberta-base-irony/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/b318877b5e8f6efe46df15ea1f284fe87c4f0f4d7990da954a56a7f93da3e887.d526c7548113da2236674baa2b82e3bc598efb72510d3d6cdbc3ede3d2fe08bb\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/twitter-roberta-base-irony\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading configuration file https://huggingface.co/cardiffnlp/twitter-roberta-base-irony/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/b318877b5e8f6efe46df15ea1f284fe87c4f0f4d7990da954a56a7f93da3e887.d526c7548113da2236674baa2b82e3bc598efb72510d3d6cdbc3ede3d2fe08bb\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/twitter-roberta-base-irony\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading configuration file https://huggingface.co/cardiffnlp/twitter-roberta-base-irony/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/b318877b5e8f6efe46df15ea1f284fe87c4f0f4d7990da954a56a7f93da3e887.d526c7548113da2236674baa2b82e3bc598efb72510d3d6cdbc3ede3d2fe08bb\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/twitter-roberta-base-irony\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/cardiffnlp/twitter-roberta-base-irony/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/2a0369351c138b3da55b7f1fcc060312292b9ec9f7a2f7935ce6247e12254329.993814007b1f009124e7b60fe354c67847f928d7bf198740bd12c32c8973c226\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-irony.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["loaded irony model for tweets\n","loaded offensive model\n","loading offensive model for users\n","---------\n","---------\n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file https://huggingface.co/cardiffnlp/twitter-roberta-base-emoji/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/13caedf051daa06346e381f819924d2ad6ac5ef01602b614ab7669effc5e9982.60ac9bc11a600708d9aef4c0ad14f218936a1e4899d0b5db248e04e93b1b65c8\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/twitter-roberta-base-emoji\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\",\n","    \"6\": \"LABEL_6\",\n","    \"7\": \"LABEL_7\",\n","    \"8\": \"LABEL_8\",\n","    \"9\": \"LABEL_9\",\n","    \"10\": \"LABEL_10\",\n","    \"11\": \"LABEL_11\",\n","    \"12\": \"LABEL_12\",\n","    \"13\": \"LABEL_13\",\n","    \"14\": \"LABEL_14\",\n","    \"15\": \"LABEL_15\",\n","    \"16\": \"LABEL_16\",\n","    \"17\": \"LABEL_17\",\n","    \"18\": \"LABEL_18\",\n","    \"19\": \"LABEL_19\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_10\": 10,\n","    \"LABEL_11\": 11,\n","    \"LABEL_12\": 12,\n","    \"LABEL_13\": 13,\n","    \"LABEL_14\": 14,\n","    \"LABEL_15\": 15,\n","    \"LABEL_16\": 16,\n","    \"LABEL_17\": 17,\n","    \"LABEL_18\": 18,\n","    \"LABEL_19\": 19,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5,\n","    \"LABEL_6\": 6,\n","    \"LABEL_7\": 7,\n","    \"LABEL_8\": 8,\n","    \"LABEL_9\": 9\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading file https://huggingface.co/cardiffnlp/twitter-roberta-base-emoji/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/0e40ef8a24b0daf57a06685c69fa598582f9d045584d225e91ff2fe75e2df2c0.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\n","loading file https://huggingface.co/cardiffnlp/twitter-roberta-base-emoji/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/69f23586c2a42dbfaf335dff098f67f647e10b14ad6e3ddd4103d37a2048dfd9.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","loading file https://huggingface.co/cardiffnlp/twitter-roberta-base-emoji/resolve/main/tokenizer.json from cache at None\n","loading file https://huggingface.co/cardiffnlp/twitter-roberta-base-emoji/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/cardiffnlp/twitter-roberta-base-emoji/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/df98b00b444e134464564ea04b555eb0a8057041e0fffea3838c2a766083bb59.0dc5b1041f62041ebbd23b1297f2f573769d5c97d8b7c28180ec86b8f6185aa8\n","loading file https://huggingface.co/cardiffnlp/twitter-roberta-base-emoji/resolve/main/tokenizer_config.json from cache at None\n","loading configuration file https://huggingface.co/cardiffnlp/twitter-roberta-base-emoji/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/13caedf051daa06346e381f819924d2ad6ac5ef01602b614ab7669effc5e9982.60ac9bc11a600708d9aef4c0ad14f218936a1e4899d0b5db248e04e93b1b65c8\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/twitter-roberta-base-emoji\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\",\n","    \"6\": \"LABEL_6\",\n","    \"7\": \"LABEL_7\",\n","    \"8\": \"LABEL_8\",\n","    \"9\": \"LABEL_9\",\n","    \"10\": \"LABEL_10\",\n","    \"11\": \"LABEL_11\",\n","    \"12\": \"LABEL_12\",\n","    \"13\": \"LABEL_13\",\n","    \"14\": \"LABEL_14\",\n","    \"15\": \"LABEL_15\",\n","    \"16\": \"LABEL_16\",\n","    \"17\": \"LABEL_17\",\n","    \"18\": \"LABEL_18\",\n","    \"19\": \"LABEL_19\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_10\": 10,\n","    \"LABEL_11\": 11,\n","    \"LABEL_12\": 12,\n","    \"LABEL_13\": 13,\n","    \"LABEL_14\": 14,\n","    \"LABEL_15\": 15,\n","    \"LABEL_16\": 16,\n","    \"LABEL_17\": 17,\n","    \"LABEL_18\": 18,\n","    \"LABEL_19\": 19,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5,\n","    \"LABEL_6\": 6,\n","    \"LABEL_7\": 7,\n","    \"LABEL_8\": 8,\n","    \"LABEL_9\": 9\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading configuration file https://huggingface.co/cardiffnlp/twitter-roberta-base-emoji/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/13caedf051daa06346e381f819924d2ad6ac5ef01602b614ab7669effc5e9982.60ac9bc11a600708d9aef4c0ad14f218936a1e4899d0b5db248e04e93b1b65c8\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/twitter-roberta-base-emoji\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\",\n","    \"6\": \"LABEL_6\",\n","    \"7\": \"LABEL_7\",\n","    \"8\": \"LABEL_8\",\n","    \"9\": \"LABEL_9\",\n","    \"10\": \"LABEL_10\",\n","    \"11\": \"LABEL_11\",\n","    \"12\": \"LABEL_12\",\n","    \"13\": \"LABEL_13\",\n","    \"14\": \"LABEL_14\",\n","    \"15\": \"LABEL_15\",\n","    \"16\": \"LABEL_16\",\n","    \"17\": \"LABEL_17\",\n","    \"18\": \"LABEL_18\",\n","    \"19\": \"LABEL_19\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_10\": 10,\n","    \"LABEL_11\": 11,\n","    \"LABEL_12\": 12,\n","    \"LABEL_13\": 13,\n","    \"LABEL_14\": 14,\n","    \"LABEL_15\": 15,\n","    \"LABEL_16\": 16,\n","    \"LABEL_17\": 17,\n","    \"LABEL_18\": 18,\n","    \"LABEL_19\": 19,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5,\n","    \"LABEL_6\": 6,\n","    \"LABEL_7\": 7,\n","    \"LABEL_8\": 8,\n","    \"LABEL_9\": 9\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading configuration file https://huggingface.co/cardiffnlp/twitter-roberta-base-emoji/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/13caedf051daa06346e381f819924d2ad6ac5ef01602b614ab7669effc5e9982.60ac9bc11a600708d9aef4c0ad14f218936a1e4899d0b5db248e04e93b1b65c8\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/twitter-roberta-base-emoji\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\",\n","    \"6\": \"LABEL_6\",\n","    \"7\": \"LABEL_7\",\n","    \"8\": \"LABEL_8\",\n","    \"9\": \"LABEL_9\",\n","    \"10\": \"LABEL_10\",\n","    \"11\": \"LABEL_11\",\n","    \"12\": \"LABEL_12\",\n","    \"13\": \"LABEL_13\",\n","    \"14\": \"LABEL_14\",\n","    \"15\": \"LABEL_15\",\n","    \"16\": \"LABEL_16\",\n","    \"17\": \"LABEL_17\",\n","    \"18\": \"LABEL_18\",\n","    \"19\": \"LABEL_19\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_10\": 10,\n","    \"LABEL_11\": 11,\n","    \"LABEL_12\": 12,\n","    \"LABEL_13\": 13,\n","    \"LABEL_14\": 14,\n","    \"LABEL_15\": 15,\n","    \"LABEL_16\": 16,\n","    \"LABEL_17\": 17,\n","    \"LABEL_18\": 18,\n","    \"LABEL_19\": 19,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5,\n","    \"LABEL_6\": 6,\n","    \"LABEL_7\": 7,\n","    \"LABEL_8\": 8,\n","    \"LABEL_9\": 9\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/cardiffnlp/twitter-roberta-base-emoji/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4a168f05bd25772b9488bbdc011e5f45a99a95848a7d7b61b7320ae01d8e8ec9.1fe591f243c628bc0828384432f0fee747d800e489ab124309feadd5e70d7537\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emoji.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["loaded emoji model\n","---------\n","---------\n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/pysentimiento/bertweet-hate-speech/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/0c8f98b89844cfd90c61b9671ab81f30f6d2ffd807131275cc3d9fa6c8061b04.92562558566fffb3ff2db6e6250351c34cb7206924b30d0b25bc6447925d32c1\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"pysentimiento/bertweet-hate-speech\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"hateful\",\n","    \"1\": \"targeted\",\n","    \"2\": \"aggressive\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"aggressive\": 2,\n","    \"hateful\": 0,\n","    \"targeted\": 1\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"multi_label_classification\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading weights file https://huggingface.co/pysentimiento/bertweet-hate-speech/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/b1b28641afa821c55bf1216f931286a1e2d6f2bdf6ce43893b82fd118ff62542.742154f1fb0df2aeaab5e0f8ce81baeb42a062ffddff7febde2369888239bf57\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at pysentimiento/bertweet-hate-speech.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","loading file https://huggingface.co/pysentimiento/bertweet-hate-speech/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/af97f2d33aa35c368f65a032c5f6b065e8b88d9bdef502fbc2ce78e04579a0fa.f8a4dfe5c3c45a26f9df849d732decb191dc0c05ab270799695430332d143982\n","loading file https://huggingface.co/pysentimiento/bertweet-hate-speech/resolve/main/bpe.codes from cache at /root/.cache/huggingface/transformers/c50391b2daa6a8c1be706382b1cd8d7f4996b5664c2a0bbab6c15097305ae731.75877d86011e5d5d46614d3a21757b705e9d20ed45a019805d25159b4837b0a4\n","loading file https://huggingface.co/pysentimiento/bertweet-hate-speech/resolve/main/added_tokens.json from cache at /root/.cache/huggingface/transformers/8715836fc39b559eca950b73d5a4651bf34ab8d3acb1d0b81aae6d76e1a38fac.c1e7052e39d2135302ec27455f6db22e1520e6539942ff60a849c7f83f8ec6dc\n","loading file https://huggingface.co/pysentimiento/bertweet-hate-speech/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/fdf2edf9c4b32d36848f3a7f4b9edd83fe65a7caa032a01e30f72cd87b62affc.0dc5b1041f62041ebbd23b1297f2f573769d5c97d8b7c28180ec86b8f6185aa8\n","loading file https://huggingface.co/pysentimiento/bertweet-hate-speech/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/9eec04ca3ab9bdc071fafcf37e270203d932078b7715016c2df05a0f3bd1bf07.7cf9bfd7af087e7e2ea689b159647c3c8a45e7f0b92609dce7bca18d806905da\n","loading file https://huggingface.co/pysentimiento/bertweet-hate-speech/resolve/main/tokenizer.json from cache at None\n","Adding <mask> to the vocabulary\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["predicted hate of tweets\n","loaded hate model\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/finiteautomata/bertweet-base-emotion-analysis/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c246eed05359b1a49c45955b0265b488e35b0cbd2628e3ead7dd54c8815162ee.a2dff24b4e0a884c6d58a09968c5b68e7391e749eb698ad92541818d420fd01b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"finiteautomata/bertweet-base-emotion-analysis\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"others\",\n","    \"1\": \"joy\",\n","    \"2\": \"sadness\",\n","    \"3\": \"anger\",\n","    \"4\": \"surprise\",\n","    \"5\": \"disgust\",\n","    \"6\": \"fear\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"anger\": 3,\n","    \"disgust\": 5,\n","    \"fear\": 6,\n","    \"joy\": 1,\n","    \"others\": 0,\n","    \"sadness\": 2,\n","    \"surprise\": 4\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading weights file https://huggingface.co/finiteautomata/bertweet-base-emotion-analysis/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/61c5894a0aca5ed63159e2ec6a5501db48124c1e6de287b82bc634334f031203.9c3c4c16d0dd174434d42471b9d4670734d982be506a06fc3111c12bee4380c7\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at finiteautomata/bertweet-base-emotion-analysis.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","loading configuration file https://huggingface.co/finiteautomata/bertweet-base-emotion-analysis/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c246eed05359b1a49c45955b0265b488e35b0cbd2628e3ead7dd54c8815162ee.a2dff24b4e0a884c6d58a09968c5b68e7391e749eb698ad92541818d420fd01b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"finiteautomata/bertweet-base-emotion-analysis\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"others\",\n","    \"1\": \"joy\",\n","    \"2\": \"sadness\",\n","    \"3\": \"anger\",\n","    \"4\": \"surprise\",\n","    \"5\": \"disgust\",\n","    \"6\": \"fear\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"anger\": 3,\n","    \"disgust\": 5,\n","    \"fear\": 6,\n","    \"joy\": 1,\n","    \"others\": 0,\n","    \"sadness\": 2,\n","    \"surprise\": 4\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading file https://huggingface.co/finiteautomata/bertweet-base-emotion-analysis/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/b7837213513a9f3852dcb04048f70c13cbd0590be030e534734ffd42cbdcf45a.f8a4dfe5c3c45a26f9df849d732decb191dc0c05ab270799695430332d143982\n","loading file https://huggingface.co/finiteautomata/bertweet-base-emotion-analysis/resolve/main/bpe.codes from cache at /root/.cache/huggingface/transformers/76e357e2554ebe053d1c4c613506bc2cc19d66ae27fec8218261a7f73c6456b9.75877d86011e5d5d46614d3a21757b705e9d20ed45a019805d25159b4837b0a4\n","loading file https://huggingface.co/finiteautomata/bertweet-base-emotion-analysis/resolve/main/added_tokens.json from cache at /root/.cache/huggingface/transformers/c4b7522f44ed8adb95e62288c6458da591654f7466e3ce2f9c730bb4087411d2.c1e7052e39d2135302ec27455f6db22e1520e6539942ff60a849c7f83f8ec6dc\n","loading file https://huggingface.co/finiteautomata/bertweet-base-emotion-analysis/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/01581144d9bf96cb9c7d8a77ee93c8b1f1095af5c1204b1b038a8cb0e3247aa8.0dc5b1041f62041ebbd23b1297f2f573769d5c97d8b7c28180ec86b8f6185aa8\n","loading file https://huggingface.co/finiteautomata/bertweet-base-emotion-analysis/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/1740697312c59fe96586f476c7765cd6f08516a6102ea96f22ffee64f7553234.c260b44e952f7f2a825aac395f2ebbed4ac9553800d1e320af246e81a548f37c\n","loading file https://huggingface.co/finiteautomata/bertweet-base-emotion-analysis/resolve/main/tokenizer.json from cache at None\n","loading configuration file https://huggingface.co/finiteautomata/bertweet-base-emotion-analysis/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c246eed05359b1a49c45955b0265b488e35b0cbd2628e3ead7dd54c8815162ee.a2dff24b4e0a884c6d58a09968c5b68e7391e749eb698ad92541818d420fd01b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"finiteautomata/bertweet-base-emotion-analysis\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"others\",\n","    \"1\": \"joy\",\n","    \"2\": \"sadness\",\n","    \"3\": \"anger\",\n","    \"4\": \"surprise\",\n","    \"5\": \"disgust\",\n","    \"6\": \"fear\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"anger\": 3,\n","    \"disgust\": 5,\n","    \"fear\": 6,\n","    \"joy\": 1,\n","    \"others\": 0,\n","    \"sadness\": 2,\n","    \"surprise\": 4\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","Adding <mask> to the vocabulary\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]},{"output_type":"stream","name":"stdout","text":["predicted emotion of tweets\n","loaded emotion model\n","loading psysentimento for user feeds\n","tokenizing and predicting user feeds hate\n","predicted hate of feeds\n","finshed scoring tweets in 0.005266802046034071 hours\n","-------\n","---------------------\n","-------------------------------------------------\n","finished classifying all the tweets\n","-------------------------------------------------\n","---------------------\n","-------\n","im almost a genius\n","now scoring each tweet in feed. good luck code\n","finshed scoring each tweet in users feed in 0.07479715810881721 hours\n","-------\n","---------------------\n","-------------------------------------------------\n","\n","\n","\n","classified each tweet in users feed\n","-------------------------------------------------\n","---------------------\n","-------\n","pulled w joblib\n","\n","\n","\n"," now averaging all the twitter users tweets from feed\n","finished getting all metrics!!!!!!!\n","done!!!!\n","----------------\n","DONE \n","supercuphashtag  \n","----------------\n"]}],"source":["# hashtags = ['avengers','blm','borisjohnson','brexit','climatechange','covid','gaza','loveisland','monkeypox','nhs','olivianewtonjohn','supercup','UkraineWar']\n","\n","hashtags = ['supercup']\n","\n","for hashtag in hashtags:\n","\n","    a = Analyzer(hashtag)\n","\n","    a.tweet_analysis()\n","\n","    print(f'----------------\\nDONE \\n{hashtag}hashtag  \\n----------------')"]}]}