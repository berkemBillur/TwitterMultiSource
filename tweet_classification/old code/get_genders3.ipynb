{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","collapsed_sections":[],"authorship_tag":"ABX9TyM4Tuc7ilWGPtj1JCrO3p3G"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","metadata":{"id":"4MdwUck88MEI"},"source":["## MOUNT"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":42097,"status":"ok","timestamp":1662931979332,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"},"user_tz":-60},"id":"njrEJjvc8DnR","outputId":"fe0676e6-ee3c-495a-f9e5-5d6e46b971f6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/My Drive/MSc_project/.MAIN\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd /content/drive/My Drive/MSc_project/.MAIN"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"ZO-PczKG8gPC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662932072860,"user_tz":-60,"elapsed":93544,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"}},"outputId":"b1ef2d89-2c44-4755-8369-c0984a22c7ce"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n","Collecting install\n","  Downloading install-1.3.5-py3-none-any.whl (3.2 kB)\n","Collecting tweet-preprocessor\n","  Downloading tweet_preprocessor-0.6.0-py3-none-any.whl (27 kB)\n","Installing collected packages: tweet-preprocessor, install\n","Successfully installed install-1.3.5 tweet-preprocessor-0.6.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pycountry\n","  Downloading pycountry-22.3.5.tar.gz (10.1 MB)\n","\u001b[K     |████████████████████████████████| 10.1 MB 2.1 MB/s \n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pycountry) (57.4.0)\n","Building wheels for collected packages: pycountry\n","  Building wheel for pycountry (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pycountry: filename=pycountry-22.3.5-py2.py3-none-any.whl size=10681845 sha256=42d46e8c757a61bcc58e8e5727214bd5fc567b409cd0d6a3270d7ca78d414435\n","  Stored in directory: /root/.cache/pip/wheels/0e/06/e8/7ee176e95ea9a8a8c3b3afcb1869f20adbd42413d4611c6eb4\n","Successfully built pycountry\n","Installing collected packages: pycountry\n","Successfully installed pycountry-22.3.5\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pandas==1.2.3\n","  Downloading pandas-1.2.3-cp37-cp37m-manylinux1_x86_64.whl (9.9 MB)\n","\u001b[K     |████████████████████████████████| 9.9 MB 2.1 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.3) (1.21.6)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.3) (2022.2.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.3) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas==1.2.3) (1.15.0)\n","Installing collected packages: pandas\n","  Attempting uninstall: pandas\n","    Found existing installation: pandas 1.3.5\n","    Uninstalling pandas-1.3.5:\n","      Successfully uninstalled pandas-1.3.5\n","Successfully installed pandas-1.2.3\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.21.3-py3-none-any.whl (4.7 MB)\n","\u001b[K     |████████████████████████████████| 4.7 MB 2.2 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 51.3 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n","\u001b[K     |████████████████████████████████| 120 kB 61.7 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.9.1 tokenizers-0.12.1 transformers-4.21.3\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (0.90)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.21.6)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.7.3)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","\u001b[31mERROR: Could not find a version that satisfies the requirement mislib (from versions: none)\u001b[0m\n","\u001b[31mERROR: No matching distribution found for mislib\u001b[0m\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting langdetect\n","  Downloading langdetect-1.0.9.tar.gz (981 kB)\n","\u001b[K     |████████████████████████████████| 981 kB 2.1 MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from langdetect) (1.15.0)\n","Building wheels for collected packages: langdetect\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=9683e76fe500d78ea1790410395056ef557fbf06a9ac93f2ed70c8a53479499e\n","  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n","Successfully built langdetect\n","Installing collected packages: langdetect\n","Successfully installed langdetect-1.0.9\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting readability\n","  Downloading readability-0.3.1.tar.gz (34 kB)\n","Building wheels for collected packages: readability\n","  Building wheel for readability (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for readability: filename=readability-0.3.1-py3-none-any.whl size=35475 sha256=9584927205852a2aad665120eae4decd57c025ae2a1d1e4eccd56cffaafeeadd\n","  Stored in directory: /root/.cache/pip/wheels/b9/8b/5a/ba40b81d8e91c7bc1d4226fa51d7b5943d147be122df515c19\n","Successfully built readability\n","Installing collected packages: readability\n","Successfully installed readability-0.3.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pysentimiento\n","  Downloading pysentimiento-0.4.2-py3-none-any.whl (30 kB)\n","Collecting transformers==4.13\n","  Downloading transformers-4.13.0-py3-none-any.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 3.3 MB/s \n","\u001b[?25hCollecting sklearn<0.1,>=0.0\n","  Downloading sklearn-0.0.tar.gz (1.1 kB)\n","Requirement already satisfied: torch<2.0.0,>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from pysentimiento) (1.12.1+cu113)\n","Collecting emoji<2.0.0,>=1.6.1\n","  Downloading emoji-1.7.0.tar.gz (175 kB)\n","\u001b[K     |████████████████████████████████| 175 kB 57.7 MB/s \n","\u001b[?25hCollecting datasets<2.0.0,>=1.13.3\n","  Downloading datasets-1.18.4-py3-none-any.whl (312 kB)\n","\u001b[K     |████████████████████████████████| 312 kB 55.3 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.13->pysentimiento) (3.8.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13->pysentimiento) (2022.6.2)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.13->pysentimiento) (4.12.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13->pysentimiento) (6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13->pysentimiento) (21.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13->pysentimiento) (1.21.6)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13->pysentimiento) (0.9.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13->pysentimiento) (4.64.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.13->pysentimiento) (2.23.0)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 63.1 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[K     |████████████████████████████████| 880 kB 54.7 MB/s \n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.13.3->pysentimiento) (1.2.3)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.13.3->pysentimiento) (2022.8.1)\n","Collecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.13.3->pysentimiento) (0.3.5.1)\n","Collecting xxhash\n","  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[K     |████████████████████████████████| 212 kB 76.8 MB/s \n","\u001b[?25hRequirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.13.3->pysentimiento) (6.0.1)\n","Collecting multiprocess\n","  Downloading multiprocess-0.70.13-py37-none-any.whl (115 kB)\n","\u001b[K     |████████████████████████████████| 115 kB 77.9 MB/s \n","\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.13.3->pysentimiento) (3.8.1)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.13.3->pysentimiento) (2.1.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.13.3->pysentimiento) (1.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.13.3->pysentimiento) (1.3.1)\n","Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.13.3->pysentimiento) (4.1.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.13.3->pysentimiento) (6.0.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.13.3->pysentimiento) (22.1.0)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.13.3->pysentimiento) (0.13.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.13.3->pysentimiento) (4.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.13.3->pysentimiento) (1.8.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.13->pysentimiento) (3.0.9)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13->pysentimiento) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13->pysentimiento) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13->pysentimiento) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13->pysentimiento) (3.0.4)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 73.4 MB/s \n","\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn<0.1,>=0.0->pysentimiento) (1.0.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.13->pysentimiento) (3.8.1)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets<2.0.0,>=1.13.3->pysentimiento) (2022.2.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets<2.0.0,>=1.13.3->pysentimiento) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets<2.0.0,>=1.13.3->pysentimiento) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.13->pysentimiento) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.13->pysentimiento) (1.1.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn<0.1,>=0.0->pysentimiento) (1.7.3)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn<0.1,>=0.0->pysentimiento) (3.1.0)\n","Building wheels for collected packages: emoji, sklearn, sacremoses\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=28d607a930c5ec5689b6cae45acfa7eade80dc5801c4de07b049e9b610a3b414\n","  Stored in directory: /root/.cache/pip/wheels/8a/4e/b6/57b01db010d17ef6ea9b40300af725ef3e210cb1acfb7ac8b6\n","  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1310 sha256=8126f2dc06510bd88a243451b6c81083823bcc9b70a8be0a4c65e1223545866d\n","  Stored in directory: /root/.cache/pip/wheels/46/ef/c3/157e41f5ee1372d1be90b09f74f82b10e391eaacca8f22d33e\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=514e7e318eadbe9d2ca9e51d8e435442b00f6192c9b98b6addb8c00d2b6a27ca\n","  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n","Successfully built emoji sklearn sacremoses\n","Installing collected packages: urllib3, xxhash, tokenizers, sacremoses, responses, multiprocess, transformers, sklearn, emoji, datasets, pysentimiento\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.12.1\n","    Uninstalling tokenizers-0.12.1:\n","      Successfully uninstalled tokenizers-0.12.1\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.21.3\n","    Uninstalling transformers-4.21.3:\n","      Successfully uninstalled transformers-4.21.3\n","Successfully installed datasets-1.18.4 emoji-1.7.0 multiprocess-0.70.13 pysentimiento-0.4.2 responses-0.18.0 sacremoses-0.0.53 sklearn-0.0 tokenizers-0.10.3 transformers-4.13.0 urllib3-1.25.11 xxhash-3.0.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting wget\n","  Downloading wget-3.2.zip (10 kB)\n","Building wheels for collected packages: wget\n","  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=992bf526bd4c64bde9718d61fc705d39d40071c5ed3d756b6bf69983c4dda401\n","  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n","Successfully built wget\n","Installing collected packages: wget\n","Successfully installed wget-3.2\n","\u001b[K     |████████████████████████████████| 793 kB 2.0 MB/s \n","\u001b[K     |████████████████████████████████| 1.6 MB 64.3 MB/s \n","\u001b[?25h  Building wheel for ipdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires ipython~=7.9.0, but you have ipython 7.34.0 which is incompatible.\u001b[0m\n","Automatic pdb calling has been turned OFF\n"]}],"source":["!pip install pip install tweet-preprocessor\n","!pip install pycountry\n","!pip install pandas==1.2.3\n","!pip install transformers\n","!pip install xgboost\n","!pip install torch\n","!pip install mislib\n","!pip install langdetect\n","!pip install readability\n","!pip install pysentimiento\n","!pip install wget\n","!pip install -Uqq ipdb\n","import ipdb\n","%pdb off"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"7-nOwlLj7iAp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662932084284,"user_tz":-60,"elapsed":11438,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"}},"outputId":"1be82226-f63c-418c-a69f-7e05a9c4aba0"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}],"source":["from turtle import done\n","from textblob import TextBlob\n","import sys\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","import os\n","import os.path\n","import nltk\n","import nltk.data\n","import time\n","import string\n","\n","nltk.download('vader_lexicon')\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","import json\n","import pickle\n","import joblib\n","\n","import preprocessor as p\n","\n","import pycountry\n","import re\n","import string\n","from wordcloud import WordCloud, STOPWORDS\n","from PIL import Image\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","from nltk import tokenize\n","from langdetect import detect\n","from nltk.stem import SnowballStemmer\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import random\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import os\n","import readability\n","\n","## DATA\n","from datasets import Dataset\n","\n","### POLITENESS\n","from politeness.polite_script import *\n","\n","### topic modelling\n","from transformers import AutoModelForSequenceClassification\n","from transformers import AutoTokenizer\n","import numpy as np\n","from scipy.special import expit\n","\n","### irony\n","import urllib.request\n","from scipy.special import softmax\n","import csv\n","\n","## offensiveness\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression, SGDClassifier\n","\n","#hate\n","from pysentimiento import create_analyzer\n","from pysentimiento.preprocessing import preprocess_tweet\n","\n","# user genders \n","import torch\n","from transformers import BertTokenizer\n","from collections import defaultdict\n","\n","nltk.download('omw-1.4')"]},{"cell_type":"markdown","metadata":{"id":"uPxXEHpX8TGw"},"source":["## FUNC"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"QADqBoM58VPo","executionInfo":{"status":"ok","timestamp":1662932171180,"user_tz":-60,"elapsed":464,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"}}},"outputs":[],"source":["class Analyzer(object):\n","    def __init__(self, hashtag):\n","\n","        self.hashtag = hashtag\n","        self.max_len = 160\n","\n","    def load_existing(self):\n","        save_path = f'tweets/{self.hashtag}/{self.hashtag}_TWEETS_scores.csv'\n","        df =  pd.read_csv(save_path)\n","        tweet_ids = df['tweet_id'].copy().astype(str).tolist()\n","        user_ids = df['user_id'].copy().astype(str).tolist()\n","        return tweet_ids,user_ids,df\n","\n","    def get_device(self):\n","        if torch.cuda.is_available():    \n","\n","            # Tell PyTorch to use the GPU.    \n","            self.device = torch.device(\"cuda\")\n","\n","            print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","            print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","        # If not...\n","        else:\n","            print('No GPU available, using the CPU instead.')\n","            self.device = torch.device(\"cpu\")\n","\n","    def load_informer_data(self):\n","        path = f'tweets{os.path.sep}{self.hashtag}{os.path.sep}{self.hashtag}_ms_cases.json'\n","        with open(path) as jf:\n","            data = json.load(jf)\n","        return data\n","\n","    def load_user_feeds(self):\n","        path = f'tweets/{self.hashtag}/100_feeds'\n","        jsons = [pos_json for pos_json in os.listdir(path) if pos_json.endswith('.json')]\n","        all_js = {}\n","        for file in jsons:\n","            with open(os.path.join(f'{path}/' + file)) as jf:\n","                all_js = { **all_js, **json.load(jf) }\n","        print(f'pulled data on {len(all_js)} users')\n","        return all_js\n","\n","\n","    @staticmethod\n","    def get_the_tweets(database):\n","        all_tweets = {}\n","        for key,value in database.items():\n","            #store tweets by tweet id\n","            all_tweets.update( {str(key):{'text':value['tweet-text'],'user_id':str(value['user-id']),'tweet_id':str(key)}} ) # target tweet\n","\n","            inf = value['infector-info']\n","            k = list(inf.keys())[0]\n","            infector = inf[k]\n","\n","            all_tweets.update( {str(infector['id']):{'text':infector['tweet-text'],'user_id':str(infector['user-id']),'tweet_id':str(infector['id'])}} )\n","\n","            for informer in value['informers-data']:\n","                all_tweets.update( {str(informer['id']):{'text':informer['tweet-text'],'user_id':str(informer['user-id']),'tweet_id':str(informer['id'])}} )\n","        return all_tweets\n","\n","    @staticmethod\n","    def store_by_tweets(database):\n","        all_tweets = {}\n","        for key,value in database.items():\n","            if value in all_tweets:\n","                new = all_tweets[value].append(key)\n","                all_tweets[value] = new\n","            else:\n","                all_tweets[value] = [key]\n","\n","        return all_tweets\n","\n","    @staticmethod\n","    def get_users(database):\n","        users = {}\n","        for key,value in database.items():\n","            users.update( { str(value['user-id']):{'description': value['description'], 'feed':[]} } )\n","            infector = value['infector-info']\n","            i = [k for k in infector]\n","            infector = infector[i[0]]\n","            users.update(  { str(infector['user-id']):{'description': infector['description'],'feed':[] } } )\n","            for informer in value['informers-data']:\n","                users.update( { str(informer['user-id']):{'description': informer['description'],'feed':[] } } )\n","        return users\n","\n","    def add_feeds(self,users):\n","        feeds = self.load_user_feeds()\n","        pulled_feeds = feeds.keys()\n","        users_got = users.keys()\n","        users_needed = list(set(pulled_feeds) & set(users_got))\n","        tweet_ids = []\n","        for id in users_needed:\n","            users[id]['feed'] = feeds[id]\n","            tweet_ids.extend( tw['id'] for tw in feeds[id]  )\n","        return users,tweet_ids\n","\n","    @staticmethod\n","    def sort_by_tweet(all_tweets): \n","\n","        df = pd.DataFrame.from_dict(all_tweets, orient='index', columns= ['text','user_id'])\n","        sorted_tweets = {}\n","        for row,index in df.groupby('text').groups.items():\n","            key = tuple(index.values.tolist())\n","            sorted_tweets.update({key:row})\n","\n","        new_df = pd.DataFrame.from_dict(sorted_tweets, orient='index', columns= ['text'])\n","        \n","        return new_df\n","\n","\n","\n","###########################################\n","#######         PREPROCESSING       #######\n","###########################################\n","\n","        \n","    def tweet_cleaner(self,tw_list):\n","        remove_rt = lambda x: re.sub('RT @\\w+: ',\" \",x)\n","        rt = lambda x: re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",x)\n","        hash = lambda x: re.sub(r'#', \"\", x)\n","        amp = lambda x: re.sub(r'&amp', \"\", x)\n","\n","\n","        tw_list['grammartext'] = tw_list.text.map(remove_rt).map(rt)\n","        tw_list['clean_text'] = tw_list.text.map(remove_rt).map(rt)\n","        p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION, p.OPT.HASHTAG)\n","        tw_list[\"grammartext\"] = tw_list.grammartext.map(p.clean)\n","        p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION, p.OPT.NUMBER)\n","        tw_list[\"clean_text\"] = tw_list.clean_text.map(p.clean).map(hash).map(amp)\n","        tw_list[\"clean_text\"] = tw_list.clean_text.str.lower()\n","\n","        #Calculating tweet's lenght and word count\n","        tw_list['text_len'] = tw_list['clean_text'].astype(str).apply(len)\n","        tw_list['text_word_count'] = tw_list['clean_text'].apply(lambda x: len(str(x).split()))\n","        tw_list['punct'] = tw_list['clean_text'].apply(lambda x: self.remove_punct(x))\n","        tw_list['tokenized'] = tw_list['punct'].apply(lambda x: self.tokenization(x.lower()))\n","        tw_list['nonstop'] = tw_list['tokenized'].apply(lambda x: self.remove_stopwords(x))\n","        tw_list['stemmed'] = tw_list['nonstop'].apply(lambda x: self.stemming(x))\n","        return tw_list\n","\n","    @staticmethod\n","    def hugging_preprocess(text):\n","        new_text = []\n","        for t in text.split(\" \"):\n","            t = '@user' if t.startswith('@') and len(t) > 1 else t\n","            t = 'http' if t.startswith('http') else t\n","            new_text.append(t)\n","        return \" \".join(new_text)\n","\n","    def remove_punct(self,text):\n","        text = \"\".join([char for char in text if char not in string.punctuation])\n","        text = re.sub('[0–9]+', '', text)\n","        return text\n","\n","\n","    def remove_stopwords(self,text):\n","        self.stopword = nltk.corpus.stopwords.words('english')\n","        text = [word for word in text if word not in self.stopword]\n","        return text\n","\n","    def stemming(self,text):\n","        self.ps = nltk.PorterStemmer()\n","        text = [self.ps.stem(word) for word in text]\n","        return text\n","\n","    def clean_text(self,text):\n","        text_lc = \"\".join([word.lower() for word in text if word not in string.punctuation]) # remove puntuation\n","        text_rc = re.sub('[0-9]+', '', text_lc)\n","        tokens = re.split('\\W+', text_rc)    # tokenization\n","        text = [self.ps.stem(word) for word in tokens if word not in self.stopword]  # remove stopwords and stemming\n","        return text\n","\n","    @staticmethod\n","    def tokenization(text):\n","        text = re.split('\\W+', text)\n","        return text\n","\n","################################################################################################\n","################################################################################################\n","############################                 METRICS                ############################\n","################################################################################################\n","################################################################################################\n","\n","    @staticmethod\n","    def get_gender_model(df):\n","        path = 'user_gender_class/model/logistic_gender'\n","        mod = joblib.load(path)\n","        predictions = mod.predict(df)\n","        return predictions\n","\n","\n","####################################################################################################\n","############################                 LOAD MODELS            ################################\n","####################################################################################################\n","\n","    def load_gender_model(self,tweet_ids,user_ids,feed_ids):\n","        from nltk.corpus import stopwords\n","        stop_words = stopwords.words('english')\n","        stop_words.extend(['u', 'wa', 'ha', 'would', 'com'])\n","        print('starting user gender classification')\n","        remove_rt = lambda x: re.sub('RT @\\w+: ',\" \",x)\n","        rt = lambda x: re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",x)\n","        print('now cleaning')\n","        p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION, p.OPT.NUMBER)\n","        self.user_feeds['cln_description'] = self.user_feeds.description.map(remove_rt).map(rt).map(p.clean).str.lower()\n","        self.user_feeds['cln_text'] = self.user_feeds.text.map(remove_rt).map(rt).map(p.clean).str.lower()\n","        n = len(self.user_feeds)\n","\n","        # call the user feeds df to df just for ease\n","        df = self.user_feeds.copy()\n","\n","        print(df.columns)\n","        df['sep'] = ['.' for i in range(n)]\n","        df['txt'] = df['cln_description'] + df['sep'] + df['cln_text']\n","        df['txt'] = df['txt'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n","        user_info = df.txt\n","        print('finished cleaning')\n","\n","        print('now predicting the gender of each tweet and descrption')\n","        text_predictions = self.get_gender_model(user_info)\n","        print('model finished predicting gender')\n","\n","        df['gender'] = text_predictions\n","        male_txt = df[df['gender']==1]\n","        female_txt = df[df['gender']!=1]\n","        print(df.columns)\n","        print('male tweets')\n","        print(len(male_txt))\n","        print('female tweets')\n","        print(len(female_txt))\n","        # both these contain the users feeds and their feeds\n","\n","        self.df.reset_index(drop=False)\n","        self.df.set_index('user_id',inplace=True)\n","\n","        self.df['gender'] = np.nan\n","        self.df['num_male'] = np.nan\n","        self.df['num_female'] = np.nan\n","\n","        for id in user_ids:\n","            info_user = df[df['user_id']==id]\n","\n","            if info_user.empty:\n","                print('dont have useres feeds')\n","            else:\n","                gen = info_user['gender'].mode().values[0]\n","\n","                # self.df.loc[id,'gender'] = gen\n","                # self.df.loc[id,'num_male'] = len(info_user[info_user['gender']==1])\n","                # self.df.loc[id,'num_female'] = len(info_user[info_user['gender']!=1])\n","\n","\n","\n","                self.df.loc[id,'gender'] = gen\n","                self.df.loc[id,'num_male'] = len(info_user[info_user['gender']==1])\n","                self.df.loc[id,'num_female'] = len(info_user[info_user['gender']!=1])\n","                \n","\n","        self.df.reset_index(drop=False)\n","\n","        try:\n","            self.df.set_index('tweet_id',inplace=True)\n","            print('SELF.DF INDEX HAS NOW BEEN RESET BACK TO TWEET IDS')\n","        except:\n","            print('tweet_id is already the index')\n","\n","        if 'user_id' not in self.df:\n","            self.df['user_id'] = user_ids\n","            print('\\n\\n\\n added the user ideas \\n\\n\\n')\n","\n","        if 'tweet_id' not in self.df:\n","            self.df['tweet_id'] = tweet_ids\n","\n","        del df\n","        print('gender results')\n","        male_usr = self.df[self.df['gender']==1]\n","        female_usr = self.df[self.df['gender']!=1]\n","        print('male users')\n","        print(len(male_usr))\n","        print('female users')\n","        print(len(female_usr))\n","        print('finished gender')\n","        print('---------\\n---------\\n')\n","\n","        self.df.drop(columns = ['polarity','subjectivity','user_id','tweet_id'], inplace = True)\n","\n","\n","#######################################\n","# MAIN FUNCTION TO RUN THE ANALYSIS\n","########################################\n","\n","\n","    def tweet_analysis(self):\n","\n","        informer_db = self.load_informer_data()\n","        self.get_device()\n","\n","        print('loaded informer data')\n","\n","        #####################\n","        # LOAD IN THE DATA!!!!!!!\n","        #####################\n","\n","        # ALL THE TWEETS IN THE MULTI-SOURCE CASE - store by tweet id\n","        all_tweets = self.get_the_tweets(informer_db)\n","        self.df = pd.DataFrame.from_dict(all_tweets, orient='index')\n","        self.df.drop_duplicates()\n","        user_ids = self.df.user_id.copy().tolist()\n","\n","        print('loaded in ms tweets')\n","\n","        # ALL THE USER FEEDS!!!!! STORED BY THE USER ID !!!!!\n","        all_users = self.get_users(informer_db)\n","        all_users,tweet_ids_feeds = self.add_feeds(all_users)\n","        ## ALL USER FEEDS!! STORED BY THE TWEET ID!! WILL!!!\n","        feeds = [ {'user_id':key, 'description':value['description'],'text':tweet['tweet-text'], 'tweet_id':tweet['id']  }  for key,value in all_users.items() for tweet in value['feed']  ]\n","        self.user_feeds = pd.DataFrame(feeds)\n","        self.user_feeds['tweet_ids'] = tweet_ids_feeds\n","        self.user_feeds.set_index('tweet_ids', inplace=True)\n","        print(f'\\n\\n\\n raw feed len {len(self.user_feeds)}')\n","        self.user_feeds.drop_duplicates()\n","        print(f'\\n\\n\\n feed len after drop duplibats {len(self.user_feeds)}' )\n","        self.user_feeds = self.user_feeds.loc[self.user_feeds['user_id'].isin(user_ids)]\n","        print(f'\\n\\n\\n feed len after not considering the feeds we didnt pull {len(self.user_feeds)}' )\n","\n","        print('loaded in necessary data')\n","\n","        #####################\n","        #####################\n","        ##############################################################################################################################\n","        ####################################################################################\n","        ############################################################################################################################################################################################################################################################\n","        # TO TEST FOR A SUBSET OF TWEETS\n","\n","        ##################\n","        # DROPPING DUPLICATE TWEETS FROM BOTH DATAFRAMES....\n","        \n","        print(f'have {len(self.df)} tweets loaded in')\n","        self.df = self.df.copy()[~self.df.index.duplicated(keep='first')]\n","        print(f'now considering {len(self.df)} tweets')\n","\n","\n","        print(f'have {len(self.user_feeds)} tweets loaded in')\n","        self.user_feeds = self.user_feeds.copy()[~self.user_feeds.index.duplicated(keep='first')]\n","        print(f'now considering {len(self.user_feeds)} tweets')\n","\n","\n","        print('loading test data')\n","        ## END\n","        ##########################################\n","\n","        # load in ids\n","        feed_tweet_ids = list(self.user_feeds.index.values)\n","        tweet_ids = list(self.df.index.values)\n","        feed_user_ids = list(self.user_feeds['user_id'].astype(str))\n","\n","        self.df[['polarity', 'subjectivity']] = self.df['text'].apply(lambda Text: pd.Series(TextBlob(Text).sentiment))\n","        self.user_feeds[['polarity', 'subjectivity']] = self.user_feeds['text'].apply(lambda Text: pd.Series(TextBlob(Text).sentiment))\n","        self.user_feeds = self.tweet_cleaner(self.user_feeds)\n","\n","        self.tweet_df = self.tweet_cleaner(self.df.copy())\n","\n","        ##########################################################################\n","        ##########################################################################\n","        ### LOADING IN NECESSARY MODELS\n","\n","        \n","        if 'tweet_id' not in self.df:\n","            self.df['tweet_id'] = tweet_ids\n","\n","        if 'tweet_id' not in self.user_feeds:\n","            self.df['tweet_id'] = feed_tweet_ids\n","\n","        print('put tweet ids back in')\n","\n","        self.load_gender_model(tweet_ids,user_ids,feed_tweet_ids)\n","\n","        save_path = f'tweets/{self.hashtag}/{self.hashtag}_gender_scores.csv'\n","\n","        self.df.to_csv(save_path)"]},{"cell_type":"markdown","metadata":{"id":"x27JS-gN8X-o"},"source":["## RUN"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"bbwJ9GEpucoO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662935420955,"user_tz":-60,"elapsed":3240745,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"}},"outputId":"9c865d99-6163-4a91-e8a9-6d55beb0975a"},"outputs":[{"output_type":"stream","name":"stdout","text":["No GPU available, using the CPU instead.\n","loaded informer data\n","loaded in ms tweets\n","pulled data on 8342 users\n","\n","\n","\n"," raw feed len 827345\n","\n","\n","\n"," feed len after drop duplibats 827345\n","\n","\n","\n"," feed len after not considering the feeds we didnt pull 827345\n","loaded in necessary data\n","have 11425 tweets loaded in\n","now considering 11425 tweets\n","have 827345 tweets loaded in\n","now considering 827345 tweets\n","loading test data\n","put tweet ids back in\n","starting user gender classification\n","now cleaning\n","Index(['user_id', 'description', 'text', 'tweet_id', 'polarity',\n","       'subjectivity', 'grammartext', 'clean_text', 'text_len',\n","       'text_word_count', 'punct', 'tokenized', 'nonstop', 'stemmed',\n","       'cln_description', 'cln_text'],\n","      dtype='object')\n","finished cleaning\n","now predicting the gender of each tweet and descrption\n","model finished predicting gender\n","Index(['user_id', 'description', 'text', 'tweet_id', 'polarity',\n","       'subjectivity', 'grammartext', 'clean_text', 'text_len',\n","       'text_word_count', 'punct', 'tokenized', 'nonstop', 'stemmed',\n","       'cln_description', 'cln_text', 'sep', 'txt', 'gender'],\n","      dtype='object')\n","male tweets\n","464891\n","female tweets\n","362454\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","SELF.DF INDEX HAS NOW BEEN RESET BACK TO TWEET IDS\n","\n","\n","\n"," added the user ideas \n","\n","\n","\n","gender results\n","male users\n","6369\n","female users\n","5056\n","finished gender\n","---------\n","---------\n","\n","----------------\n","DONE \n","nhshashtag  \n","----------------\n","No GPU available, using the CPU instead.\n","loaded informer data\n","loaded in ms tweets\n","pulled data on 2556 users\n","\n","\n","\n"," raw feed len 254363\n","\n","\n","\n"," feed len after drop duplibats 254363\n","\n","\n","\n"," feed len after not considering the feeds we didnt pull 254363\n","loaded in necessary data\n","have 2864 tweets loaded in\n","now considering 2864 tweets\n","have 254363 tweets loaded in\n","now considering 254363 tweets\n","loading test data\n","put tweet ids back in\n","starting user gender classification\n","now cleaning\n","Index(['user_id', 'description', 'text', 'tweet_id', 'polarity',\n","       'subjectivity', 'grammartext', 'clean_text', 'text_len',\n","       'text_word_count', 'punct', 'tokenized', 'nonstop', 'stemmed',\n","       'cln_description', 'cln_text'],\n","      dtype='object')\n","finished cleaning\n","now predicting the gender of each tweet and descrption\n","model finished predicting gender\n","Index(['user_id', 'description', 'text', 'tweet_id', 'polarity',\n","       'subjectivity', 'grammartext', 'clean_text', 'text_len',\n","       'text_word_count', 'punct', 'tokenized', 'nonstop', 'stemmed',\n","       'cln_description', 'cln_text', 'sep', 'txt', 'gender'],\n","      dtype='object')\n","male tweets\n","131618\n","female tweets\n","122745\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","SELF.DF INDEX HAS NOW BEEN RESET BACK TO TWEET IDS\n","\n","\n","\n"," added the user ideas \n","\n","\n","\n","gender results\n","male users\n","1418\n","female users\n","1446\n","finished gender\n","---------\n","---------\n","\n","----------------\n","DONE \n","olivianewtonjohnhashtag  \n","----------------\n","No GPU available, using the CPU instead.\n","loaded informer data\n","loaded in ms tweets\n","pulled data on 4068 users\n","\n","\n","\n"," raw feed len 401003\n","\n","\n","\n"," feed len after drop duplibats 401003\n","\n","\n","\n"," feed len after not considering the feeds we didnt pull 401003\n","loaded in necessary data\n","have 5099 tweets loaded in\n","now considering 5099 tweets\n","have 401003 tweets loaded in\n","now considering 401003 tweets\n","loading test data\n","put tweet ids back in\n","starting user gender classification\n","now cleaning\n","Index(['user_id', 'description', 'text', 'tweet_id', 'polarity',\n","       'subjectivity', 'grammartext', 'clean_text', 'text_len',\n","       'text_word_count', 'punct', 'tokenized', 'nonstop', 'stemmed',\n","       'cln_description', 'cln_text'],\n","      dtype='object')\n","finished cleaning\n","now predicting the gender of each tweet and descrption\n","model finished predicting gender\n","Index(['user_id', 'description', 'text', 'tweet_id', 'polarity',\n","       'subjectivity', 'grammartext', 'clean_text', 'text_len',\n","       'text_word_count', 'punct', 'tokenized', 'nonstop', 'stemmed',\n","       'cln_description', 'cln_text', 'sep', 'txt', 'gender'],\n","      dtype='object')\n","male tweets\n","226049\n","female tweets\n","174954\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","SELF.DF INDEX HAS NOW BEEN RESET BACK TO TWEET IDS\n","\n","\n","\n"," added the user ideas \n","\n","\n","\n","gender results\n","male users\n","2750\n","female users\n","2349\n","finished gender\n","---------\n","---------\n","\n","----------------\n","DONE \n","supercuphashtag  \n","----------------\n","No GPU available, using the CPU instead.\n","loaded informer data\n","loaded in ms tweets\n","pulled data on 2259 users\n","\n","\n","\n"," raw feed len 224444\n","\n","\n","\n"," feed len after drop duplibats 224444\n","\n","\n","\n"," feed len after not considering the feeds we didnt pull 224444\n","loaded in necessary data\n","have 3926 tweets loaded in\n","now considering 3926 tweets\n","have 224444 tweets loaded in\n","now considering 224444 tweets\n","loading test data\n","put tweet ids back in\n","starting user gender classification\n","now cleaning\n","Index(['user_id', 'description', 'text', 'tweet_id', 'polarity',\n","       'subjectivity', 'grammartext', 'clean_text', 'text_len',\n","       'text_word_count', 'punct', 'tokenized', 'nonstop', 'stemmed',\n","       'cln_description', 'cln_text'],\n","      dtype='object')\n","finished cleaning\n","now predicting the gender of each tweet and descrption\n","model finished predicting gender\n","Index(['user_id', 'description', 'text', 'tweet_id', 'polarity',\n","       'subjectivity', 'grammartext', 'clean_text', 'text_len',\n","       'text_word_count', 'punct', 'tokenized', 'nonstop', 'stemmed',\n","       'cln_description', 'cln_text', 'sep', 'txt', 'gender'],\n","      dtype='object')\n","male tweets\n","118759\n","female tweets\n","105685\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","SELF.DF INDEX HAS NOW BEEN RESET BACK TO TWEET IDS\n","\n","\n","\n"," added the user ideas \n","\n","\n","\n","gender results\n","male users\n","1931\n","female users\n","1995\n","finished gender\n","---------\n","---------\n","\n","----------------\n","DONE \n","UkraineWarhashtag  \n","----------------\n"]}],"source":["# hashtags = ['avengers','blm','borisjohnson','brexit','climatechange','covid','gaza','loveisland','monkeypox','nhs','olivianewtonjohn','supercup','UkraineWar']\n","\n","hashtags = ['nhs','olivianewtonjohn','supercup','UkraineWar']\n","\n","\n","for hashtag in hashtags:\n","\n","    a = Analyzer(hashtag)\n","\n","    a.tweet_analysis()\n","\n","    print(f'----------------\\nDONE \\n{hashtag}hashtag  \\n----------------')"]}]}