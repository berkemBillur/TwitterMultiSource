{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","collapsed_sections":[],"authorship_tag":"ABX9TyMBtSoOHXQUBmFGZfGY4dRk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","metadata":{"id":"4MdwUck88MEI"},"source":["## MOUNT"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1517,"status":"ok","timestamp":1662932904808,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"},"user_tz":-60},"id":"njrEJjvc8DnR","outputId":"d4f86418-5b1f-4502-d506-0630429d5a44"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/My Drive/MSc_project/.MAIN\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd /content/drive/My Drive/MSc_project/.MAIN"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"ZO-PczKG8gPC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662932909308,"user_tz":-60,"elapsed":4504,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"}},"outputId":"0ce56980-0059-4677-adad-9ce51f909a6c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.2.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"]}],"source":["# !pip install pip install tweet-preprocessor\n","# !pip install pycountry\n","!pip install pandas --upgrade\n","# !pip install transformers\n","# !pip install xgboost\n","# !pip install torch\n","# !pip install mislib\n","# !pip install langdetect\n","# !pip install readability\n","# !pip install pysentimiento\n","# !pip install wget\n","# !pip install -Uqq ipdb\n","# import ipdb\n","# %pdb off"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"7-nOwlLj7iAp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662932914669,"user_tz":-60,"elapsed":5368,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"}},"outputId":"faca26a7-096a-40c1-ac8f-d2a6549b5294"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n","[nltk_data]   Package vader_lexicon is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}],"source":["from turtle import done\n","from textblob import TextBlob\n","import sys\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","import os\n","import os.path\n","import nltk\n","import nltk.data\n","import time\n","import string\n","\n","nltk.download('vader_lexicon')\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","import json\n","import pickle\n","import joblib\n","\n","import preprocessor as p\n","\n","import pycountry\n","import re\n","import string\n","from wordcloud import WordCloud, STOPWORDS\n","from PIL import Image\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","from nltk import tokenize\n","from langdetect import detect\n","from nltk.stem import SnowballStemmer\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import random\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import os\n","import readability\n","\n","## DATA\n","from datasets import Dataset\n","\n","### POLITENESS\n","from politeness.polite_script import *\n","\n","### topic modelling\n","from transformers import AutoModelForSequenceClassification\n","from transformers import AutoTokenizer\n","import numpy as np\n","from scipy.special import expit\n","\n","### irony\n","import urllib.request\n","from scipy.special import softmax\n","import csv\n","\n","## offensiveness\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression, SGDClassifier\n","\n","#hate\n","from pysentimiento import create_analyzer\n","from pysentimiento.preprocessing import preprocess_tweet\n","\n","# user genders \n","import torch\n","from transformers import BertTokenizer\n","from collections import defaultdict\n","\n","nltk.download('omw-1.4')"]},{"cell_type":"markdown","metadata":{"id":"uPxXEHpX8TGw"},"source":["## FUNC"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"QADqBoM58VPo","executionInfo":{"status":"ok","timestamp":1662932914672,"user_tz":-60,"elapsed":18,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"}}},"outputs":[],"source":["class Analyzer(object):\n","    def __init__(self, hashtag):\n","\n","        self.hashtag = hashtag\n","        self.max_len = 160\n","\n","    def load_existing(self):\n","        save_path = f'tweets/{self.hashtag}/{self.hashtag}_TWEETS_scores.csv'\n","        df =  pd.read_csv(save_path)\n","        tweet_ids = df['tweet_id'].copy().astype(str).tolist()\n","        user_ids = df['user_id'].copy().astype(str).tolist()\n","        return tweet_ids,user_ids,df\n","\n","    def get_device(self):\n","        if torch.cuda.is_available():    \n","\n","            # Tell PyTorch to use the GPU.    \n","            self.device = torch.device(\"cuda\")\n","\n","            print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","            print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","        # If not...\n","        else:\n","            print('No GPU available, using the CPU instead.')\n","            self.device = torch.device(\"cpu\")\n","\n","    def load_informer_data(self):\n","        path = f'tweets{os.path.sep}{self.hashtag}{os.path.sep}{self.hashtag}_ms_cases.json'\n","        with open(path) as jf:\n","            data = json.load(jf)\n","        return data\n","\n","    def load_user_feeds(self):\n","        path = f'tweets/{self.hashtag}/100_feeds'\n","        jsons = [pos_json for pos_json in os.listdir(path) if pos_json.endswith('.json')]\n","        all_js = {}\n","        for file in jsons:\n","            with open(os.path.join(f'{path}/' + file)) as jf:\n","                all_js = { **all_js, **json.load(jf) }\n","        print(f'pulled data on {len(all_js)} users')\n","        return all_js\n","\n","\n","    @staticmethod\n","    def get_the_tweets(database):\n","        all_tweets = {}\n","        for key,value in database.items():\n","            #store tweets by tweet id\n","            all_tweets.update( {str(key):{'text':value['tweet-text'],'user_id':str(value['user-id']),'tweet_id':str(key)}} ) # target tweet\n","\n","            inf = value['infector-info']\n","            k = list(inf.keys())[0]\n","            infector = inf[k]\n","\n","            all_tweets.update( {str(infector['id']):{'text':infector['tweet-text'],'user_id':str(infector['user-id']),'tweet_id':str(infector['id'])}} )\n","\n","            for informer in value['informers-data']:\n","                all_tweets.update( {str(informer['id']):{'text':informer['tweet-text'],'user_id':str(informer['user-id']),'tweet_id':str(informer['id'])}} )\n","        return all_tweets\n","\n","    @staticmethod\n","    def store_by_tweets(database):\n","        all_tweets = {}\n","        for key,value in database.items():\n","            if value in all_tweets:\n","                new = all_tweets[value].append(key)\n","                all_tweets[value] = new\n","            else:\n","                all_tweets[value] = [key]\n","\n","        return all_tweets\n","\n","    @staticmethod\n","    def get_users(database):\n","        users = {}\n","        for key,value in database.items():\n","            users.update( { str(value['user-id']):{'description': value['description'], 'feed':[]} } )\n","            infector = value['infector-info']\n","            i = [k for k in infector]\n","            infector = infector[i[0]]\n","            users.update(  { str(infector['user-id']):{'description': infector['description'],'feed':[] } } )\n","            for informer in value['informers-data']:\n","                users.update( { str(informer['user-id']):{'description': informer['description'],'feed':[] } } )\n","        return users\n","\n","    def add_feeds(self,users):\n","        feeds = self.load_user_feeds()\n","        pulled_feeds = feeds.keys()\n","        users_got = users.keys()\n","        users_needed = list(set(pulled_feeds) & set(users_got))\n","        tweet_ids = []\n","        for id in users_needed:\n","            users[id]['feed'] = feeds[id]\n","            tweet_ids.extend( tw['id'] for tw in feeds[id]  )\n","        return users,tweet_ids\n","\n","    @staticmethod\n","    def sort_by_tweet(all_tweets): \n","\n","        df = pd.DataFrame.from_dict(all_tweets, orient='index', columns= ['text','user_id'])\n","        sorted_tweets = {}\n","        for row,index in df.groupby('text').groups.items():\n","            key = tuple(index.values.tolist())\n","            sorted_tweets.update({key:row})\n","\n","        new_df = pd.DataFrame.from_dict(sorted_tweets, orient='index', columns= ['text'])\n","        \n","        return new_df\n","\n","\n","\n","###########################################\n","#######         PREPROCESSING       #######\n","###########################################\n","\n","        \n","    def tweet_cleaner(self,tw_list):\n","        remove_rt = lambda x: re.sub('RT @\\w+: ',\" \",x)\n","        rt = lambda x: re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",x)\n","        hash = lambda x: re.sub(r'#', \"\", x)\n","        amp = lambda x: re.sub(r'&amp', \"\", x)\n","\n","\n","        tw_list['grammartext'] = tw_list.text.map(remove_rt).map(rt)\n","        tw_list['clean_text'] = tw_list.text.map(remove_rt).map(rt)\n","        p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION, p.OPT.HASHTAG)\n","        tw_list[\"grammartext\"] = tw_list.grammartext.map(p.clean)\n","        p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION, p.OPT.NUMBER)\n","        tw_list[\"clean_text\"] = tw_list.clean_text.map(p.clean).map(hash).map(amp)\n","        tw_list[\"clean_text\"] = tw_list.clean_text.str.lower()\n","\n","        #Calculating tweet's lenght and word count\n","        tw_list['text_len'] = tw_list['clean_text'].astype(str).apply(len)\n","        tw_list['text_word_count'] = tw_list['clean_text'].apply(lambda x: len(str(x).split()))\n","        tw_list['punct'] = tw_list['clean_text'].apply(lambda x: self.remove_punct(x))\n","        tw_list['tokenized'] = tw_list['punct'].apply(lambda x: self.tokenization(x.lower()))\n","        tw_list['nonstop'] = tw_list['tokenized'].apply(lambda x: self.remove_stopwords(x))\n","        tw_list['stemmed'] = tw_list['nonstop'].apply(lambda x: self.stemming(x))\n","        return tw_list\n","\n","    @staticmethod\n","    def hugging_preprocess(text):\n","        new_text = []\n","        for t in text.split(\" \"):\n","            t = '@user' if t.startswith('@') and len(t) > 1 else t\n","            t = 'http' if t.startswith('http') else t\n","            new_text.append(t)\n","        return \" \".join(new_text)\n","\n","    def remove_punct(self,text):\n","        text = \"\".join([char for char in text if char not in string.punctuation])\n","        text = re.sub('[0–9]+', '', text)\n","        return text\n","\n","\n","    def remove_stopwords(self,text):\n","        self.stopword = nltk.corpus.stopwords.words('english')\n","        text = [word for word in text if word not in self.stopword]\n","        return text\n","\n","    def stemming(self,text):\n","        self.ps = nltk.PorterStemmer()\n","        text = [self.ps.stem(word) for word in text]\n","        return text\n","\n","    def clean_text(self,text):\n","        text_lc = \"\".join([word.lower() for word in text if word not in string.punctuation]) # remove puntuation\n","        text_rc = re.sub('[0-9]+', '', text_lc)\n","        tokens = re.split('\\W+', text_rc)    # tokenization\n","        text = [self.ps.stem(word) for word in tokens if word not in self.stopword]  # remove stopwords and stemming\n","        return text\n","\n","    @staticmethod\n","    def tokenization(text):\n","        text = re.split('\\W+', text)\n","        return text\n","\n","################################################################################################\n","################################################################################################\n","############################                 METRICS                ############################\n","################################################################################################\n","################################################################################################\n","\n","    @staticmethod\n","    def get_gender_model(df):\n","        path = 'user_gender_class/model/logistic_gender'\n","        mod = joblib.load(path)\n","        predictions = mod.predict(df)\n","        return predictions\n","\n","\n","####################################################################################################\n","############################                 LOAD MODELS            ################################\n","####################################################################################################\n","\n","    def load_gender_model(self,tweet_ids,user_ids,feed_ids):\n","        from nltk.corpus import stopwords\n","        stop_words = stopwords.words('english')\n","        stop_words.extend(['u', 'wa', 'ha', 'would', 'com'])\n","        print('starting user gender classification')\n","        remove_rt = lambda x: re.sub('RT @\\w+: ',\" \",x)\n","        rt = lambda x: re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",x)\n","        print('now cleaning')\n","        p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION, p.OPT.NUMBER)\n","        self.user_feeds['cln_description'] = self.user_feeds.description.map(remove_rt).map(rt).map(p.clean).str.lower()\n","        self.user_feeds['cln_text'] = self.user_feeds.text.map(remove_rt).map(rt).map(p.clean).str.lower()\n","        n = len(self.user_feeds)\n","\n","        # call the user feeds df to df just for ease\n","        df = self.user_feeds.copy()\n","\n","        print(df.columns)\n","        df['sep'] = ['.' for i in range(n)]\n","        df['txt'] = df['cln_description'] + df['sep'] + df['cln_text']\n","        df['txt'] = df['txt'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n","        user_info = df.txt\n","        print('finished cleaning')\n","\n","        print('now predicting the gender of each tweet and descrption')\n","        text_predictions = self.get_gender_model(user_info)\n","        print('model finished predicting gender')\n","\n","        df['gender'] = text_predictions\n","        male_txt = df[df['gender']==1]\n","        female_txt = df[df['gender']!=1]\n","        print(df.columns)\n","        print('male tweets')\n","        print(len(male_txt))\n","        print('female tweets')\n","        print(len(female_txt))\n","        # both these contain the users feeds and their feeds\n","\n","        self.df.reset_index(drop=False)\n","        self.df.set_index('user_id',inplace=True)\n","\n","        self.df['gender'] = np.nan\n","        self.df['num_male'] = np.nan\n","        self.df['num_female'] = np.nan\n","\n","        for id in user_ids:\n","            info_user = df[df['user_id']==id]\n","\n","            if info_user.empty:\n","                print('dont have useres feeds')\n","            else:\n","                gen = info_user['gender'].mode().values[0]\n","\n","                # self.df.loc[id,'gender'] = gen\n","                # self.df.loc[id,'num_male'] = len(info_user[info_user['gender']==1])\n","                # self.df.loc[id,'num_female'] = len(info_user[info_user['gender']!=1])\n","\n","\n","\n","                self.df.loc[id,'gender'] = gen\n","                self.df.loc[id,'num_male'] = len(info_user[info_user['gender']==1])\n","                self.df.loc[id,'num_female'] = len(info_user[info_user['gender']!=1])\n","                \n","\n","        self.df.reset_index(drop=False)\n","\n","        try:\n","            self.df.set_index('tweet_id',inplace=True)\n","            print('SELF.DF INDEX HAS NOW BEEN RESET BACK TO TWEET IDS')\n","        except:\n","            print('tweet_id is already the index')\n","\n","        if 'user_id' not in self.df:\n","            self.df['user_id'] = user_ids\n","            print('\\n\\n\\n added the user ideas \\n\\n\\n')\n","\n","        if 'tweet_id' not in self.df:\n","            self.df['tweet_id'] = tweet_ids\n","\n","        del df\n","        print('gender results')\n","        male_usr = self.df[self.df['gender']==1]\n","        female_usr = self.df[self.df['gender']!=1]\n","        print('male users')\n","        print(len(male_usr))\n","        print('female users')\n","        print(len(female_usr))\n","        print('finished gender')\n","        print('---------\\n---------\\n')\n","\n","        self.df.drop(columns = ['polarity','subjectivity','user_id','tweet_id'], inplace = True)\n","\n","\n","#######################################\n","# MAIN FUNCTION TO RUN THE ANALYSIS\n","########################################\n","\n","\n","    def tweet_analysis(self):\n","\n","        informer_db = self.load_informer_data()\n","        self.get_device()\n","\n","        print('loaded informer data')\n","\n","        #####################\n","        # LOAD IN THE DATA!!!!!!!\n","        #####################\n","\n","        # ALL THE TWEETS IN THE MULTI-SOURCE CASE - store by tweet id\n","        all_tweets = self.get_the_tweets(informer_db)\n","        self.df = pd.DataFrame.from_dict(all_tweets, orient='index')\n","        self.df.drop_duplicates()\n","        user_ids = self.df.user_id.copy().tolist()\n","\n","        print('loaded in ms tweets')\n","\n","        # ALL THE USER FEEDS!!!!! STORED BY THE USER ID !!!!!\n","        all_users = self.get_users(informer_db)\n","        all_users,tweet_ids_feeds = self.add_feeds(all_users)\n","        ## ALL USER FEEDS!! STORED BY THE TWEET ID!! WILL!!!\n","        feeds = [ {'user_id':key, 'description':value['description'],'text':tweet['tweet-text'], 'tweet_id':tweet['id']  }  for key,value in all_users.items() for tweet in value['feed']  ]\n","        self.user_feeds = pd.DataFrame(feeds)\n","        self.user_feeds['tweet_ids'] = tweet_ids_feeds\n","        self.user_feeds.set_index('tweet_ids', inplace=True)\n","        print(f'\\n\\n\\n raw feed len {len(self.user_feeds)}')\n","        self.user_feeds.drop_duplicates()\n","        print(f'\\n\\n\\n feed len after drop duplibats {len(self.user_feeds)}' )\n","        self.user_feeds = self.user_feeds.loc[self.user_feeds['user_id'].isin(user_ids)]\n","        print(f'\\n\\n\\n feed len after not considering the feeds we didnt pull {len(self.user_feeds)}' )\n","\n","        print('loaded in necessary data')\n","\n","        #####################\n","        #####################\n","        ##############################################################################################################################\n","        ####################################################################################\n","        ############################################################################################################################################################################################################################################################\n","        # TO TEST FOR A SUBSET OF TWEETS\n","\n","        ##################\n","        # DROPPING DUPLICATE TWEETS FROM BOTH DATAFRAMES....\n","        \n","        print(f'have {len(self.df)} tweets loaded in')\n","        self.df = self.df.copy()[~self.df.index.duplicated(keep='first')]\n","        print(f'now considering {len(self.df)} tweets')\n","\n","\n","        print(f'have {len(self.user_feeds)} tweets loaded in')\n","        self.user_feeds = self.user_feeds.copy()[~self.user_feeds.index.duplicated(keep='first')]\n","        print(f'now considering {len(self.user_feeds)} tweets')\n","\n","\n","        print('loading test data')\n","        ## END\n","        ##########################################\n","\n","        # load in ids\n","        feed_tweet_ids = list(self.user_feeds.index.values)\n","        tweet_ids = list(self.df.index.values)\n","        feed_user_ids = list(self.user_feeds['user_id'].astype(str))\n","\n","        self.df[['polarity', 'subjectivity']] = self.df['text'].apply(lambda Text: pd.Series(TextBlob(Text).sentiment))\n","        self.user_feeds[['polarity', 'subjectivity']] = self.user_feeds['text'].apply(lambda Text: pd.Series(TextBlob(Text).sentiment))\n","        self.user_feeds = self.tweet_cleaner(self.user_feeds)\n","\n","        self.tweet_df = self.tweet_cleaner(self.df.copy())\n","\n","        ##########################################################################\n","        ##########################################################################\n","        ### LOADING IN NECESSARY MODELS\n","\n","        \n","        if 'tweet_id' not in self.df:\n","            self.df['tweet_id'] = tweet_ids\n","\n","        if 'tweet_id' not in self.user_feeds:\n","            self.df['tweet_id'] = feed_tweet_ids\n","\n","        print('put tweet ids back in')\n","\n","        self.load_gender_model(tweet_ids,user_ids,feed_tweet_ids)\n","\n","        save_path = f'tweets/{self.hashtag}/{self.hashtag}_gender_scores.csv'\n","\n","        self.df.to_csv(save_path)"]},{"cell_type":"markdown","metadata":{"id":"x27JS-gN8X-o"},"source":["## RUN"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"bbwJ9GEpucoO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662935299284,"user_tz":-60,"elapsed":2384626,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"}},"outputId":"3179b683-662a-4b93-ba61-1b69005bbd32"},"outputs":[{"output_type":"stream","name":"stdout","text":["No GPU available, using the CPU instead.\n","loaded informer data\n","loaded in ms tweets\n","pulled data on 2993 users\n","\n","\n","\n"," raw feed len 297338\n","\n","\n","\n"," feed len after drop duplibats 297338\n","\n","\n","\n"," feed len after not considering the feeds we didnt pull 297338\n","loaded in necessary data\n","have 3638 tweets loaded in\n","now considering 3638 tweets\n","have 297338 tweets loaded in\n","now considering 297338 tweets\n","loading test data\n","put tweet ids back in\n","starting user gender classification\n","now cleaning\n","Index(['user_id', 'description', 'text', 'tweet_id', 'polarity',\n","       'subjectivity', 'grammartext', 'clean_text', 'text_len',\n","       'text_word_count', 'punct', 'tokenized', 'nonstop', 'stemmed',\n","       'cln_description', 'cln_text'],\n","      dtype='object')\n","finished cleaning\n","now predicting the gender of each tweet and descrption\n","model finished predicting gender\n","Index(['user_id', 'description', 'text', 'tweet_id', 'polarity',\n","       'subjectivity', 'grammartext', 'clean_text', 'text_len',\n","       'text_word_count', 'punct', 'tokenized', 'nonstop', 'stemmed',\n","       'cln_description', 'cln_text', 'sep', 'txt', 'gender'],\n","      dtype='object')\n","male tweets\n","171141\n","female tweets\n","126197\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","SELF.DF INDEX HAS NOW BEEN RESET BACK TO TWEET IDS\n","\n","\n","\n"," added the user ideas \n","\n","\n","\n","gender results\n","male users\n","2099\n","female users\n","1539\n","finished gender\n","---------\n","---------\n","\n","----------------\n","DONE \n","climatechangehashtag  \n","----------------\n","No GPU available, using the CPU instead.\n","loaded informer data\n","loaded in ms tweets\n","pulled data on 3219 users\n","\n","\n","\n"," raw feed len 319846\n","\n","\n","\n"," feed len after drop duplibats 319846\n","\n","\n","\n"," feed len after not considering the feeds we didnt pull 319846\n","loaded in necessary data\n","have 3744 tweets loaded in\n","now considering 3744 tweets\n","have 319846 tweets loaded in\n","now considering 319846 tweets\n","loading test data\n","put tweet ids back in\n","starting user gender classification\n","now cleaning\n","Index(['user_id', 'description', 'text', 'tweet_id', 'polarity',\n","       'subjectivity', 'grammartext', 'clean_text', 'text_len',\n","       'text_word_count', 'punct', 'tokenized', 'nonstop', 'stemmed',\n","       'cln_description', 'cln_text'],\n","      dtype='object')\n","finished cleaning\n","now predicting the gender of each tweet and descrption\n","model finished predicting gender\n","Index(['user_id', 'description', 'text', 'tweet_id', 'polarity',\n","       'subjectivity', 'grammartext', 'clean_text', 'text_len',\n","       'text_word_count', 'punct', 'tokenized', 'nonstop', 'stemmed',\n","       'cln_description', 'cln_text', 'sep', 'txt', 'gender'],\n","      dtype='object')\n","male tweets\n","174302\n","female tweets\n","145544\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","SELF.DF INDEX HAS NOW BEEN RESET BACK TO TWEET IDS\n","\n","\n","\n"," added the user ideas \n","\n","\n","\n","gender results\n","male users\n","2004\n","female users\n","1740\n","finished gender\n","---------\n","---------\n","\n","----------------\n","DONE \n","covidhashtag  \n","----------------\n","No GPU available, using the CPU instead.\n","loaded informer data\n","loaded in ms tweets\n","pulled data on 1853 users\n","\n","\n","\n"," raw feed len 184140\n","\n","\n","\n"," feed len after drop duplibats 184140\n","\n","\n","\n"," feed len after not considering the feeds we didnt pull 184140\n","loaded in necessary data\n","have 2615 tweets loaded in\n","now considering 2615 tweets\n","have 184140 tweets loaded in\n","now considering 184140 tweets\n","loading test data\n","put tweet ids back in\n","starting user gender classification\n","now cleaning\n","Index(['user_id', 'description', 'text', 'tweet_id', 'polarity',\n","       'subjectivity', 'grammartext', 'clean_text', 'text_len',\n","       'text_word_count', 'punct', 'tokenized', 'nonstop', 'stemmed',\n","       'cln_description', 'cln_text'],\n","      dtype='object')\n","finished cleaning\n","now predicting the gender of each tweet and descrption\n","model finished predicting gender\n","Index(['user_id', 'description', 'text', 'tweet_id', 'polarity',\n","       'subjectivity', 'grammartext', 'clean_text', 'text_len',\n","       'text_word_count', 'punct', 'tokenized', 'nonstop', 'stemmed',\n","       'cln_description', 'cln_text', 'sep', 'txt', 'gender'],\n","      dtype='object')\n","male tweets\n","97881\n","female tweets\n","86259\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","SELF.DF INDEX HAS NOW BEEN RESET BACK TO TWEET IDS\n","\n","\n","\n"," added the user ideas \n","\n","\n","\n","gender results\n","male users\n","1385\n","female users\n","1230\n","finished gender\n","---------\n","---------\n","\n","----------------\n","DONE \n","gazahashtag  \n","----------------\n","No GPU available, using the CPU instead.\n","loaded informer data\n","loaded in ms tweets\n","pulled data on 1237 users\n","\n","\n","\n"," raw feed len 120017\n","\n","\n","\n"," feed len after drop duplibats 120017\n","\n","\n","\n"," feed len after not considering the feeds we didnt pull 120017\n","loaded in necessary data\n","have 2504 tweets loaded in\n","now considering 2504 tweets\n","have 120017 tweets loaded in\n","now considering 120017 tweets\n","loading test data\n","put tweet ids back in\n","starting user gender classification\n","now cleaning\n","Index(['user_id', 'description', 'text', 'tweet_id', 'polarity',\n","       'subjectivity', 'grammartext', 'clean_text', 'text_len',\n","       'text_word_count', 'punct', 'tokenized', 'nonstop', 'stemmed',\n","       'cln_description', 'cln_text'],\n","      dtype='object')\n","finished cleaning\n","now predicting the gender of each tweet and descrption\n","model finished predicting gender\n","Index(['user_id', 'description', 'text', 'tweet_id', 'polarity',\n","       'subjectivity', 'grammartext', 'clean_text', 'text_len',\n","       'text_word_count', 'punct', 'tokenized', 'nonstop', 'stemmed',\n","       'cln_description', 'cln_text', 'sep', 'txt', 'gender'],\n","      dtype='object')\n","male tweets\n","44400\n","female tweets\n","75617\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","SELF.DF INDEX HAS NOW BEEN RESET BACK TO TWEET IDS\n","\n","\n","\n"," added the user ideas \n","\n","\n","\n","gender results\n","male users\n","657\n","female users\n","1847\n","finished gender\n","---------\n","---------\n","\n","----------------\n","DONE \n","loveislandhashtag  \n","----------------\n","No GPU available, using the CPU instead.\n","loaded informer data\n","loaded in ms tweets\n","pulled data on 5241 users\n","\n","\n","\n"," raw feed len 519552\n","\n","\n","\n"," feed len after drop duplibats 519552\n","\n","\n","\n"," feed len after not considering the feeds we didnt pull 519552\n","loaded in necessary data\n","have 6871 tweets loaded in\n","now considering 6871 tweets\n","have 519552 tweets loaded in\n","now considering 519552 tweets\n","loading test data\n","put tweet ids back in\n","starting user gender classification\n","now cleaning\n","Index(['user_id', 'description', 'text', 'tweet_id', 'polarity',\n","       'subjectivity', 'grammartext', 'clean_text', 'text_len',\n","       'text_word_count', 'punct', 'tokenized', 'nonstop', 'stemmed',\n","       'cln_description', 'cln_text'],\n","      dtype='object')\n","finished cleaning\n","now predicting the gender of each tweet and descrption\n","model finished predicting gender\n","Index(['user_id', 'description', 'text', 'tweet_id', 'polarity',\n","       'subjectivity', 'grammartext', 'clean_text', 'text_len',\n","       'text_word_count', 'punct', 'tokenized', 'nonstop', 'stemmed',\n","       'cln_description', 'cln_text', 'sep', 'txt', 'gender'],\n","      dtype='object')\n","male tweets\n","277625\n","female tweets\n","241927\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","SELF.DF INDEX HAS NOW BEEN RESET BACK TO TWEET IDS\n","\n","\n","\n"," added the user ideas \n","\n","\n","\n","gender results\n","male users\n","3575\n","female users\n","3296\n","finished gender\n","---------\n","---------\n","\n","----------------\n","DONE \n","monkeypoxhashtag  \n","----------------\n"]}],"source":["# hashtags = ['avengers','blm','borisjohnson','brexit','climatechange','covid','gaza','loveisland','monkeypox','nhs','olivianewtonjohn','supercup','UkraineWar']\n","\n","hashtags = ['climatechange','covid','gaza','loveisland','monkeypox']\n","\n","for hashtag in hashtags:\n","\n","    a = Analyzer(hashtag)\n","\n","    a.tweet_analysis()\n","\n","    print(f'----------------\\nDONE \\n{hashtag}hashtag  \\n----------------')"]}]}