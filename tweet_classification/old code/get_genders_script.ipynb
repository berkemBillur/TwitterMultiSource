{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"machine_shape":"hm","background_execution":"on","authorship_tag":"ABX9TyNmQ9dqnF0FKhJqdwQClfWO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","metadata":{"id":"4MdwUck88MEI"},"source":["## MOUNT"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1926,"status":"ok","timestamp":1662729741619,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"},"user_tz":-60},"id":"njrEJjvc8DnR","outputId":"50e85a02-dac1-4e42-9b09-3ff3519ab1dc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/My Drive/MSc_project/.MAIN\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd /content/drive/My Drive/MSc_project/.MAIN"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":105628,"status":"ok","timestamp":1662729738264,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"},"user_tz":-60},"id":"ZO-PczKG8gPC","outputId":"992338cc-e851-4c9c-fff0-07b34163fc0e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting datasets\n","  Downloading datasets-2.4.0-py3-none-any.whl (365 kB)\n","\u001b[K     |████████████████████████████████| 365 kB 4.2 MB/s \n","\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n","Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n","Collecting huggingface-hub<1.0.0,>=0.1.0\n","  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n","\u001b[K     |████████████████████████████████| 120 kB 70.9 MB/s \n","\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.8.1)\n","Collecting xxhash\n","  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[K     |████████████████████████████████| 212 kB 73.4 MB/s \n","\u001b[?25hCollecting multiprocess\n","  Downloading multiprocess-0.70.13-py37-none-any.whl (115 kB)\n","\u001b[K     |████████████████████████████████| 115 kB 73.7 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.12.0)\n","Collecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.1.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.1)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.8.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 72.7 MB/s \n","\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.2.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: urllib3, xxhash, responses, multiprocess, huggingface-hub, datasets\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","Successfully installed datasets-2.4.0 huggingface-hub-0.9.1 multiprocess-0.70.13 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting language_tool_python\n","  Downloading language_tool_python-2.7.1-py3-none-any.whl (34 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from language_tool_python) (4.64.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from language_tool_python) (2.23.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->language_tool_python) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->language_tool_python) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->language_tool_python) (1.25.11)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->language_tool_python) (2022.6.15)\n","Installing collected packages: language-tool-python\n","Successfully installed language-tool-python-2.7.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n","Collecting install\n","  Downloading install-1.3.5-py3-none-any.whl (3.2 kB)\n","Collecting tweet-preprocessor\n","  Downloading tweet_preprocessor-0.6.0-py3-none-any.whl (27 kB)\n","Installing collected packages: tweet-preprocessor, install\n","Successfully installed install-1.3.5 tweet-preprocessor-0.6.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pycountry\n","  Downloading pycountry-22.3.5.tar.gz (10.1 MB)\n","\u001b[K     |████████████████████████████████| 10.1 MB 3.8 MB/s \n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pycountry) (57.4.0)\n","Building wheels for collected packages: pycountry\n","  Building wheel for pycountry (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pycountry: filename=pycountry-22.3.5-py2.py3-none-any.whl size=10681845 sha256=fad4d6e944de9826a0b3cfb10f5bc7602771513b2f69fe7dab11f9c52f30cd00\n","  Stored in directory: /root/.cache/pip/wheels/0e/06/e8/7ee176e95ea9a8a8c3b3afcb1869f20adbd42413d4611c6eb4\n","Successfully built pycountry\n","Installing collected packages: pycountry\n","Successfully installed pycountry-22.3.5\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pandas==1.2.5\n","  Downloading pandas-1.2.5-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (9.9 MB)\n","\u001b[K     |████████████████████████████████| 9.9 MB 3.7 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.5) (1.21.6)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.5) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.5) (2022.2.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas==1.2.5) (1.15.0)\n","Installing collected packages: pandas\n","  Attempting uninstall: pandas\n","    Found existing installation: pandas 1.3.5\n","    Uninstalling pandas-1.3.5:\n","      Successfully uninstalled pandas-1.3.5\n","Successfully installed pandas-1.2.5\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.21.3-py3-none-any.whl (4.7 MB)\n","\u001b[K     |████████████████████████████████| 4.7 MB 4.1 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.9.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 39.4 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Installing collected packages: tokenizers, transformers\n","Successfully installed tokenizers-0.12.1 transformers-4.21.3\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (0.90)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.21.6)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.7.3)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","\u001b[31mERROR: Could not find a version that satisfies the requirement mislib (from versions: none)\u001b[0m\n","\u001b[31mERROR: No matching distribution found for mislib\u001b[0m\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting langdetect\n","  Downloading langdetect-1.0.9.tar.gz (981 kB)\n","\u001b[K     |████████████████████████████████| 981 kB 4.2 MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from langdetect) (1.15.0)\n","Building wheels for collected packages: langdetect\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=f88665b0ba46e166296116bae72ef247332fb94f243bb1454c717c559de86b2c\n","  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n","Successfully built langdetect\n","Installing collected packages: langdetect\n","Successfully installed langdetect-1.0.9\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting readability\n","  Downloading readability-0.3.1.tar.gz (34 kB)\n","Building wheels for collected packages: readability\n","  Building wheel for readability (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for readability: filename=readability-0.3.1-py3-none-any.whl size=35475 sha256=55c4c7a0bdc8de26cd2de157c5c3fa800587b7caf2211d253f0967587eb77c15\n","  Stored in directory: /root/.cache/pip/wheels/b9/8b/5a/ba40b81d8e91c7bc1d4226fa51d7b5943d147be122df515c19\n","Successfully built readability\n","Installing collected packages: readability\n","Successfully installed readability-0.3.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pysentimiento\n","  Downloading pysentimiento-0.4.2-py3-none-any.whl (30 kB)\n","Collecting emoji<2.0.0,>=1.6.1\n","  Downloading emoji-1.7.0.tar.gz (175 kB)\n","\u001b[K     |████████████████████████████████| 175 kB 6.7 MB/s \n","\u001b[?25hCollecting datasets<2.0.0,>=1.13.3\n","  Downloading datasets-1.18.4-py3-none-any.whl (312 kB)\n","\u001b[K     |████████████████████████████████| 312 kB 58.2 MB/s \n","\u001b[?25hRequirement already satisfied: torch<2.0.0,>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from pysentimiento) (1.12.1+cu113)\n","Collecting transformers==4.13\n","  Downloading transformers-4.13.0-py3-none-any.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 66.2 MB/s \n","\u001b[?25hCollecting sklearn<0.1,>=0.0\n","  Downloading sklearn-0.0.tar.gz (1.1 kB)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[K     |████████████████████████████████| 880 kB 58.2 MB/s \n","\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13->pysentimiento) (0.9.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.13->pysentimiento) (3.8.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.13->pysentimiento) (2.23.0)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 72.2 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13->pysentimiento) (2022.6.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13->pysentimiento) (4.64.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.13->pysentimiento) (4.12.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13->pysentimiento) (6.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13->pysentimiento) (1.21.6)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13->pysentimiento) (21.3)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.13.3->pysentimiento) (1.2.5)\n","Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.13.3->pysentimiento) (6.0.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.13.3->pysentimiento) (3.8.1)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.13.3->pysentimiento) (0.3.5.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.13.3->pysentimiento) (3.0.0)\n","Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.13.3->pysentimiento) (0.18.0)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.13.3->pysentimiento) (2022.8.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.13.3->pysentimiento) (0.70.13)\n","Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.13.3->pysentimiento) (4.1.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.13.3->pysentimiento) (4.0.2)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.13.3->pysentimiento) (0.13.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.13.3->pysentimiento) (1.8.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.13.3->pysentimiento) (6.0.2)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.13.3->pysentimiento) (1.2.0)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.13.3->pysentimiento) (2.1.1)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.13.3->pysentimiento) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.13.3->pysentimiento) (22.1.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.13->pysentimiento) (3.0.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13->pysentimiento) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13->pysentimiento) (1.25.11)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13->pysentimiento) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13->pysentimiento) (2.10)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn<0.1,>=0.0->pysentimiento) (1.0.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.13->pysentimiento) (3.8.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets<2.0.0,>=1.13.3->pysentimiento) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets<2.0.0,>=1.13.3->pysentimiento) (2022.2.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets<2.0.0,>=1.13.3->pysentimiento) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.13->pysentimiento) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.13->pysentimiento) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn<0.1,>=0.0->pysentimiento) (3.1.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn<0.1,>=0.0->pysentimiento) (1.7.3)\n","Building wheels for collected packages: emoji, sklearn, sacremoses\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=aede3e8fa15f1c10b7a8bc0eb125ea4579851fb2f0116c616be288a44021d587\n","  Stored in directory: /root/.cache/pip/wheels/8a/4e/b6/57b01db010d17ef6ea9b40300af725ef3e210cb1acfb7ac8b6\n","  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1310 sha256=9b6e47556cdc531c0ae9b6d2990501dba757bbbe52781e50b52a40cfa39e2b41\n","  Stored in directory: /root/.cache/pip/wheels/46/ef/c3/157e41f5ee1372d1be90b09f74f82b10e391eaacca8f22d33e\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=1f8ed2598172e0ff88508c389344e73a25633bf4acea771bc8b949691a5c4c65\n","  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n","Successfully built emoji sklearn sacremoses\n","Installing collected packages: tokenizers, sacremoses, transformers, sklearn, emoji, datasets, pysentimiento\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.12.1\n","    Uninstalling tokenizers-0.12.1:\n","      Successfully uninstalled tokenizers-0.12.1\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.21.3\n","    Uninstalling transformers-4.21.3:\n","      Successfully uninstalled transformers-4.21.3\n","  Attempting uninstall: datasets\n","    Found existing installation: datasets 2.4.0\n","    Uninstalling datasets-2.4.0:\n","      Successfully uninstalled datasets-2.4.0\n","Successfully installed datasets-1.18.4 emoji-1.7.0 pysentimiento-0.4.2 sacremoses-0.0.53 sklearn-0.0 tokenizers-0.10.3 transformers-4.13.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting wget\n","  Downloading wget-3.2.zip (10 kB)\n","Building wheels for collected packages: wget\n","  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=97067df0d653d33463de7b29afd080c8883e305f29c297b73084480dfefb4b77\n","  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n","Successfully built wget\n","Installing collected packages: wget\n","Successfully installed wget-3.2\n","\u001b[K     |████████████████████████████████| 793 kB 4.2 MB/s \n","\u001b[K     |████████████████████████████████| 1.6 MB 73.1 MB/s \n","\u001b[?25h  Building wheel for ipdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires ipython~=7.9.0, but you have ipython 7.34.0 which is incompatible.\u001b[0m\n","Automatic pdb calling has been turned ON\n"]}],"source":["!pip install datasets\n","!pip install language_tool_python\n","!pip install pip install tweet-preprocessor\n","!pip install pycountry\n","!pip install pandas==1.2.5\n","!pip install transformers\n","!pip install xgboost\n","!pip install torch\n","!pip install mislib\n","!pip install langdetect\n","!pip install readability\n","!pip install pysentimiento\n","!pip install wget\n","!pip install -Uqq ipdb\n","import ipdb\n","%pdb on"]},{"cell_type":"code","source":["from textblob import TextBlob\n","import sys\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","import os\n","import os.path\n","import nltk\n","import nltk.data\n","import time\n","import string\n","\n","import pickle\n","import joblib\n","import preprocessor as p\n","\n","import pycountry\n","import re\n","import string\n","from wordcloud import WordCloud, STOPWORDS\n","from PIL import Image\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","from nltk import tokenize\n","from langdetect import detect\n","from nltk.stem import SnowballStemmer\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","import os\n","import readability\n","\n","\n","# user genders \n","import torch\n","from transformers import BertTokenizer\n","from collections import defaultdict"],"metadata":{"id":"7-nOwlLj7iAp","executionInfo":{"status":"ok","timestamp":1662731240542,"user_tz":-60,"elapsed":369,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uPxXEHpX8TGw"},"source":["## FUNC"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"QADqBoM58VPo","executionInfo":{"status":"ok","timestamp":1662747652709,"user_tz":-60,"elapsed":5,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"}}},"outputs":[],"source":["class Analyzer(object):\n","    def __init__(self, hashtag):\n","\n","        self.hashtag = hashtag\n","        self.save_path = f'informer_results{os.path.sep}{hashtag}'\n","        self.tool = language_tool_python.LanguageTool('en-US')\n","        self.max_len = 160\n","    \n","    def load_existing(self):\n","        save_path = f'tweets/{self.hashtag}/{self.hashtag}_ALL_classified.csv'\n","        df =  pd.read_csv(save_path)\n","        tweet_ids = df['tweet_id'].copy().astype(str).tolist()\n","        user_ids = df['user_id'].copy().astype(str).tolist()\n","        return tweet_ids,user_ids,df\n","\n","\n","    def get_device(self):\n","        if torch.cuda.is_available():    \n","\n","            # Tell PyTorch to use the GPU.    \n","            self.device = torch.device(\"cuda\")\n","\n","            print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","            print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","        # If not...\n","        else:\n","            print('No GPU available, using the CPU instead.')\n","            self.device = torch.device(\"cpu\")\n","\n","\n","    def load_informer_data(self):\n","        path = f'tweets{os.path.sep}{self.hashtag}{os.path.sep}{self.hashtag}_ms_cases.json'\n","        with open(path) as jf:\n","            data = json.load(jf)\n","        return data\n","\n","    def load_user_feeds(self):\n","        path = f'tweets/{self.hashtag}/100_feeds'\n","        jsons = [pos_json for pos_json in os.listdir(path) if pos_json.endswith('.json')]\n","        all_js = {}\n","        for file in jsons:\n","            with open(os.path.join(f'{path}/' + file)) as jf:\n","                all_js = { **all_js, **json.load(jf) }\n","        print(f'pulled data on {len(all_js)} users')\n","        return all_js\n","\n","\n","    @staticmethod\n","    def get_the_tweets(database):\n","        all_tweets = {}\n","        for key,value in database.items():\n","            #store tweets by tweet id\n","            all_tweets.update( {str(key):{'text':value['tweet-text'],'user_id':str(value['user-id']),'tweet_id':str(key)}} )\n","\n","            infector = value['infector-info']\n","            i = [str(k) for k in infector]\n","            infector = infector[i[0]]\n","            all_tweets.update( {str(i[0]):{'text':infector['tweet-text'],'user_id':str(infector['user-id']),'tweet_id':str(infector['id'])}} )\n","\n","            for informer in value['informers-data']:\n","                all_tweets.update({str(informer['id']):informer['tweet-text']})\n","                all_tweets.update( {str(informer['id']):{'text':informer['tweet-text'],'user_id':str(informer['user-id']),'tweet_id':str(informer['id'])}} )\n","        return all_tweets\n","\n","    @staticmethod\n","    def store_by_tweets(database):\n","        all_tweets = {}\n","        for key,value in database.items():\n","            if value in all_tweets:\n","                new = all_tweets[value].append(key)\n","                all_tweets[value] = new\n","            else:\n","                all_tweets[value] = [key]\n","\n","        return all_tweets\n","\n","    @staticmethod\n","    def get_users(database):\n","        users = {}\n","        for key,value in database.items():\n","            users.update( { str(value['user-id']):{'description': value['description'], 'feed':[]} } )\n","            infector = value['infector-info']\n","            i = [k for k in infector]\n","            infector = infector[i[0]]\n","            users.update(  { str(infector['user-id']):{'description': infector['description'],'feed':[] } } )\n","            for informer in value['informers-data']:\n","                users.update( { str(informer['user-id']):{'description': informer['description'],'feed':[] } } )\n","        return users\n","\n","    def add_feeds(self,users):\n","        feeds = self.load_user_feeds()\n","        pulled_feeds = feeds.keys()\n","        users_got = users.keys()\n","        users_needed = list(set(pulled_feeds) & set(users_got))\n","        tweet_ids = []\n","        for id in users_needed:\n","            users[id]['feed'] = feeds[id]\n","            tweet_ids.extend( tw['id'] for tw in feeds[id]  )\n","        return users,tweet_ids\n","\n","    @staticmethod\n","    def sort_by_tweet(all_tweets): \n","\n","        df = pd.DataFrame.from_dict(all_tweets, orient='index', columns= ['text','user_id'])\n","        sorted_tweets = {}\n","        for row,index in df.groupby('text').groups.items():\n","            key = tuple(index.values.tolist())\n","            sorted_tweets.update({key:row})\n","\n","        new_df = pd.DataFrame.from_dict(sorted_tweets, orient='index', columns= ['text'])\n","        \n","        return new_df\n","\n","\n","\n","###########################################\n","#######         PREPROCESSING       #######\n","###########################################\n","\n","        \n","    def tweet_cleaner(self,tw_list):\n","        remove_rt = lambda x: re.sub('RT @\\w+: ',\" \",x)\n","        rt = lambda x: re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",x)\n","        hash = lambda x: re.sub(r'#', \"\", x)\n","        amp = lambda x: re.sub(r'&amp', \"\", x)\n","\n","\n","        tw_list['grammartext'] = tw_list.text.map(remove_rt).map(rt)\n","        tw_list['clean_text'] = tw_list.text.map(remove_rt).map(rt)\n","        p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION, p.OPT.HASHTAG)\n","        tw_list[\"grammartext\"] = tw_list.grammartext.map(p.clean)\n","        p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION, p.OPT.NUMBER)\n","        tw_list[\"clean_text\"] = tw_list.clean_text.map(p.clean).map(hash).map(amp)\n","        tw_list[\"clean_text\"] = tw_list.clean_text.str.lower()\n","\n","        #Calculating tweet's lenght and word count\n","        tw_list['text_len'] = tw_list['clean_text'].astype(str).apply(len)\n","        tw_list['text_word_count'] = tw_list['clean_text'].apply(lambda x: len(str(x).split()))\n","        tw_list['punct'] = tw_list['clean_text'].apply(lambda x: self.remove_punct(x))\n","        tw_list['tokenized'] = tw_list['punct'].apply(lambda x: self.tokenization(x.lower()))\n","        tw_list['nonstop'] = tw_list['tokenized'].apply(lambda x: self.remove_stopwords(x))\n","        tw_list['stemmed'] = tw_list['nonstop'].apply(lambda x: self.stemming(x))\n","        return tw_list\n","\n","    @staticmethod\n","    def hugging_preprocess(text):\n","        new_text = []\n","        for t in text.split(\" \"):\n","            t = '@user' if t.startswith('@') and len(t) > 1 else t\n","            t = 'http' if t.startswith('http') else t\n","            new_text.append(t)\n","        return \" \".join(new_text)\n","\n","    def remove_punct(self,text):\n","        text = \"\".join([char for char in text if char not in string.punctuation])\n","        text = re.sub('[0–9]+', '', text)\n","        return text\n","\n","\n","    def remove_stopwords(self,text):\n","        self.stopword = nltk.corpus.stopwords.words('english')\n","        text = [word for word in text if word not in self.stopword]\n","        return text\n","\n","    def stemming(self,text):\n","        self.ps = nltk.PorterStemmer()\n","        text = [self.ps.stem(word) for word in text]\n","        return text\n","\n","    def clean_text(self,text):\n","        text_lc = \"\".join([word.lower() for word in text if word not in string.punctuation]) # remove puntuation\n","        text_rc = re.sub('[0-9]+', '', text_lc)\n","        tokens = re.split('\\W+', text_rc)    # tokenization\n","        text = [self.ps.stem(word) for word in tokens if word not in self.stopword]  # remove stopwords and stemming\n","        return text\n","\n","    @staticmethod\n","    def tokenization(text):\n","        text = re.split('\\W+', text)\n","        return text\n","\n","################################################################################################\n","################################################################################################\n","############################                 METRICS                ############################\n","################################################################################################\n","################################################################################################\n","\n","    @staticmethod\n","    def get_sentiment(df,index,row):\n","        score = SentimentIntensityAnalyzer().polarity_scores(row.clean_text)\n","        neg = score['neg']\n","        neu = score['neu']\n","        pos = score['pos']\n","        comp = score['compound']\n","        if neg > pos:\n","            df.loc[index, 'sentiment'] = \"negative\"\n","        elif pos > neg:\n","            df.loc[index, 'sentiment'] = \"positive\"\n","        else:\n","            df.loc[index, 'sentiment'] = \"neutral\"\n","        df.loc[index, 'neg'] = neg\n","        df.loc[index, 'neu'] = neu\n","        df.loc[index, 'pos'] = pos\n","        df.loc[index, 'compound'] = comp\n","\n","    def get_grammar(self,df,index, row):\n","        # https://michaeljanz-data.science/deepllearning/natural-language-processing/scoring-texts-by-their-grammar-in-python/\n","        scores_word_based_sentence = []\n","        scores_sentence_based_sentence = []\n","        s1 = time.perf_counter()\n","        sentences = nltk.tokenize.sent_tokenize(row.grammartext)\n","        e1 = time.perf_counter()\n","        # sentences = self.split_into_sentences(row)\n","        for sentence in sentences:\n","        # for sentence in helpers.text_to_sentences(text):\n","            matches = self.tool.check(sentence)\n","            count_errors = len(matches)\n","            # only check if the sentence is correct or not\n","            scores_sentence_based_sentence.append(np.min([count_errors, 1]))\n","            scores_word_based_sentence.append(count_errors)\n","            \n","        word_count = len(nltk.tokenize.word_tokenize(row.grammartext))\n","        sum_count_errors_word_based = np.sum(scores_word_based_sentence)\n","        score_word_based = 1 - (sum_count_errors_word_based / word_count)\n","        \n","        sentence_count = len(sentences)       \n","        sum_count_errors_sentence_based = np.sum(scores_sentence_based_sentence)\n","        score_sentence_based = 1 - np.sum(sum_count_errors_sentence_based / sentence_count)\n","\n","        df.loc[index, 'grammar-word-score'] = score_word_based\n","        df.loc[index, 'grammar-sentence-score'] = score_sentence_based\n","\n","    @staticmethod\n","    def get_readability(df,index,row):\n","        if not row.clean_text:\n","            a=9\n","            # print('sentence has no real text')\n","        else:\n","            results = readability.getmeasures(row.clean_text,lang='en')\n","            # [ df.loc[index, score] = results['readability grades'][score] for score in \\\n","            #  ['Kincaid','ARI', 'Coleman-Liau', 'FleschReadingEase', 'GunningFogIndex', \\\n","            #   'LIX', 'SMOGIndex', 'RIX', 'DaleChallIndex'] ]\n","                    \n","            # readability grades\n","            df.loc[index, 'Kincaid'] = results['readability grades']['Kincaid']\n","            df.loc[index, 'ARI'] = results['readability grades']['ARI']\n","            df.loc[index, 'Coleman-Liau'] = results['readability grades']['Coleman-Liau']\n","            df.loc[index, 'FleschReadingEase'] = results['readability grades']['FleschReadingEase']\n","            df.loc[index, 'GunningFogIndex'] = results['readability grades']['GunningFogIndex']\n","            df.loc[index, 'LIX'] = results['readability grades']['LIX']\n","            df.loc[index, 'SMOGIndex'] = results['readability grades']['SMOGIndex']\n","            df.loc[index, 'RIX'] = results['readability grades']['RIX']\n","            df.loc[index, 'DaleChallIndex'] = results['readability grades']['DaleChallIndex']\n","            # sentence info\n","            # self.df.loc[index,'characters_per_word'] = results['sentence info']['characters_per_word']\n","            # self.df.loc[index,'syll_per_word'] = results['sentence info']['syll_per_word']\n","            # self.df.loc[index,'words_per_sentence'] = results['sentence info']['words_per_sentence']\n","            # self.df.loc[index,'sentences_per_paragraph'] = results['sentence info']['sentences_per_paragraph']\n","            # self.df.loc[index,'type_token_ratio'] = results['sentence info']['type_token_ratio']\n","            # self.df.loc[index,'characters'] = results['sentence info']['characters']\n","            # self.df.loc[index,'syllables'] = results['sentence info']['syllables']\n","            # self.df.loc[index,'words'] = results['sentence info']['words']\n","            # self.df.loc[index,'wordtypes'] = results['sentence info']['wordtypes']\n","            # self.df.loc[index,'long_words'] = results['sentence info']['long_words']\n","            df.loc[index,'complex_words'] = results['sentence info']['complex_words']\n","            df.loc[index,'complex_words_dc'] = results['sentence info']['complex_words_dc']\n","\n","\n","    def get_topic(self,df,index,row):\n","        # tokens = self.topic_tokenizer(row.clean_text,return_tensors='pt')\n","        output = self.topic_model(**row.topic_tokens.to(self.device))\n","        scores = output[0][0].detach().cpu().numpy()\n","        scores = expit(scores)\n","        pred = np.argmax(scores)\n","\n","        df.loc[index,'topic'] = pred\n","\n","        for i in range(19):\n","            label = str(self.topic_classes[i])\n","            df.loc[index, label] = scores[i]\n","\n","    def get_topic_single(self,df,index,row):\n","        # tokens = self.topic_tokenizer(row.clean_text,return_tensors='pt')\n","        output = self.topic_model_single(**row.topic_tokens.to(self.device))\n","        scores = output[0][0].detach().cpu().numpy()\n","        scores = expit(scores)\n","        pred = np.argmax(scores)\n","\n","        df.loc[index,'topic_single'] = pred\n","\n","        for i in range(6):\n","            label = str(self.topic_classes_single[i])\n","            df.loc[index, label] = scores[i]\n","\n","    def get_politeness(self,df,index,row):\n","        df.loc[index, 'politeness'] = row.politeness    \n","\n","    def get_offensive(self,df,index,row):\n","        df.loc[index, 'offensive'] = row.offensive  \n","\n","    def get_irony(self,df,index,row):\n","        output = self.irony_model(**row.cardiff_tokens.to(self.device))\n","        scores = output[0][0].detach().cpu().numpy()\n","        scores = softmax(scores)\n","\n","        ranking = np.argsort(scores)\n","        ranking = ranking[::-1]\n","\n","        df.loc[index, 'irony'] = ranking[0]\n","\n","    def get_emoji(self,df,index,row):\n","        output = self.emoji_model(**row.cardiff_tokens.to(self.device))\n","        scores = output[0][0].detach().cpu().numpy()\n","        scores = softmax(scores)\n","\n","        ranking = np.argsort(scores)\n","        ranking = ranking[::-1]\n","\n","        df.loc[index, 'emoji'] = ranking[0] \n","\n","    def get_hate(self,df,index,row):\n","        for i in range(3):\n","            df.loc[index, self.hate_labels[i]] = row.hate_output.probas[self.hate_labels[i]]\n","\n","    def get_emotion(self,df,index,row):\n","        for i in range(6):\n","            df.loc[index, self.emo_labels[i]] = row.emo_output.probas[self.emo_labels[i]]\n","\n","    @staticmethod\n","    def get_gender_model(df):\n","        path = 'user_gender_class/model/logistic_gender'\n","        mod = joblib.load(path)\n","        predictions = mod.predict(df)\n","        return predictions\n","\n","\n","####################################################################################################\n","############################                 LOAD MODELS            ################################\n","####################################################################################################\n","\n","    def load_gender_model(self,tweet_ids,user_ids,feed_ids):\n","        from nltk.corpus import stopwords\n","        stop_words = stopwords.words('english')\n","        stop_words.extend(['u', 'wa', 'ha', 'would', 'com'])\n","        print('starting user gender classification')\n","        remove_rt = lambda x: re.sub('RT @\\w+: ',\" \",x)\n","        rt = lambda x: re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",x)\n","        print('now cleaning')\n","        p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION, p.OPT.NUMBER)\n","        self.user_feeds['cln_description'] = self.user_feeds.description.map(remove_rt).map(rt).map(p.clean).str.lower()\n","        self.user_feeds['cln_text'] = self.user_feeds.text.map(remove_rt).map(rt).map(p.clean).str.lower()\n","        n = len(self.user_feeds)\n","\n","        # call the user feeds df to df just for ease\n","        df = self.user_feeds.copy()\n","\n","        print(df.columns)\n","        df['sep'] = ['.' for i in range(n)]\n","        df['txt'] = df['cln_description'] + df['sep'] + df['cln_text']\n","        df['txt'] = df['txt'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n","        user_info = df.txt\n","        print('finished cleaning')\n","\n","        print('now predicting the gender of each tweet and descrption')\n","        text_predictions = self.get_gender_model(user_info)\n","        print('model finished predicting gender')\n","\n","        df['gender'] = text_predictions\n","        male_txt = df[df['gender']==1]\n","        female_txt = df[df['gender']!=1]\n","        print(df.columns)\n","        print('male tweets')\n","        print(len(male_txt))\n","        print('female tweets')\n","        print(len(female_txt))\n","        # both these contain the users feeds and their feeds\n","\n","        self.df.reset_index(drop=False)\n","        self.df.set_index('user_id',inplace=True)\n","\n","        self.df['gender'] = np.nan\n","        self.df['num_male'] = np.nan\n","        self.df['num_female'] = np.nan\n","\n","        for id in user_ids:\n","            info_user = df[df['user_id']==id]\n","            rows = info_user['user_id'].astype(str).tolist()\n","            if info_user.empty:\n","                print('dont have useres feeds')\n","            else:\n","                gen = info_user['gender'].mode().values[0]\n","\n","                # self.df.loc[id,'gender'] = gen\n","                # self.df.loc[id,'num_male'] = len(info_user[info_user['gender']==1])\n","                # self.df.loc[id,'num_female'] = len(info_user[info_user['gender']!=1])\n","\n","                self.df.loc[rows,'gender'] = gen\n","                self.df.loc[rows,'num_male'] = len(info_user[info_user['gender']==1])\n","                self.df.loc[rows,'num_female'] = len(info_user[info_user['gender']!=1])\n","                \n","\n","        self.df.reset_index(drop=False)\n","\n","        try:\n","            self.df.set_index('tweet_id',inplace=True)\n","            print('SELF.DF INDEX HAS NOW BEEN RESET BACK TO TWEET IDS')\n","        except:\n","            print('tweet_id is already the index')\n","\n","        if 'user_id' not in self.df:\n","            self.df['user_id'] = user_ids\n","            print('\\n\\n\\n added the user ideas \\n\\n\\n')\n","\n","        if 'tweet_id' not in self.df:\n","            self.df['tweet_id'] = tweet_ids\n","\n","        del df\n","        print('gender results')\n","        male_usr = self.df[self.df['gender']==1]\n","        female_usr = self.df[self.df['gender']!=1]\n","        print('male users')\n","        print(len(male_usr))\n","        print('female users')\n","        print(len(female_usr))\n","        print('finished gender')\n","        print('---------\\n---------\\n')\n","        \n","\n","    def load_topic_model(self):\n","\n","        MODEL = f\"cardiffnlp/tweet-topic-21-multi\"\n","        tokenizer = AutoTokenizer.from_pretrained(MODEL)\n","\n","        tokens = self.tweet_df.text.apply(lambda row: tokenizer(row, return_tensors='pt'))\n","        with torch.no_grad():\n","            self.topic_model = AutoModelForSequenceClassification.from_pretrained(MODEL).to(self.device)\n","\n","        self.topic_classes = self.topic_model.config.id2label\n","\n","        s_MODEL = f\"cardiffnlp/tweet-topic-21-single\"\n","        with torch.no_grad():\n","            self.topic_model_single = AutoModelForSequenceClassification.from_pretrained(s_MODEL).to(self.device)\n","        self.topic_classes_single = self.topic_model_single.config.id2label\n","\n","        self.tweet_df['topic_tokens'] = tokens\n","        print('loaded topic model')\n","\n","        print('now loading the topic tokens for all user feeds')\n","        feed_tokens = self.user_feeds.text.apply(lambda row: tokenizer(row, return_tensors='pt'))\n","        self.user_feeds['topic_tokens'] = feed_tokens\n","        print('---------\\n---------\\n')\n","        \n","        \n","\n","    @staticmethod\n","    def run_offensive_model(test):\n","        current = os.getcwd()\n","        new_dir = current+'/offensiveness'\n","\n","        os.chdir(new_dir)\n","        df_scraped = pd.read_csv('labeled_tweets.csv')\n","        df_public = pd.read_csv('public_data_labeled.csv')\n","        df_scraped.drop_duplicates(inplace = True)\n","        df_scraped.drop('id', axis = 'columns', inplace = True)\n","        df_public.drop_duplicates(inplace = True)\n","        df = pd.concat([df_scraped, df_public])\n","        df['label'] = df.label.map({'Offensive': 1, 'Non-offensive': 0})\n","        X_train, X_test, y_train, y_test = train_test_split(df['full_text'], \n","                                                    df['label'], \n","                                                    random_state=42)\n","\n","        os.chdir(current)\n","        # Instantiate the CountVectorizer method\n","        count_vector = CountVectorizer(stop_words = 'english', lowercase = True)\n","\n","        # Fit the training data and then return the matrix\n","        training_data = count_vector.fit_transform(X_train)\n","        testing_data = count_vector.transform(test)\n","        model = SGDClassifier()\n","        model.fit(training_data, y_train)\n","        preds = model.predict(testing_data)\n","        return preds\n","    \n","\n","    def load_offensive_model(self):\n","        test_data = self.tweet_df.text\n","        preds = self.run_offensive_model(test_data)\n","        self.tweet_df['offensive'] = preds\n","        print('loaded offensive model')\n","\n","        print('loading offensive model for users')\n","        feed_test = self.user_feeds.text\n","        feed_preds = self.run_offensive_model(feed_test)\n","        self.user_feeds['offensive'] = feed_preds\n","        print('---------\\n---------\\n')\n","\n","\n","    def load_irony_model(self):\n","        task='irony'\n","        MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n","        tokenizer = AutoTokenizer.from_pretrained(MODEL)\n","        tokens = self.tweet_df.text.apply(lambda row: tokenizer(row, return_tensors='pt'))\n","        # download label mapping\n","        mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n","        with urllib.request.urlopen(mapping_link) as f:\n","            html = f.read().decode('utf-8').split(\"\\n\")\n","            csvreader = csv.reader(html, delimiter='\\t')\n","        self.irony_labels = [row[1] for row in csvreader if len(row) > 1]\n","        # PT\n","        with torch.no_grad():\n","            self.irony_model = AutoModelForSequenceClassification.from_pretrained(MODEL).to(self.device)\n","        self.tweet_df['cardiff_tokens'] = tokens\n","        print('loaded irony model for tweets')\n","\n","\n","\n","    def load_emoji_model(self):\n","        task='emoji'\n","        MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n","        tokenizer = AutoTokenizer.from_pretrained(MODEL)\n","        # download label mapping\n","        mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n","        with urllib.request.urlopen(mapping_link) as f:\n","            html = f.read().decode('utf-8').split(\"\\n\")\n","            csvreader = csv.reader(html, delimiter='\\t')\n","        self.emoji_labels = [row[1] for row in csvreader if len(row) > 1]\n","\n","        with torch.no_grad():\n","            self.emoji_model = AutoModelForSequenceClassification.from_pretrained(MODEL).to(self.device)\n","        print('loaded emoji model')\n","        print('---------\\n---------\\n')\n","\n","    def load_politeness(self):\n","        ## Scoring each tweet based on politeness \n","        class SimpleDataset:\n","            def __init__(self, tokenized_texts):\n","                self.tokenized_texts = tokenized_texts\n","            \n","            def __len__(self):\n","                return len(self.tokenized_texts[\"input_ids\"])\n","            \n","            def __getitem__(self, idx):\n","                return {k: v[idx] for k, v in self.tokenized_texts.items()}\n","        from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n","\n","\n","        # Tokenize same way as training data\n","        model_name = 'roberta-base'\n","        path = f'politeness{os.path.sep}results/checkpoint-52500/'\n","        print('loaded politeness')\n","        tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","        print('politeness per tweet')\n","        with torch.no_grad():\n","            model = AutoModelForSequenceClassification.from_pretrained(path).to(self.device)\n","        tweets = self.tweet_df.grammartext.tolist()\n","        test_encodings = tokenizer(tweets , truncation=True, padding=True, max_length=256)\n","        test_dataset = SimpleDataset(test_encodings)       \n","\n","\n","        trainer = Trainer(model=model)\n","        predictions = trainer.predict(test_dataset)\n","        self.tweet_df['politeness'] = predictions[0]\n","\n","\n","        print('politeness per user')\n","        with torch.no_grad():\n","            model = AutoModelForSequenceClassification.from_pretrained(path).to(self.device)\n","        feeds = self.user_feeds.text.tolist()\n","        feed_encodings = tokenizer(feeds , truncation=True, padding=True, max_length=256)\n","        print('tokenized feeds')\n","        feed_dataset = SimpleDataset(feed_encodings)       \n","\n","        print('now predicting')\n","        feed_trainer = Trainer(model=model)\n","        feed_predictions = feed_trainer.predict(feed_dataset)\n","        self.user_feeds['politeness'] = feed_predictions[0]\n","        \n","        print('loaded politeness model for users')\n","        print('---------\\n---------\\n')\n","\n","        \n","\n","    def load_psysentimento_model(self):\n","            \n","        tweets = self.tweet_df.text.to_list()\n","\n","        # hateful\n","        analyzer = create_analyzer(task=\"hate_speech\", lang=\"en\")\n","        self.hate_labels = ['hateful', 'targeted', 'aggressive']\n","\n","        predictions = [ analyzer.predict(preprocess_tweet(txt)) for txt in tweets ]\n","        print('predicted hate of tweets')\n","        # predictions = process_(analyzer,tweets)\n","        self.tweet_df['hate_output'] = predictions\n","        \n","        print('loaded hate model')\n","\n","        #emotion\n","        e_analyzer = create_analyzer(task=\"emotion\", lang=\"en\")\n","        self.emo_labels = ['joy','sadness','others','anger','surprise','disgust','fear']     \n","\n","        # e_predictions = process_(e_analyzer,tweets)\n","        e_predictions = [ e_analyzer.predict(preprocess_tweet(txt)) for txt in tweets ]\n","        print('predicted emotion of tweets')\n","        self.tweet_df['emo_output'] = e_predictions\n","\n","        print('loaded emotion model')\n","        feeds = self.user_feeds.text.to_list()\n","        print('loading psysentimento for user feeds')\n","\n","        print('tokenizing and predicting user feeds hate')\n","        # fd_hate_pred = process_(analyzer,feeds)\n","        fd_hate_pred = [ analyzer.predict(preprocess_tweet(txt)) for txt in feeds ]\n","        self.user_feeds['hate_output'] = fd_hate_pred\n","        print('predicted hate of feeds')\n","\n","\n","\n","#######################################\n","# MAIN FUNCTION TO RUN THE ANALYSIS\n","########################################\n","\n","\n","    def tweet_analysis(self):\n","\n","        informer_db = self.load_informer_data()\n","        self.get_device()\n","\n","        print('loaded informer data')\n","\n","        #####################\n","        # LOAD IN THE DATA!!!!!!!\n","        #####################\n","\n","        # ALL THE TWEETS IN THE MULTI-SOURCE CASE - store by tweet id\n","        all_tweets = self.get_the_tweets(informer_db)\n","        self.df = pd.DataFrame.from_dict(all_tweets, orient='index')\n","        self.df.drop_duplicates()\n","        user_ids = self.df.user_id.copy().tolist()\n","\n","        print('loaded in ms tweets')\n","\n","        # ALL THE USER FEEDS!!!!! STORED BY THE USER ID !!!!!\n","        all_users = self.get_users(informer_db)\n","        all_users,tweet_ids_feeds = self.add_feeds(all_users)\n","        ## ALL USER FEEDS!! STORED BY THE TWEET ID!! WILL!!!\n","        feeds = [ {'user_id':key, 'description':value['description'],'text':tweet['tweet-text'], 'tweet_id':tweet['id']  }  for key,value in all_users.items() for tweet in value['feed']  ]\n","        self.user_feeds = pd.DataFrame(feeds)\n","        self.user_feeds['tweet_ids'] = tweet_ids_feeds\n","        self.user_feeds.set_index('tweet_ids', inplace=True)\n","        print(f'\\n\\n\\n raw feed len {len(self.user_feeds)}')\n","        self.user_feeds.drop_duplicates()\n","        print(f'\\n\\n\\n feed len after drop duplibats {len(self.user_feeds)}' )\n","        self.user_feeds = self.user_feeds.loc[self.user_feeds['user_id'].isin(user_ids)]\n","        print(f'\\n\\n\\n feed len after not considering the feeds we didnt pull {len(self.user_feeds)}' )\n","\n","        print('loaded in necessary data')\n","\n","        #####################\n","        #####################\n","\n","\n","        ##########################################\n","        # TO TEST FOR A SUBSET OF TWEETS\n","\n","        # remove existing\n","\n","        done_ids, done_users, og_df = self.load_existing()\n","        print(f'\\n\\n\\nalready done {len(done_ids)} tweets')\n","        self.df = self.df[self.df.index.isin(done_ids)]\n","        print(f'\\n\\n\\ now have {len(self.df)} tweets left to analyse')\n","\n","        print(f'\\n\\n\\ reduced user feeds from {len(self.user_feeds)} ')\n","        self.user_feeds = self.user_feeds.copy()[self.user_feeds.user_id.isin(done_users)]\n","        print(f'to {len(self.user_feeds)} ')\n","\n","        yusers = self.df['user_id'].tolist()\n","        user_ids =yusers\n","\n","\n","        self.user_feeds = self.user_feeds[self.user_feeds['user_id'].isin(yusers)]\n","        print('loading test data')\n","        ## END\n","        ##########################################\n","\n","        # load in ids\n","        feed_tweet_ids = list(self.user_feeds.index.values)\n","        tweet_ids = list(self.df.index.values)\n","        feed_user_ids = list(self.user_feeds['user_id'].astype(str))\n","\n","        self.df[['polarity', 'subjectivity']] = self.df['text'].apply(lambda Text: pd.Series(TextBlob(Text).sentiment))\n","        # self.user_feeds[['polarity', 'subjectivity']] = self.user_feeds['text'].apply(lambda Text: pd.Series(TextBlob(Text).sentiment))\n","        self.user_feeds = self.tweet_cleaner(self.user_feeds)\n","\n","        # self.tweet_df = self.tweet_cleaner(self.df.copy())\n","\n","        ##########################################################################\n","        ##########################################################################\n","        ### LOADING IN NECESSARY MODELS\n","\n","        \n","        if 'tweet_id' not in self.df:\n","            self.df['tweet_id'] = tweet_ids\n","\n","        if 'tweet_id' not in self.user_feeds:\n","            self.df['tweet_id'] = feed_tweet_ids\n","\n","        print('put tweet ids back in')\n","\n","        self.load_gender_model(tweet_ids,user_ids,feed_tweet_ids)\n","\n","        print('loaded gender ')\n","\n","        save_path_t = f'tweets/{self.hashtag}/{self.hashtag}_ALL_and_GENDER'\n","\n","        fltr = self.df[['gender','num_male','num_female']]  # INDEX VALUES HERE ARE A STRING!!! KEEP IT THAT WAY\n","        \n","\n","        OG_IDS = og_df['tweet_id'].astype(str).tolist()\n","        og_df['tweet_id'] = OG_IDS\n","        og_df.set_index('tweet_id', inplace = True)\n","\n","        for index, row in  fltr.iterrows():\n","            og_df.loc[index,['gender','num_male','num_female']] = row\n","\n","        # save user feeds df\n","        og_df.to_csv( save_path_t + '.csv') # csv"]},{"cell_type":"markdown","metadata":{"id":"x27JS-gN8X-o"},"source":["## RUN"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"bbwJ9GEpucoO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662749461452,"user_tz":-60,"elapsed":1806333,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"}},"outputId":"e6dfce90-1274-40cd-d401-2d25c01a29e5"},"outputs":[{"output_type":"stream","name":"stdout","text":["No GPU available, using the CPU instead.\n","loaded informer data\n","loaded in ms tweets\n","pulled data on 1508 users\n","\n","\n","\n"," raw feed len 147924\n","\n","\n","\n"," feed len after drop duplibats 147924\n","\n","\n","\n"," feed len after not considering the feeds we didnt pull 147924\n","loaded in necessary data\n","\n","\n","\n","already done 1500 tweets\n","\n","\n","\\ now have 1409 tweets left to analyse\n","\n","\n","\\ reduced user feeds from 147924 \n","to 102962 \n","loading test data\n","put tweet ids back in\n","starting user gender classification\n","now cleaning\n","Index(['user_id', 'description', 'text', 'tweet_id', 'grammartext',\n","       'clean_text', 'text_len', 'text_word_count', 'punct', 'tokenized',\n","       'nonstop', 'stemmed', 'cln_description', 'cln_text'],\n","      dtype='object')\n","finished cleaning\n","now predicting the gender of each tweet and descrption\n","model finished predicting gender\n","Index(['user_id', 'description', 'text', 'tweet_id', 'grammartext',\n","       'clean_text', 'text_len', 'text_word_count', 'punct', 'tokenized',\n","       'nonstop', 'stemmed', 'cln_description', 'cln_text', 'sep', 'txt',\n","       'gender'],\n","      dtype='object')\n","male tweets\n","62633\n","female tweets\n","38056\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","SELF.DF INDEX HAS NOW BEEN RESET BACK TO TWEET IDS\n","\n","\n","\n"," added the user ideas \n","\n","\n","\n","gender results\n","male users\n","820\n","female users\n","589\n","finished gender\n","---------\n","---------\n","\n","loaded gender \n","----------------\n","DONE \n","avengershashtag  \n","---------------- \n","---------------- \n","----------------\n","No GPU available, using the CPU instead.\n","loaded informer data\n","loaded in ms tweets\n","pulled data on 4773 users\n","\n","\n","\n"," raw feed len 474122\n","\n","\n","\n"," feed len after drop duplibats 474122\n","\n","\n","\n"," feed len after not considering the feeds we didnt pull 474122\n","loaded in necessary data\n","\n","\n","\n","already done 1500 tweets\n","\n","\n","\\ now have 1466 tweets left to analyse\n","\n","\n","\\ reduced user feeds from 474122 \n","to 130349 \n","loading test data\n","put tweet ids back in\n","starting user gender classification\n","now cleaning\n","Index(['user_id', 'description', 'text', 'tweet_id', 'grammartext',\n","       'clean_text', 'text_len', 'text_word_count', 'punct', 'tokenized',\n","       'nonstop', 'stemmed', 'cln_description', 'cln_text'],\n","      dtype='object')\n","finished cleaning\n","now predicting the gender of each tweet and descrption\n","model finished predicting gender\n","Index(['user_id', 'description', 'text', 'tweet_id', 'grammartext',\n","       'clean_text', 'text_len', 'text_word_count', 'punct', 'tokenized',\n","       'nonstop', 'stemmed', 'cln_description', 'cln_text', 'sep', 'txt',\n","       'gender'],\n","      dtype='object')\n","male tweets\n","67807\n","female tweets\n","60541\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","SELF.DF INDEX HAS NOW BEEN RESET BACK TO TWEET IDS\n","\n","\n","\n"," added the user ideas \n","\n","\n","\n","gender results\n","male users\n","747\n","female users\n","719\n","finished gender\n","---------\n","---------\n","\n","loaded gender \n","----------------\n","DONE \n","blmhashtag  \n","---------------- \n","---------------- \n","----------------\n","No GPU available, using the CPU instead.\n","loaded informer data\n","loaded in ms tweets\n","pulled data on 3911 users\n","\n","\n","\n"," raw feed len 389053\n","\n","\n","\n"," feed len after drop duplibats 389053\n","\n","\n","\n"," feed len after not considering the feeds we didnt pull 389053\n","loaded in necessary data\n","\n","\n","\n","already done 1500 tweets\n","\n","\n","\\ now have 1456 tweets left to analyse\n","\n","\n","\\ reduced user feeds from 389053 \n","to 126550 \n","loading test data\n","put tweet ids back in\n","starting user gender classification\n","now cleaning\n","Index(['user_id', 'description', 'text', 'tweet_id', 'grammartext',\n","       'clean_text', 'text_len', 'text_word_count', 'punct', 'tokenized',\n","       'nonstop', 'stemmed', 'cln_description', 'cln_text'],\n","      dtype='object')\n","finished cleaning\n","now predicting the gender of each tweet and descrption\n","model finished predicting gender\n","Index(['user_id', 'description', 'text', 'tweet_id', 'grammartext',\n","       'clean_text', 'text_len', 'text_word_count', 'punct', 'tokenized',\n","       'nonstop', 'stemmed', 'cln_description', 'cln_text', 'sep', 'txt',\n","       'gender'],\n","      dtype='object')\n","male tweets\n","71896\n","female tweets\n","52160\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","SELF.DF INDEX HAS NOW BEEN RESET BACK TO TWEET IDS\n","\n","\n","\n"," added the user ideas \n","\n","\n","\n","gender results\n","male users\n","832\n","female users\n","624\n","finished gender\n","---------\n","---------\n","\n","loaded gender \n","----------------\n","DONE \n","borisjohnsonhashtag  \n","---------------- \n","---------------- \n","----------------\n","No GPU available, using the CPU instead.\n","loaded informer data\n","loaded in ms tweets\n","pulled data on 4394 users\n","\n","\n","\n"," raw feed len 437128\n","\n","\n","\n"," feed len after drop duplibats 437128\n","\n","\n","\n"," feed len after not considering the feeds we didnt pull 437128\n","loaded in necessary data\n","\n","\n","\n","already done 1500 tweets\n","\n","\n","\\ now have 1460 tweets left to analyse\n","\n","\n","\\ reduced user feeds from 437128 \n","to 134051 \n","loading test data\n","put tweet ids back in\n","starting user gender classification\n","now cleaning\n","Index(['user_id', 'description', 'text', 'tweet_id', 'grammartext',\n","       'clean_text', 'text_len', 'text_word_count', 'punct', 'tokenized',\n","       'nonstop', 'stemmed', 'cln_description', 'cln_text'],\n","      dtype='object')\n","finished cleaning\n","now predicting the gender of each tweet and descrption\n","model finished predicting gender\n","Index(['user_id', 'description', 'text', 'tweet_id', 'grammartext',\n","       'clean_text', 'text_len', 'text_word_count', 'punct', 'tokenized',\n","       'nonstop', 'stemmed', 'cln_description', 'cln_text', 'sep', 'txt',\n","       'gender'],\n","      dtype='object')\n","male tweets\n","78832\n","female tweets\n","52626\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","SELF.DF INDEX HAS NOW BEEN RESET BACK TO TWEET IDS\n","\n","\n","\n"," added the user ideas \n","\n","\n","\n","gender results\n","male users\n","894\n","female users\n","566\n","finished gender\n","---------\n","---------\n","\n","loaded gender \n","----------------\n","DONE \n","brexithashtag  \n","---------------- \n","---------------- \n","----------------\n","No GPU available, using the CPU instead.\n","loaded informer data\n","loaded in ms tweets\n","pulled data on 2993 users\n","\n","\n","\n"," raw feed len 297338\n","\n","\n","\n"," feed len after drop duplibats 297338\n","\n","\n","\n"," feed len after not considering the feeds we didnt pull 297338\n","loaded in necessary data\n","\n","\n","\n","already done 1500 tweets\n","\n","\n","\\ now have 1368 tweets left to analyse\n","\n","\n","\\ reduced user feeds from 297338 \n","to 133845 \n","loading test data\n","put tweet ids back in\n","starting user gender classification\n","now cleaning\n","Index(['user_id', 'description', 'text', 'tweet_id', 'grammartext',\n","       'clean_text', 'text_len', 'text_word_count', 'punct', 'tokenized',\n","       'nonstop', 'stemmed', 'cln_description', 'cln_text'],\n","      dtype='object')\n","finished cleaning\n","now predicting the gender of each tweet and descrption\n","model finished predicting gender\n","Index(['user_id', 'description', 'text', 'tweet_id', 'grammartext',\n","       'clean_text', 'text_len', 'text_word_count', 'punct', 'tokenized',\n","       'nonstop', 'stemmed', 'cln_description', 'cln_text', 'sep', 'txt',\n","       'gender'],\n","      dtype='object')\n","male tweets\n","73277\n","female tweets\n","54044\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","SELF.DF INDEX HAS NOW BEEN RESET BACK TO TWEET IDS\n","\n","\n","\n"," added the user ideas \n","\n","\n","\n","gender results\n","male users\n","771\n","female users\n","597\n","finished gender\n","---------\n","---------\n","\n","loaded gender \n","----------------\n","DONE \n","climatechangehashtag  \n","---------------- \n","---------------- \n","----------------\n","No GPU available, using the CPU instead.\n","loaded informer data\n","loaded in ms tweets\n","pulled data on 3219 users\n","\n","\n","\n"," raw feed len 319846\n","\n","\n","\n"," feed len after drop duplibats 319846\n","\n","\n","\n"," feed len after not considering the feeds we didnt pull 319846\n","loaded in necessary data\n","\n","\n","\n","already done 1500 tweets\n","\n","\n","\\ now have 1410 tweets left to analyse\n","\n","\n","\\ reduced user feeds from 319846 \n","to 136328 \n","loading test data\n","put tweet ids back in\n","starting user gender classification\n","now cleaning\n","Index(['user_id', 'description', 'text', 'tweet_id', 'grammartext',\n","       'clean_text', 'text_len', 'text_word_count', 'punct', 'tokenized',\n","       'nonstop', 'stemmed', 'cln_description', 'cln_text'],\n","      dtype='object')\n","finished cleaning\n","now predicting the gender of each tweet and descrption\n","model finished predicting gender\n","Index(['user_id', 'description', 'text', 'tweet_id', 'grammartext',\n","       'clean_text', 'text_len', 'text_word_count', 'punct', 'tokenized',\n","       'nonstop', 'stemmed', 'cln_description', 'cln_text', 'sep', 'txt',\n","       'gender'],\n","      dtype='object')\n","male tweets\n","74186\n","female tweets\n","57653\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","SELF.DF INDEX HAS NOW BEEN RESET BACK TO TWEET IDS\n","\n","\n","\n"," added the user ideas \n","\n","\n","\n","gender results\n","male users\n","780\n","female users\n","630\n","finished gender\n","---------\n","---------\n","\n","loaded gender \n","----------------\n","DONE \n","covidhashtag  \n","---------------- \n","---------------- \n","----------------\n","No GPU available, using the CPU instead.\n","loaded informer data\n","loaded in ms tweets\n","pulled data on 1853 users\n","\n","\n","\n"," raw feed len 184140\n","\n","\n","\n"," feed len after drop duplibats 184140\n","\n","\n","\n"," feed len after not considering the feeds we didnt pull 184140\n","loaded in necessary data\n","\n","\n","\n","already done 1500 tweets\n","\n","\n","\\ now have 1366 tweets left to analyse\n","\n","\n","\\ reduced user feeds from 184140 \n","to 119676 \n","loading test data\n","put tweet ids back in\n","starting user gender classification\n","now cleaning\n","Index(['user_id', 'description', 'text', 'tweet_id', 'grammartext',\n","       'clean_text', 'text_len', 'text_word_count', 'punct', 'tokenized',\n","       'nonstop', 'stemmed', 'cln_description', 'cln_text'],\n","      dtype='object')\n","finished cleaning\n","now predicting the gender of each tweet and descrption\n","model finished predicting gender\n","Index(['user_id', 'description', 'text', 'tweet_id', 'grammartext',\n","       'clean_text', 'text_len', 'text_word_count', 'punct', 'tokenized',\n","       'nonstop', 'stemmed', 'cln_description', 'cln_text', 'sep', 'txt',\n","       'gender'],\n","      dtype='object')\n","male tweets\n","58072\n","female tweets\n","53116\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","SELF.DF INDEX HAS NOW BEEN RESET BACK TO TWEET IDS\n","\n","\n","\n"," added the user ideas \n","\n","\n","\n","gender results\n","male users\n","680\n","female users\n","686\n","finished gender\n","---------\n","---------\n","\n","loaded gender \n","----------------\n","DONE \n","gazahashtag  \n","---------------- \n","---------------- \n","----------------\n","No GPU available, using the CPU instead.\n","loaded informer data\n","loaded in ms tweets\n","pulled data on 1237 users\n","\n","\n","\n"," raw feed len 120017\n","\n","\n","\n"," feed len after drop duplibats 120017\n","\n","\n","\n"," feed len after not considering the feeds we didnt pull 120017\n","loaded in necessary data\n","\n","\n","\n","already done 1500 tweets\n","\n","\n","\\ now have 1441 tweets left to analyse\n","\n","\n","\\ reduced user feeds from 120017 \n","to 86055 \n","loading test data\n","put tweet ids back in\n","starting user gender classification\n","now cleaning\n","Index(['user_id', 'description', 'text', 'tweet_id', 'grammartext',\n","       'clean_text', 'text_len', 'text_word_count', 'punct', 'tokenized',\n","       'nonstop', 'stemmed', 'cln_description', 'cln_text'],\n","      dtype='object')\n","finished cleaning\n","now predicting the gender of each tweet and descrption\n","model finished predicting gender\n","Index(['user_id', 'description', 'text', 'tweet_id', 'grammartext',\n","       'clean_text', 'text_len', 'text_word_count', 'punct', 'tokenized',\n","       'nonstop', 'stemmed', 'cln_description', 'cln_text', 'sep', 'txt',\n","       'gender'],\n","      dtype='object')\n","male tweets\n","30711\n","female tweets\n","53351\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","SELF.DF INDEX HAS NOW BEEN RESET BACK TO TWEET IDS\n","\n","\n","\n"," added the user ideas \n","\n","\n","\n","gender results\n","male users\n","357\n","female users\n","1084\n","finished gender\n","---------\n","---------\n","\n","loaded gender \n","----------------\n","DONE \n","loveislandhashtag  \n","---------------- \n","---------------- \n","----------------\n","No GPU available, using the CPU instead.\n","loaded informer data\n","loaded in ms tweets\n","pulled data on 5241 users\n","\n","\n","\n"," raw feed len 519552\n","\n","\n","\n"," feed len after drop duplibats 519552\n","\n","\n","\n"," feed len after not considering the feeds we didnt pull 519552\n","loaded in necessary data\n","\n","\n","\n","already done 1500 tweets\n","\n","\n","\\ now have 1430 tweets left to analyse\n","\n","\n","\\ reduced user feeds from 519552 \n","to 139039 \n","loading test data\n","put tweet ids back in\n","starting user gender classification\n","now cleaning\n","Index(['user_id', 'description', 'text', 'tweet_id', 'grammartext',\n","       'clean_text', 'text_len', 'text_word_count', 'punct', 'tokenized',\n","       'nonstop', 'stemmed', 'cln_description', 'cln_text'],\n","      dtype='object')\n","finished cleaning\n","now predicting the gender of each tweet and descrption\n","model finished predicting gender\n","Index(['user_id', 'description', 'text', 'tweet_id', 'grammartext',\n","       'clean_text', 'text_len', 'text_word_count', 'punct', 'tokenized',\n","       'nonstop', 'stemmed', 'cln_description', 'cln_text', 'sep', 'txt',\n","       'gender'],\n","      dtype='object')\n","male tweets\n","72201\n","female tweets\n","61965\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","SELF.DF INDEX HAS NOW BEEN RESET BACK TO TWEET IDS\n","\n","\n","\n"," added the user ideas \n","\n","\n","\n","gender results\n","male users\n","741\n","female users\n","689\n","finished gender\n","---------\n","---------\n","\n","loaded gender \n","----------------\n","DONE \n","monkeypoxhashtag  \n","---------------- \n","---------------- \n","----------------\n","No GPU available, using the CPU instead.\n","loaded informer data\n","loaded in ms tweets\n","pulled data on 8342 users\n","\n","\n","\n"," raw feed len 827345\n","\n","\n","\n"," feed len after drop duplibats 827345\n","\n","\n","\n"," feed len after not considering the feeds we didnt pull 827345\n","loaded in necessary data\n","\n","\n","\n","already done 1500 tweets\n","\n","\n","\\ now have 1464 tweets left to analyse\n","\n","\n","\\ reduced user feeds from 827345 \n","to 137440 \n","loading test data\n","put tweet ids back in\n","starting user gender classification\n","now cleaning\n","Index(['user_id', 'description', 'text', 'tweet_id', 'grammartext',\n","       'clean_text', 'text_len', 'text_word_count', 'punct', 'tokenized',\n","       'nonstop', 'stemmed', 'cln_description', 'cln_text'],\n","      dtype='object')\n","finished cleaning\n","now predicting the gender of each tweet and descrption\n","model finished predicting gender\n","Index(['user_id', 'description', 'text', 'tweet_id', 'grammartext',\n","       'clean_text', 'text_len', 'text_word_count', 'punct', 'tokenized',\n","       'nonstop', 'stemmed', 'cln_description', 'cln_text', 'sep', 'txt',\n","       'gender'],\n","      dtype='object')\n","male tweets\n","77080\n","female tweets\n","58151\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","SELF.DF INDEX HAS NOW BEEN RESET BACK TO TWEET IDS\n","\n","\n","\n"," added the user ideas \n","\n","\n","\n","gender results\n","male users\n","834\n","female users\n","630\n","finished gender\n","---------\n","---------\n","\n","loaded gender \n","----------------\n","DONE \n","nhshashtag  \n","---------------- \n","---------------- \n","----------------\n","No GPU available, using the CPU instead.\n","loaded informer data\n","loaded in ms tweets\n","pulled data on 2556 users\n","\n","\n","\n"," raw feed len 254363\n","\n","\n","\n"," feed len after drop duplibats 254363\n","\n","\n","\n"," feed len after not considering the feeds we didnt pull 254363\n","loaded in necessary data\n","\n","\n","\n","already done 1500 tweets\n","\n","\n","\\ now have 1326 tweets left to analyse\n","\n","\n","\\ reduced user feeds from 254363 \n","to 134917 \n","loading test data\n","put tweet ids back in\n","starting user gender classification\n","now cleaning\n","Index(['user_id', 'description', 'text', 'tweet_id', 'grammartext',\n","       'clean_text', 'text_len', 'text_word_count', 'punct', 'tokenized',\n","       'nonstop', 'stemmed', 'cln_description', 'cln_text'],\n","      dtype='object')\n","finished cleaning\n","now predicting the gender of each tweet and descrption\n","model finished predicting gender\n","Index(['user_id', 'description', 'text', 'tweet_id', 'grammartext',\n","       'clean_text', 'text_len', 'text_word_count', 'punct', 'tokenized',\n","       'nonstop', 'stemmed', 'cln_description', 'cln_text', 'sep', 'txt',\n","       'gender'],\n","      dtype='object')\n","male tweets\n","64914\n","female tweets\n","61215\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","SELF.DF INDEX HAS NOW BEEN RESET BACK TO TWEET IDS\n","\n","\n","\n"," added the user ideas \n","\n","\n","\n","gender results\n","male users\n","662\n","female users\n","664\n","finished gender\n","---------\n","---------\n","\n","loaded gender \n","----------------\n","DONE \n","olivianewtonjohnhashtag  \n","---------------- \n","---------------- \n","----------------\n","No GPU available, using the CPU instead.\n","loaded informer data\n","loaded in ms tweets\n","pulled data on 4068 users\n","\n","\n","\n"," raw feed len 401003\n","\n","\n","\n"," feed len after drop duplibats 401003\n","\n","\n","\n"," feed len after not considering the feeds we didnt pull 401003\n","loaded in necessary data\n","\n","\n","\n","already done 100 tweets\n","\n","\n","\\ now have 95 tweets left to analyse\n","\n","\n","\\ reduced user feeds from 401003 \n","to 9091 \n","loading test data\n","put tweet ids back in\n","starting user gender classification\n","now cleaning\n","Index(['user_id', 'description', 'text', 'tweet_id', 'grammartext',\n","       'clean_text', 'text_len', 'text_word_count', 'punct', 'tokenized',\n","       'nonstop', 'stemmed', 'cln_description', 'cln_text'],\n","      dtype='object')\n","finished cleaning\n","now predicting the gender of each tweet and descrption\n","model finished predicting gender\n","Index(['user_id', 'description', 'text', 'tweet_id', 'grammartext',\n","       'clean_text', 'text_len', 'text_word_count', 'punct', 'tokenized',\n","       'nonstop', 'stemmed', 'cln_description', 'cln_text', 'sep', 'txt',\n","       'gender'],\n","      dtype='object')\n","male tweets\n","6464\n","female tweets\n","2627\n","dont have useres feeds\n","SELF.DF INDEX HAS NOW BEEN RESET BACK TO TWEET IDS\n","\n","\n","\n"," added the user ideas \n","\n","\n","\n","gender results\n","male users\n","67\n","female users\n","28\n","finished gender\n","---------\n","---------\n","\n","loaded gender \n","----------------\n","DONE \n","supercuphashtag  \n","---------------- \n","---------------- \n","----------------\n","No GPU available, using the CPU instead.\n","loaded informer data\n","loaded in ms tweets\n","pulled data on 2259 users\n","\n","\n","\n"," raw feed len 224444\n","\n","\n","\n"," feed len after drop duplibats 224444\n","\n","\n","\n"," feed len after not considering the feeds we didnt pull 224444\n","loaded in necessary data\n","\n","\n","\n","already done 1500 tweets\n","\n","\n","\\ now have 1461 tweets left to analyse\n","\n","\n","\\ reduced user feeds from 224444 \n","to 108181 \n","loading test data\n","put tweet ids back in\n","starting user gender classification\n","now cleaning\n","Index(['user_id', 'description', 'text', 'tweet_id', 'grammartext',\n","       'clean_text', 'text_len', 'text_word_count', 'punct', 'tokenized',\n","       'nonstop', 'stemmed', 'cln_description', 'cln_text'],\n","      dtype='object')\n","finished cleaning\n","now predicting the gender of each tweet and descrption\n","model finished predicting gender\n","Index(['user_id', 'description', 'text', 'tweet_id', 'grammartext',\n","       'clean_text', 'text_len', 'text_word_count', 'punct', 'tokenized',\n","       'nonstop', 'stemmed', 'cln_description', 'cln_text', 'sep', 'txt',\n","       'gender'],\n","      dtype='object')\n","male tweets\n","56757\n","female tweets\n","49927\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","dont have useres feeds\n","SELF.DF INDEX HAS NOW BEEN RESET BACK TO TWEET IDS\n","\n","\n","\n"," added the user ideas \n","\n","\n","\n","gender results\n","male users\n","679\n","female users\n","782\n","finished gender\n","---------\n","---------\n","\n","loaded gender \n","----------------\n","DONE \n","UkraineWarhashtag  \n","---------------- \n","---------------- \n","----------------\n"]}],"source":["hashtags = ['avengers','blm','borisjohnson','brexit','climatechange','covid','gaza','loveisland','monkeypox','nhs','olivianewtonjohn','supercup','UkraineWar']\n","\n","for hashtag in hashtags:\n","\n","    a = Analyzer(hashtag)\n","\n","    a.tweet_analysis()\n","\n","    print(f'----------------\\nDONE \\n{hashtag}hashtag  \\n---------------- \\n---------------- \\n----------------')"]}]}