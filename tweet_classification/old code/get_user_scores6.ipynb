{"cells":[{"cell_type":"markdown","metadata":{"id":"4MdwUck88MEI"},"source":["## MOUNT"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2468,"status":"ok","timestamp":1663006037095,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"},"user_tz":-60},"id":"njrEJjvc8DnR","outputId":"bc3d3985-775c-4174-b3e4-11fce2e32596"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/My Drive/MSc_project/.MAIN\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd /content/drive/My Drive/MSc_project/.MAIN"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37811,"status":"ok","timestamp":1663006074901,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"},"user_tz":-60},"id":"ZO-PczKG8gPC","outputId":"e75011be-8502-424d-a673-19266987e8b9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n","Requirement already satisfied: install in /usr/local/lib/python3.7/dist-packages (1.3.5)\n","Requirement already satisfied: tweet-preprocessor in /usr/local/lib/python3.7/dist-packages (0.6.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pycountry in /usr/local/lib/python3.7/dist-packages (22.3.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pycountry) (57.4.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pandas==1.2.3 in /usr/local/lib/python3.7/dist-packages (1.2.3)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.3) (2.8.2)\n","Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.3) (1.21.6)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.3) (2022.2.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas==1.2.3) (1.15.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.13.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.53)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.9.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (0.90)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.21.6)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.7.3)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","\u001b[31mERROR: Could not find a version that satisfies the requirement mislib (from versions: none)\u001b[0m\n","\u001b[31mERROR: No matching distribution found for mislib\u001b[0m\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: langdetect in /usr/local/lib/python3.7/dist-packages (1.0.9)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from langdetect) (1.15.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: readability in /usr/local/lib/python3.7/dist-packages (0.3.1)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pysentimiento in /usr/local/lib/python3.7/dist-packages (0.4.2)\n","Requirement already satisfied: torch<2.0.0,>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from pysentimiento) (1.12.1+cu113)\n","Requirement already satisfied: sklearn<0.1,>=0.0 in /usr/local/lib/python3.7/dist-packages (from pysentimiento) (0.0)\n","Requirement already satisfied: datasets<2.0.0,>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from pysentimiento) (1.18.4)\n","Requirement already satisfied: emoji<2.0.0,>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from pysentimiento) (1.7.0)\n","Requirement already satisfied: transformers==4.13 in /usr/local/lib/python3.7/dist-packages (from pysentimiento) (4.13.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.13->pysentimiento) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13->pysentimiento) (4.64.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13->pysentimiento) (6.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.13->pysentimiento) (0.0.53)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.13->pysentimiento) (3.8.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13->pysentimiento) (21.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13->pysentimiento) (1.21.6)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13->pysentimiento) (2022.6.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13->pysentimiento) (0.9.1)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13->pysentimiento) (0.10.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.13->pysentimiento) (4.12.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.13.3->pysentimiento) (3.8.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.13.3->pysentimiento) (0.70.13)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.13.3->pysentimiento) (1.2.3)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.13.3->pysentimiento) (3.0.0)\n","Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.13.3->pysentimiento) (6.0.1)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.13.3->pysentimiento) (2022.8.2)\n","Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.13.3->pysentimiento) (0.18.0)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.13.3->pysentimiento) (0.3.5.1)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.13.3->pysentimiento) (0.13.0)\n","Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.13.3->pysentimiento) (4.1.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.13.3->pysentimiento) (4.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.13.3->pysentimiento) (1.8.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.13.3->pysentimiento) (6.0.2)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.13.3->pysentimiento) (1.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.13.3->pysentimiento) (1.3.1)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.13.3->pysentimiento) (2.1.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.13.3->pysentimiento) (22.1.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.13->pysentimiento) (3.0.9)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13->pysentimiento) (1.25.11)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13->pysentimiento) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13->pysentimiento) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13->pysentimiento) (2.10)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn<0.1,>=0.0->pysentimiento) (1.0.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.13->pysentimiento) (3.8.1)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets<2.0.0,>=1.13.3->pysentimiento) (2022.2.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets<2.0.0,>=1.13.3->pysentimiento) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets<2.0.0,>=1.13.3->pysentimiento) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.13->pysentimiento) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.13->pysentimiento) (7.1.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn<0.1,>=0.0->pysentimiento) (3.1.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn<0.1,>=0.0->pysentimiento) (1.7.3)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (3.2)\n","Automatic pdb calling has been turned ON\n"]}],"source":["!pip install pip install tweet-preprocessor\n","!pip install pycountry\n","!pip install pandas==1.2.3\n","!pip install transformers\n","!pip install xgboost\n","!pip install torch\n","!pip install mislib\n","!pip install langdetect\n","!pip install readability\n","!pip install pysentimiento\n","!pip install wget\n","!pip install -Uqq ipdb\n","import ipdb\n","%pdb on"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4499,"status":"ok","timestamp":1663006079394,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"},"user_tz":-60},"id":"7-nOwlLj7iAp","outputId":"66807c50-bdd1-491a-c815-ad16c9939762"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n","[nltk_data]   Package vader_lexicon is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["from turtle import done\n","from textblob import TextBlob\n","import sys\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","import os\n","import os.path\n","import nltk\n","import nltk.data\n","import time\n","import string\n","\n","nltk.download('vader_lexicon')\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","import json\n","import pickle\n","import joblib\n","import torch\n","\n","import preprocessor as p\n","\n","import pycountry\n","import re\n","import string\n","from wordcloud import WordCloud, STOPWORDS\n","from PIL import Image\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","from nltk import tokenize\n","from langdetect import detect\n","from nltk.stem import SnowballStemmer\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import random\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import os\n","import readability\n","\n","## DATA\n","from datasets import Dataset\n","\n","\n","### topic modelling\n","from transformers import AutoModelForSequenceClassification\n","from transformers import AutoTokenizer\n","import numpy as np\n","from scipy.special import expit\n","\n","#hate\n","from pysentimiento import create_analyzer\n","from pysentimiento.preprocessing import preprocess_tweet\n","\n","\n","nltk.download('omw-1.4')"]},{"cell_type":"markdown","metadata":{"id":"uPxXEHpX8TGw"},"source":["## FUNC"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QADqBoM58VPo"},"outputs":[],"source":["class Analyzer(object):\n","    def __init__(self, hashtag):\n","\n","        self.hashtag = hashtag\n","\n","    def get_device(self):\n","        if torch.cuda.is_available():    \n","\n","            # Tell PyTorch to use the GPU.    \n","            self.device = torch.device(\"cuda\")\n","\n","            print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","            print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","        # If not...\n","        else:\n","            print('No GPU available, using the CPU instead.')\n","            self.device = torch.device(\"cpu\")\n","\n","\n","    def load_informer_data(self):\n","        from itertools import islice\n","        \n","        def take(n, iterable):\n","            \"Return first n items of the iterable as a list\"\n","            return list(islice(iterable, n))\n","\n","        path = f'tweets{os.path.sep}{self.hashtag}{os.path.sep}{self.hashtag}_ms_cases.json'\n","        with open(path) as jf:\n","            data = json.load(jf)\n","        if len(data.keys()) > 1500:  \n","            print('only getting user feeds for 1.5k of our tweets. we must shorten our subset')\n","            return dict(take(1500,data.items()))\n","        else:\n","            return data\n","\n","    def load_user_feeds(self):\n","        path = f'tweets/{self.hashtag}/100_feeds'\n","        jsons = [pos_json for pos_json in os.listdir(path) if pos_json.endswith('.json')]\n","        all_js = {}\n","        for file in jsons:\n","            with open(os.path.join(f'{path}/' + file)) as jf:\n","                all_js = { **all_js, **json.load(jf) }\n","        print(f'pulled data on {len(all_js)} users')\n","        return all_js\n","\n","\n","    @staticmethod\n","    def get_the_tweets(database):\n","        all_tweets = {}\n","        for key,value in database.items():\n","            #store tweets by tweet id\n","            all_tweets.update( {str(key):{'text':value['tweet-text'],'user_id':str(value['user-id']),'tweet_id':str(key)}} ) # target tweet\n","\n","            inf = value['infector-info']\n","            k = list(inf.keys())[0]\n","            infector = inf[k]\n","\n","            all_tweets.update( {str(infector['id']):{'text':infector['tweet-text'],'user_id':str(infector['user-id']),'tweet_id':str(infector['id'])}} )\n","\n","            for i,informer in enumerate(value['informers-data']):\n","                if i <5:\n","                    all_tweets.update( {str(informer['id']):{'text':informer['tweet-text'],'user_id':str(informer['user-id']),'tweet_id':str(informer['id'])}} )\n","        return all_tweets\n","\n","    @staticmethod\n","    def store_by_tweets(database):\n","        all_tweets = {}\n","        for key,value in database.items():\n","            if value in all_tweets:\n","                new = all_tweets[value].append(key)\n","                all_tweets[value] = new\n","            else:\n","                all_tweets[value] = [key]\n","\n","        return all_tweets\n","\n","    @staticmethod\n","    def get_users(database):\n","        users = {}\n","        for key,value in database.items():\n","            users.update( { str(value['user-id']):{'description': value['description'], 'feed':[]} } )\n","            infector = value['infector-info']\n","            i = [k for k in infector]\n","            infector = infector[i[0]]\n","            users.update(  { str(infector['user-id']):{'description': infector['description'],'feed':[] } } )\n","            for informer in value['informers-data']:\n","                users.update( { str(informer['user-id']):{'description': informer['description'],'feed':[] } } )\n","        return users\n","\n","    def add_feeds(self,users):\n","        feeds = self.load_user_feeds()\n","        pulled_feeds = feeds.keys()\n","        users_got = users.keys()\n","        users_needed = list(set(pulled_feeds) & set(users_got))\n","        tweet_ids = []\n","        for id in users_needed:\n","            users[id]['feed'] = feeds[id]\n","            tweet_ids.extend( tw['id'] for tw in feeds[id]  )\n","        return users,tweet_ids\n","\n","    @staticmethod\n","    def sort_by_tweet(all_tweets): \n","\n","        df = pd.DataFrame.from_dict(all_tweets, orient='index', columns= ['text','user_id'])\n","        sorted_tweets = {}\n","        for row,index in df.groupby('text').groups.items():\n","            key = tuple(index.values.tolist())\n","            sorted_tweets.update({key:row})\n","\n","        new_df = pd.DataFrame.from_dict(sorted_tweets, orient='index', columns= ['text'])\n","        \n","        return new_df\n","\n","    def tweet_cleaner(self,tw_list):\n","        remove_rt = lambda x: re.sub('RT @\\w+: ',\" \",x)\n","        rt = lambda x: re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",x)\n","        hash = lambda x: re.sub(r'#', \"\", x)\n","        amp = lambda x: re.sub(r'&amp', \"\", x)\n","\n","\n","        tw_list['grammartext'] = tw_list.text.map(remove_rt).map(rt)\n","        tw_list['clean_text'] = tw_list.text.map(remove_rt).map(rt)\n","        p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION, p.OPT.HASHTAG)\n","        tw_list[\"grammartext\"] = tw_list.grammartext.map(p.clean)\n","        p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION, p.OPT.NUMBER)\n","        tw_list[\"clean_text\"] = tw_list.clean_text.map(p.clean).map(hash).map(amp)\n","        tw_list[\"clean_text\"] = tw_list.clean_text.str.lower()\n","        return tw_list\n","\n","    def remove_punct(self,text):\n","        text = \"\".join([char for char in text if char not in string.punctuation])\n","        text = re.sub('[0–9]+', '', text)\n","        return text\n","\n","\n","    def clean_text(self,text):\n","        text_lc = \"\".join([word.lower() for word in text if word not in string.punctuation]) # remove puntuation\n","        text_rc = re.sub('[0-9]+', '', text_lc)\n","        tokens = re.split('\\W+', text_rc)    # tokenization\n","        text = [self.ps.stem(word) for word in tokens if word not in self.stopword]  # remove stopwords and stemming\n","        return text\n","\n","\n","\n","################################################################################################\n","################################################################################################\n","############################                 METRICS                ############################\n","################################################################################################\n","################################################################################################\n","\n","    @staticmethod\n","    def get_sentiment(df,index,row):\n","        score = SentimentIntensityAnalyzer().polarity_scores(row.clean_text)\n","        neg = score['neg']\n","        neu = score['neu']\n","        pos = score['pos']\n","        comp = score['compound']\n","        if neg > pos:\n","            df.loc[index, 'sentiment'] = \"negative\"\n","        elif pos > neg:\n","            df.loc[index, 'sentiment'] = \"positive\"\n","        else:\n","            df.loc[index, 'sentiment'] = \"neutral\"\n","        df.loc[index, 'neg'] = neg\n","        df.loc[index, 'neu'] = neu\n","        df.loc[index, 'pos'] = pos\n","        df.loc[index, 'compound'] = comp\n","\n","    def get_topic(self,df,index,row):\n","        # tokens = self.topic_tokenizer(row.clean_text,return_tensors='pt')\n","        output = self.topic_model(**row.topic_tokens.to(self.device))\n","        scores = output[0][0].detach().cpu().numpy()\n","        scores = expit(scores)\n","        pred = np.argmax(scores)\n","\n","        df.loc[index,'topic'] = pred\n","\n","        for i in range(19):\n","            label = str(self.topic_classes[i])\n","            df.loc[index, label] = scores[i]\n","\n","\n","    def get_hate(self,df,index,row):\n","        for i in range(3):\n","            df.loc[index, self.hate_labels[i]] = row.psy_output.probas[self.hate_labels[i]]\n","\n","\n","####################################################################################################\n","############################                 LOAD MODELS            ################################\n","####################################################################################################\n","\n","    def load_topic_model(self):\n","\n","        MODEL = f\"cardiffnlp/tweet-topic-21-multi\"\n","        tokenizer = AutoTokenizer.from_pretrained(MODEL)\n","\n","        with torch.no_grad():\n","            self.topic_model = AutoModelForSequenceClassification.from_pretrained(MODEL).to(self.device)\n","\n","        self.topic_classes = self.topic_model.config.id2label\n","\n","        print('now loading the topic tokens for all user feeds')\n","        feed_tokens = self.user_feeds.text.apply(lambda row: tokenizer(row, return_tensors='pt'))\n","\n","        self.user_feeds['topic_tokens'] = feed_tokens\n","        print('---------\\n---------\\n')\n","\n","    def load_psysentimento_model(self):\n","\n","        # hateful\n","        analyzer = create_analyzer(task=\"hate_speech\", lang=\"en\")\n","        self.hate_labels = ['hateful', 'targeted', 'aggressive']\n","\n","        print('loaded emotion model')\n","        feeds = self.user_feeds.text.to_list()\n","        print('loading psysentimento for user feeds')\n","\n","\n","        t1 = time.time()\n","        # the hate predictions are the first element, emotion predictions in the second\n","        fd_psy_pred = [ analyzer.predict(preprocess_tweet(txt))  for txt in feeds ]\n","        self.user_feeds['psy_output'] = fd_psy_pred\n","        t2 = time.time()\n","        print('predicted hate of feeds')\n","        print(f'in {(t2-t1)/3600} hours')\n","    \n","        \n","\n","\n","#######################################\n","# MAIN FUNCTION TO RUN THE ANALYSIS\n","########################################\n","\n","\n","    def tweet_analysis(self):\n","\n","        informer_db = self.load_informer_data()\n","        self.get_device()\n","\n","        print('loaded informer data')\n","\n","        #####################\n","        # LOAD IN THE DATA!!!!!!!\n","        #####################\n","\n","        # ALL THE TWEETS IN THE MULTI-SOURCE CASE - store by tweet id\n","        all_tweets = self.get_the_tweets(informer_db)\n","        self.df = pd.DataFrame.from_dict(all_tweets, orient='index')\n","        self.df.drop_duplicates()\n","        user_ids = self.df.user_id.copy().tolist()\n","\n","        print('loaded in ms tweets')\n","\n","        # ALL THE USER FEEDS!!!!! STORED BY THE USER ID !!!!!\n","        all_users = self.get_users(informer_db)\n","        all_users,tweet_ids_feeds = self.add_feeds(all_users)\n","        ## ALL USER FEEDS!! STORED BY THE TWEET ID!! WILL!!!\n","        feeds = [ {'user_id':key, 'description':value['description'],'text':tweet['tweet-text'], 'tweet_id':tweet['id']  }  for key,value in all_users.items() for tweet in value['feed']  ]\n","        self.user_feeds = pd.DataFrame(feeds)\n","        self.user_feeds['tweet_ids'] = tweet_ids_feeds\n","        self.user_feeds.set_index('tweet_ids', inplace=True)\n","        print(f'\\n\\n\\n raw feed len {len(self.user_feeds)}')\n","        self.user_feeds.drop_duplicates()\n","        print(f'\\n\\n\\n feed len after drop duplibats {len(self.user_feeds)}' )\n","        self.user_feeds = self.user_feeds.loc[self.user_feeds['user_id'].isin(user_ids)]\n","        print(f'\\n\\n\\n feed len after not considering the feeds we didnt pull {len(self.user_feeds)}' )\n","\n","        print('loaded in necessary data')\n","\n","        ##################\n","        # DROPPING DUPLICATE TWEETS FROM BOTH DATAFRAMES....\n","        \n","        print(f'have {len(self.df)} tweets loaded in')\n","        self.df = self.df.copy()[~self.df.index.duplicated(keep='first')]\n","        print(f'now considering {len(self.df)} tweets')\n","\n","\n","        print(f'have {len(self.user_feeds)} tweets loaded in')\n","        self.user_feeds = self.user_feeds.copy()[~self.user_feeds.index.duplicated(keep='first')]\n","        print(f'now considering {len(self.user_feeds)} tweets')\n","\n","\n","        print('loading test data')\n","\n","        #############################################################################################\n","        ########################################################################################\n","        ## SUBSET TESTING\n","\n","        # self.df = self.df.iloc[0:10]\n","        # yusers = self.df['user_id'].tolist()\n","        # user_ids =yusers\n","\n","\n","        # self.user_feeds = self.user_feeds[self.user_feeds['user_id'].isin(yusers)]\n","        # print('loading test data')\n","        ## END\n","        ##########################################\n","\n","\n","        # load in ids\n","        feed_tweet_ids = list(self.user_feeds.index.values)\n","        tweet_ids = list(self.df.index.values)\n","        feed_user_ids = list(self.user_feeds['user_id'].astype(str))\n","\n","        self.user_feeds[['polarity', 'subjectivity']] = self.user_feeds['text'].apply(lambda Text: pd.Series(TextBlob(Text).sentiment))\n","        self.user_feeds = self.tweet_cleaner(self.user_feeds)\n","\n","        \n","        ###############################\n","        ###############################\n","        ### LOADING IN NECESSARY MODELS\n","\n","        \n","        if 'tweet_id' not in self.df:\n","            self.df['tweet_id'] = tweet_ids\n","\n","        if 'tweet_id' not in self.user_feeds:\n","            self.df['tweet_id'] = feed_tweet_ids\n","\n","        tweet_df = self.df.copy()\n","        del self.df\n","\n","        loaders = [self.load_topic_model, self.load_psysentimento_model]\n","\n","        [load() for load in loaders]\n","\n","        user_feeds_df = self.user_feeds.copy()\n","        user_feeds_dum = self.user_feeds.copy()\n","\n","        save_path_f = f'tweets/{self.hashtag}/{self.hashtag}_feeds_classified'\n","        # save_path_f = f'tweet_analysis/output/{self.hashtag}_feeds_classified2'\n","\n","        ########################################################################\n","\n","        with open( save_path_f + '.pkl', 'wb') as f:\n","            joblib.dump(self.user_feeds, f) # pkl\n","\n","        del self.user_feeds\n","\n","        return\n","\n","        funcs = [self.get_sentiment, self.get_topic, self.get_hate]\n","\n","        print('NOW SCORING EACH TWEET IN USER FEED')\n","        t1= time.time()\n","        # [ func(user_feeds_df,idx,tweet) for func in funcs for tweet, idx in user_feeds_dum.groupby('text').iterrows() ]\n","\n","\n","\n","\n","        [ func(user_feeds_df,index,row) for func in funcs for index, row in user_feeds_dum.iterrows() ]\n","\n","\n","        \n","        t2 = time.time()\n","        print(f'finshed scoring each tweet in users feed in {(t2-t1)/3600} hours')\n","\n","        del user_feeds_dum  \n","\n","        ########################################################################\n","        ### AVERAGE PER USER SCORE\n","\n","        uid = list(set(tweet_df.user_id.tolist())) \n","\n","        fid = list(set(user_feeds_df.user_id.tolist())) \n","\n","        user_ids = user_ids = [ u for u in uid if u in fid ]\n","\n","\n","        tweet_df = tweet_df[ tweet_df['user_id'].notna()]\n","        user_feeds_df = user_feeds_df[ user_feeds_df['user_id'].notna()]\n","\n","\n","        tweet_df.reset_index(drop=False)\n","        tweet_df.set_index('user_id', inplace=True)\n","\n","        user_feeds_df.reset_index(drop=False)\n","        user_feeds_df.set_index('user_id', inplace=True)\n","\n","\n","        topics = [self.topic_classes[i] for i in range(19)]\n","        del self.topic_classes\n","        del self.hate_labels\n","\n","        print('now averaging users feeds')\n","\n","        t1= time.time()\n","        for user in user_ids:\n","            self.get_user_means(str(user),tweet_df,user_feeds_df,topics)\n","            self.get_user_modes(str(user),tweet_df,user_feeds_df)\n","            self.get_counts(str(user),tweet_df,user_feeds_df,topics)\n","        t2= time.time()\n","        print(f'finshed averaging users feed in {(t2-t1)/3600} hours')\n","\n","\n","        tweet_df.reset_index(drop=False)\n","        tweet_df.set_index('tweet_id', inplace = True)\n","      \n","        save_path = f'tweets/{self.hashtag}/{self.hashtag}_USER_scores.csv'\n","\n","        tweet_df.to_csv(save_path)\n","\n","\n","    def get_user_means(self,user,tweet_df,user_feeds_df,topics):\n","        user_info = user_feeds_df.loc[user]\n","        scores = ['neg','neu','pos','compound',\n","        'polarity', 'subjectivity',\n","        'hateful', 'targeted', 'aggressive']\n","\n","        all_scores = scores + topics\n","        for score in all_scores:\n","            new = f'user_{score}_mean'\n","            tweet_df.loc[user,new] = user_info[score].mean()\n","\n","    def get_user_modes(self,user,tweet_df,user_feeds_df):\n","        user_info = user_feeds_df.loc[user]\n","        scores = ['sentiment',\n","        'topic']\n","        for score in scores:\n","            new = f'user_{score}_mode'\n","            tweet_df.loc[user,new] = user_info[score].mode().tolist()[0]\n","\n","    def get_counts(self,user,tweet_df,user_feeds_df, topics):\n","        user_sent = user_feeds_df.loc[user]['sentiment']\n","        user_topic = user_feeds_df.loc[user]['sentiment']\n","        for i in ['positive','negative','neutral']:\n","            new = f'user_{i}_count'\n","            tweet_df.loc[user,new] = len(user_sent[user_sent ==i ])\n","        for j in topics:\n","            tweet_df.loc[user,new] = len(user_topic[user_topic==j ])"]},{"cell_type":"markdown","metadata":{"id":"x27JS-gN8X-o"},"source":["## RUN"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"bbwJ9GEpucoO","outputId":"1cfd574e-b0a1-4dd5-abda-ffc32e924d2b"},"outputs":[{"name":"stdout","output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla V100-SXM2-16GB\n","loaded informer data\n","loaded in ms tweets\n","pulled data on 5241 users\n","\n","\n","\n"," raw feed len 519552\n","\n","\n","\n"," feed len after drop duplibats 519552\n","\n","\n","\n"," feed len after not considering the feeds we didnt pull 519552\n","loaded in necessary data\n","have 6871 tweets loaded in\n","now considering 6871 tweets\n","have 519552 tweets loaded in\n","now considering 519552 tweets\n","loading test data\n","now loading the topic tokens for all user feeds\n","---------\n","---------\n","\n"]},{"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/finiteautomata/bertweet-base-emotion-analysis/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c246eed05359b1a49c45955b0265b488e35b0cbd2628e3ead7dd54c8815162ee.a2dff24b4e0a884c6d58a09968c5b68e7391e749eb698ad92541818d420fd01b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"finiteautomata/bertweet-base-emotion-analysis\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"others\",\n","    \"1\": \"joy\",\n","    \"2\": \"sadness\",\n","    \"3\": \"anger\",\n","    \"4\": \"surprise\",\n","    \"5\": \"disgust\",\n","    \"6\": \"fear\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"anger\": 3,\n","    \"disgust\": 5,\n","    \"fear\": 6,\n","    \"joy\": 1,\n","    \"others\": 0,\n","    \"sadness\": 2,\n","    \"surprise\": 4\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading weights file https://huggingface.co/finiteautomata/bertweet-base-emotion-analysis/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/61c5894a0aca5ed63159e2ec6a5501db48124c1e6de287b82bc634334f031203.9c3c4c16d0dd174434d42471b9d4670734d982be506a06fc3111c12bee4380c7\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at finiteautomata/bertweet-base-emotion-analysis.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","loading configuration file https://huggingface.co/finiteautomata/bertweet-base-emotion-analysis/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c246eed05359b1a49c45955b0265b488e35b0cbd2628e3ead7dd54c8815162ee.a2dff24b4e0a884c6d58a09968c5b68e7391e749eb698ad92541818d420fd01b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"finiteautomata/bertweet-base-emotion-analysis\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"others\",\n","    \"1\": \"joy\",\n","    \"2\": \"sadness\",\n","    \"3\": \"anger\",\n","    \"4\": \"surprise\",\n","    \"5\": \"disgust\",\n","    \"6\": \"fear\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"anger\": 3,\n","    \"disgust\": 5,\n","    \"fear\": 6,\n","    \"joy\": 1,\n","    \"others\": 0,\n","    \"sadness\": 2,\n","    \"surprise\": 4\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading file https://huggingface.co/finiteautomata/bertweet-base-emotion-analysis/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/b7837213513a9f3852dcb04048f70c13cbd0590be030e534734ffd42cbdcf45a.f8a4dfe5c3c45a26f9df849d732decb191dc0c05ab270799695430332d143982\n","loading file https://huggingface.co/finiteautomata/bertweet-base-emotion-analysis/resolve/main/bpe.codes from cache at /root/.cache/huggingface/transformers/76e357e2554ebe053d1c4c613506bc2cc19d66ae27fec8218261a7f73c6456b9.75877d86011e5d5d46614d3a21757b705e9d20ed45a019805d25159b4837b0a4\n","loading file https://huggingface.co/finiteautomata/bertweet-base-emotion-analysis/resolve/main/added_tokens.json from cache at /root/.cache/huggingface/transformers/c4b7522f44ed8adb95e62288c6458da591654f7466e3ce2f9c730bb4087411d2.c1e7052e39d2135302ec27455f6db22e1520e6539942ff60a849c7f83f8ec6dc\n","loading file https://huggingface.co/finiteautomata/bertweet-base-emotion-analysis/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/01581144d9bf96cb9c7d8a77ee93c8b1f1095af5c1204b1b038a8cb0e3247aa8.0dc5b1041f62041ebbd23b1297f2f573769d5c97d8b7c28180ec86b8f6185aa8\n","loading file https://huggingface.co/finiteautomata/bertweet-base-emotion-analysis/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/1740697312c59fe96586f476c7765cd6f08516a6102ea96f22ffee64f7553234.c260b44e952f7f2a825aac395f2ebbed4ac9553800d1e320af246e81a548f37c\n","loading file https://huggingface.co/finiteautomata/bertweet-base-emotion-analysis/resolve/main/tokenizer.json from cache at None\n","loading configuration file https://huggingface.co/finiteautomata/bertweet-base-emotion-analysis/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c246eed05359b1a49c45955b0265b488e35b0cbd2628e3ead7dd54c8815162ee.a2dff24b4e0a884c6d58a09968c5b68e7391e749eb698ad92541818d420fd01b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"finiteautomata/bertweet-base-emotion-analysis\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"others\",\n","    \"1\": \"joy\",\n","    \"2\": \"sadness\",\n","    \"3\": \"anger\",\n","    \"4\": \"surprise\",\n","    \"5\": \"disgust\",\n","    \"6\": \"fear\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"anger\": 3,\n","    \"disgust\": 5,\n","    \"fear\": 6,\n","    \"joy\": 1,\n","    \"others\": 0,\n","    \"sadness\": 2,\n","    \"surprise\": 4\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","Adding <mask> to the vocabulary\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]},{"name":"stdout","output_type":"stream","text":["loaded emotion model\n","loading psysentimento for user feeds\n"]}],"source":["# hashtags = ['avengers','blm','borisjohnson','brexit','climatechange','covid','gaza','loveisland','monkeypox','nhs','olivianewtonjohn','supercup','UkraineWar']\n","\n","hashtags = ['monkeypox','blm','supercup','gaza']\n","\n","for hashtag in hashtags:\n","\n","    a = Analyzer(hashtag)\n","\n","    a.tweet_analysis()\n","\n","    print(f'----------------\\nDONE \\n{hashtag}hashtag  \\n----------------')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B6-CUZgM1iaF"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eecOcLI-5MyB"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AjjJCvcv8Zum"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XNx_F-ZJ83Bp"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cuX4NXXV9UUu"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QC3jckDF9xns"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p8F6nX9r-O6m"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UyBWl0De-sNr"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_6ySaK32_Jgs"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7SbLN_gl_mzi"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ug1r1mOSAEGg"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ziES9FAq7B-L"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JuJa05I67fRG"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nx3hhIDd78kI"},"outputs":[],"source":[]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyMUtGFEdEYhjHLdhbfY0GwI"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}