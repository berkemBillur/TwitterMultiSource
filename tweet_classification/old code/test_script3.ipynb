{"cells":[{"cell_type":"markdown","metadata":{"id":"4MdwUck88MEI"},"source":["## MOUNT"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22362,"status":"ok","timestamp":1662772295298,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"},"user_tz":-60},"id":"njrEJjvc8DnR","outputId":"590e4058-d971-4219-8384-d932e8ccd6b2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/My Drive/MSc_project/.MAIN\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd /content/drive/My Drive/MSc_project/.MAIN"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":509,"status":"ok","timestamp":1662807468700,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"},"user_tz":-60},"id":"ZO-PczKG8gPC","outputId":"ace7bd11-b8a7-442e-b8ba-f703ee26a6ef"},"outputs":[{"output_type":"stream","name":"stdout","text":["Automatic pdb calling has been turned ON\n"]}],"source":["# !pip install datasets\n","# !pip install language_tool_python\n","# !pip install pip install tweet-preprocessor\n","# !pip install pycountry\n","# !pip install pandas==1.2.3\n","# !pip install transformers\n","# !pip install xgboost\n","# !pip install torch\n","# !pip install mislib\n","# !pip install langdetect\n","# !pip install readability\n","# !pip install pysentimiento\n","# !pip install wget\n","# !pip install -Uqq ipdb\n","# import ipdb\n","%pdb on"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15337,"status":"ok","timestamp":1662772406167,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"},"user_tz":-60},"id":"7-nOwlLj7iAp","outputId":"7d95576f-c7f0-4c6c-c803-66e996dde34d"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"]}],"source":["from textblob import TextBlob\n","import sys\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","import os\n","import os.path\n","import nltk\n","import nltk.data\n","import time\n","import string\n","from datasets import load_dataset\n","\n","nltk.download('vader_lexicon')\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","import json\n","import pickle\n","import joblib\n","\n","import language_tool_python\n","import preprocessor as p\n","\n","import pycountry\n","import re\n","import string\n","from wordcloud import WordCloud, STOPWORDS\n","from PIL import Image\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","from nltk import tokenize\n","from langdetect import detect\n","from nltk.stem import SnowballStemmer\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import random\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import os\n","import readability\n","\n","## DATA\n","from datasets import Dataset\n","\n","### POLITENESS\n","from politeness.polite_script import *\n","\n","### topic modelling\n","from transformers import AutoModelForSequenceClassification\n","from transformers import AutoTokenizer\n","import numpy as np\n","from scipy.special import expit\n","\n","### irony\n","import urllib.request\n","from scipy.special import softmax\n","import csv\n","\n","## offensiveness\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression, SGDClassifier\n","\n","#hate\n","from pysentimiento import create_analyzer\n","from pysentimiento.preprocessing import preprocess_tweet\n","\n","# user genders \n","import torch\n","from transformers import BertTokenizer\n","from collections import defaultdict\n","\n","nltk.download('omw-1.4')\n","import time\n","\n","# if 'google.colab' in str(get_ipython()):\n","#   print('Running on CoLab')\n","# else:\n","#   print('Not running on CoLab')\n","#   os.chdir('G:\\My Drive\\MSc_project\\.MAIN\\offensiveness')\n"]},{"cell_type":"markdown","metadata":{"id":"uPxXEHpX8TGw"},"source":["## FUNC"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"QADqBoM58VPo","executionInfo":{"status":"ok","timestamp":1662807517397,"user_tz":-60,"elapsed":501,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"}}},"outputs":[],"source":["class Analyzer(object):\n","    def __init__(self, hashtag):\n","\n","        self.hashtag = hashtag\n","        self.save_path = f'informer_results{os.path.sep}{hashtag}'\n","        self.tool = language_tool_python.LanguageTool('en-US')\n","        self.max_len = 160\n","\n","    def load_existing(self):\n","        save_path = f'tweets/{self.hashtag}/{self.hashtag}_ALL_classified.csv'\n","        df =  pd.read_csv(save_path)\n","        tweet_ids = df['tweet_id'].copy().astype(str).tolist()\n","        user_ids = df['user_id'].copy().astype(str).tolist()\n","        return tweet_ids,user_ids,df\n","\n","    def get_device(self):\n","        if torch.cuda.is_available():    \n","\n","            # Tell PyTorch to use the GPU.    \n","            self.device = torch.device(\"cuda\")\n","\n","            print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","            print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","        # If not...\n","        else:\n","            print('No GPU available, using the CPU instead.')\n","            self.device = torch.device(\"cpu\")\n","\n","    def load_informer_data(self):\n","        path = f'tweets{os.path.sep}{self.hashtag}{os.path.sep}{self.hashtag}_ms_cases.json'\n","        with open(path) as jf:\n","            data = json.load(jf)\n","        return data\n","\n","    def load_user_feeds(self):\n","        path = f'tweets/{self.hashtag}/100_feeds'\n","        jsons = [pos_json for pos_json in os.listdir(path) if pos_json.endswith('.json')]\n","        all_js = {}\n","        for file in jsons:\n","            with open(os.path.join(f'{path}/' + file)) as jf:\n","                all_js = { **all_js, **json.load(jf) }\n","        print(f'pulled data on {len(all_js)} users')\n","        return all_js\n","\n","\n","    @staticmethod\n","    def get_the_tweets(database):\n","        all_tweets = {}\n","        for key,value in database.items():\n","            #store tweets by tweet id\n","            all_tweets.update( {str(key):{'text':value['tweet-text'],'user_id':str(value['user-id']),'tweet_id':str(key)}} ) # target tweet\n","\n","            infector = value['infector-info']\n","            i = [str(k) for k in infector]\n","            infector = infector[i[0]]\n","            all_tweets.update( {str(i[0]):{'text':infector['tweet-text'],'user_id':str(infector['user-id']),'tweet_id':str(infector['id'])}} )\n","\n","            for informer in value['informers-data']:\n","                all_tweets.update( {str(informer['id']):{'text':informer['tweet-text'],'user_id':str(informer['user-id']),'tweet_id':str(informer['id'])}} )\n","        return all_tweets\n","\n","    @staticmethod\n","    def store_by_tweets(database):\n","        all_tweets = {}\n","        for key,value in database.items():\n","            if value in all_tweets:\n","                new = all_tweets[value].append(key)\n","                all_tweets[value] = new\n","            else:\n","                all_tweets[value] = [key]\n","\n","        return all_tweets\n","\n","    @staticmethod\n","    def get_users(database):\n","        users = {}\n","        for key,value in database.items():\n","            users.update( { str(value['user-id']):{'description': value['description'], 'feed':[]} } )\n","            infector = value['infector-info']\n","            i = [k for k in infector]\n","            infector = infector[i[0]]\n","            users.update(  { str(infector['user-id']):{'description': infector['description'],'feed':[] } } )\n","            for informer in value['informers-data']:\n","                users.update( { str(informer['user-id']):{'description': informer['description'],'feed':[] } } )\n","        return users\n","\n","    def add_feeds(self,users):\n","        feeds = self.load_user_feeds()\n","        pulled_feeds = feeds.keys()\n","        users_got = users.keys()\n","        users_needed = list(set(pulled_feeds) & set(users_got))\n","        tweet_ids = []\n","        for id in users_needed:\n","            users[id]['feed'] = feeds[id]\n","            tweet_ids.extend( tw['id'] for tw in feeds[id]  )\n","        return users,tweet_ids\n","\n","    @staticmethod\n","    def sort_by_tweet(all_tweets): \n","\n","        df = pd.DataFrame.from_dict(all_tweets, orient='index', columns= ['text','user_id'])\n","        sorted_tweets = {}\n","        for row,index in df.groupby('text').groups.items():\n","            key = tuple(index.values.tolist())\n","            sorted_tweets.update({key:row})\n","\n","        new_df = pd.DataFrame.from_dict(sorted_tweets, orient='index', columns= ['text'])\n","        \n","        return new_df\n","\n","\n","\n","###########################################\n","#######         PREPROCESSING       #######\n","###########################################\n","\n","        \n","    def tweet_cleaner(self,tw_list):\n","        remove_rt = lambda x: re.sub('RT @\\w+: ',\" \",x)\n","        rt = lambda x: re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",x)\n","        hash = lambda x: re.sub(r'#', \"\", x)\n","        amp = lambda x: re.sub(r'&amp', \"\", x)\n","\n","\n","        tw_list['grammartext'] = tw_list.text.map(remove_rt).map(rt)\n","        tw_list['clean_text'] = tw_list.text.map(remove_rt).map(rt)\n","        p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION, p.OPT.HASHTAG)\n","        tw_list[\"grammartext\"] = tw_list.grammartext.map(p.clean)\n","        p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION, p.OPT.NUMBER)\n","        tw_list[\"clean_text\"] = tw_list.clean_text.map(p.clean).map(hash).map(amp)\n","        tw_list[\"clean_text\"] = tw_list.clean_text.str.lower()\n","\n","        #Calculating tweet's lenght and word count\n","        tw_list['text_len'] = tw_list['clean_text'].astype(str).apply(len)\n","        tw_list['text_word_count'] = tw_list['clean_text'].apply(lambda x: len(str(x).split()))\n","        tw_list['punct'] = tw_list['clean_text'].apply(lambda x: self.remove_punct(x))\n","        tw_list['tokenized'] = tw_list['punct'].apply(lambda x: self.tokenization(x.lower()))\n","        tw_list['nonstop'] = tw_list['tokenized'].apply(lambda x: self.remove_stopwords(x))\n","        tw_list['stemmed'] = tw_list['nonstop'].apply(lambda x: self.stemming(x))\n","        return tw_list\n","\n","    @staticmethod\n","    def hugging_preprocess(text):\n","        new_text = []\n","        for t in text.split(\" \"):\n","            t = '@user' if t.startswith('@') and len(t) > 1 else t\n","            t = 'http' if t.startswith('http') else t\n","            new_text.append(t)\n","        return \" \".join(new_text)\n","\n","    def remove_punct(self,text):\n","        text = \"\".join([char for char in text if char not in string.punctuation])\n","        text = re.sub('[0â€“9]+', '', text)\n","        return text\n","\n","\n","    def remove_stopwords(self,text):\n","        self.stopword = nltk.corpus.stopwords.words('english')\n","        text = [word for word in text if word not in self.stopword]\n","        return text\n","\n","    def stemming(self,text):\n","        self.ps = nltk.PorterStemmer()\n","        text = [self.ps.stem(word) for word in text]\n","        return text\n","\n","    def clean_text(self,text):\n","        text_lc = \"\".join([word.lower() for word in text if word not in string.punctuation]) # remove puntuation\n","        text_rc = re.sub('[0-9]+', '', text_lc)\n","        tokens = re.split('\\W+', text_rc)    # tokenization\n","        text = [self.ps.stem(word) for word in tokens if word not in self.stopword]  # remove stopwords and stemming\n","        return text\n","\n","    @staticmethod\n","    def tokenization(text):\n","        text = re.split('\\W+', text)\n","        return text\n","\n","################################################################################################\n","################################################################################################\n","############################                 METRICS                ############################\n","################################################################################################\n","################################################################################################\n","\n","    @staticmethod\n","    def get_sentiment(df,index,row):\n","        score = SentimentIntensityAnalyzer().polarity_scores(row.clean_text)\n","        neg = score['neg']\n","        neu = score['neu']\n","        pos = score['pos']\n","        comp = score['compound']\n","        if neg > pos:\n","            df.loc[index, 'sentiment'] = \"negative\"\n","        elif pos > neg:\n","            df.loc[index, 'sentiment'] = \"positive\"\n","        else:\n","            df.loc[index, 'sentiment'] = \"neutral\"\n","        df.loc[index, 'neg'] = neg\n","        df.loc[index, 'neu'] = neu\n","        df.loc[index, 'pos'] = pos\n","        df.loc[index, 'compound'] = comp\n","\n","    def get_grammar(self,df,index, row):\n","        # https://michaeljanz-data.science/deepllearning/natural-language-processing/scoring-texts-by-their-grammar-in-python/\n","        scores_word_based_sentence = []\n","        scores_sentence_based_sentence = []\n","        s1 = time.perf_counter()\n","        sentences = nltk.tokenize.sent_tokenize(row.grammartext)\n","        e1 = time.perf_counter()\n","        # sentences = self.split_into_sentences(row)\n","        for sentence in sentences:\n","        # for sentence in helpers.text_to_sentences(text):\n","            matches = self.tool.check(sentence)\n","            count_errors = len(matches)\n","            # only check if the sentence is correct or not\n","            scores_sentence_based_sentence.append(np.min([count_errors, 1]))\n","            scores_word_based_sentence.append(count_errors)\n","            \n","        word_count = len(nltk.tokenize.word_tokenize(row.grammartext))\n","        sum_count_errors_word_based = np.sum(scores_word_based_sentence)\n","        score_word_based = 1 - (sum_count_errors_word_based / word_count)\n","        \n","        sentence_count = len(sentences)       \n","        sum_count_errors_sentence_based = np.sum(scores_sentence_based_sentence)\n","        score_sentence_based = 1 - np.sum(sum_count_errors_sentence_based / sentence_count)\n","\n","        df.loc[index, 'grammar-word-score'] = score_word_based\n","        df.loc[index, 'grammar-sentence-score'] = score_sentence_based\n","\n","    @staticmethod\n","    def get_readability(df,index,row):\n","        if not row.clean_text:\n","            a=9\n","            # print('sentence has no real text')\n","        else:\n","            results = readability.getmeasures(row.clean_text,lang='en')\n","            # [ df.loc[index, score] = results['readability grades'][score] for score in \\\n","            #  ['Kincaid','ARI', 'Coleman-Liau', 'FleschReadingEase', 'GunningFogIndex', \\\n","            #   'LIX', 'SMOGIndex', 'RIX', 'DaleChallIndex'] ]\n","                    \n","            # readability grades\n","            df.loc[index, 'Kincaid'] = results['readability grades']['Kincaid']\n","            df.loc[index, 'ARI'] = results['readability grades']['ARI']\n","            df.loc[index, 'Coleman-Liau'] = results['readability grades']['Coleman-Liau']\n","            df.loc[index, 'FleschReadingEase'] = results['readability grades']['FleschReadingEase']\n","            df.loc[index, 'GunningFogIndex'] = results['readability grades']['GunningFogIndex']\n","            df.loc[index, 'LIX'] = results['readability grades']['LIX']\n","            df.loc[index, 'SMOGIndex'] = results['readability grades']['SMOGIndex']\n","            df.loc[index, 'RIX'] = results['readability grades']['RIX']\n","            df.loc[index, 'DaleChallIndex'] = results['readability grades']['DaleChallIndex']\n","            # sentence info\n","            # self.df.loc[index,'characters_per_word'] = results['sentence info']['characters_per_word']\n","            # self.df.loc[index,'syll_per_word'] = results['sentence info']['syll_per_word']\n","            # self.df.loc[index,'words_per_sentence'] = results['sentence info']['words_per_sentence']\n","            # self.df.loc[index,'sentences_per_paragraph'] = results['sentence info']['sentences_per_paragraph']\n","            # self.df.loc[index,'type_token_ratio'] = results['sentence info']['type_token_ratio']\n","            # self.df.loc[index,'characters'] = results['sentence info']['characters']\n","            # self.df.loc[index,'syllables'] = results['sentence info']['syllables']\n","            # self.df.loc[index,'words'] = results['sentence info']['words']\n","            # self.df.loc[index,'wordtypes'] = results['sentence info']['wordtypes']\n","            # self.df.loc[index,'long_words'] = results['sentence info']['long_words']\n","            df.loc[index,'complex_words'] = results['sentence info']['complex_words']\n","            df.loc[index,'complex_words_dc'] = results['sentence info']['complex_words_dc']\n","\n","\n","    def get_topic(self,df,index,row):\n","        # tokens = self.topic_tokenizer(row.clean_text,return_tensors='pt')\n","        output = self.topic_model(**row.topic_tokens.to(self.device))\n","        scores = output[0][0].detach().cpu().numpy()\n","        scores = expit(scores)\n","        pred = np.argmax(scores)\n","\n","        df.loc[index,'topic'] = pred\n","\n","        for i in range(19):\n","            label = str(self.topic_classes[i])\n","            df.loc[index, label] = scores[i]\n","\n","    def get_topic_single(self,df,index,row):\n","        # tokens = self.topic_tokenizer(row.clean_text,return_tensors='pt')\n","        output = self.topic_model_single(**row.topic_tokens.to(self.device))\n","        scores = output[0][0].detach().cpu().numpy()\n","        scores = expit(scores)\n","        pred = np.argmax(scores)\n","\n","        df.loc[index,'topic_single'] = pred\n","\n","        for i in range(6):\n","            label = str(self.topic_classes_single[i])\n","            df.loc[index, label] = scores[i]\n","\n","    def get_politeness(self,df,index,row):\n","        df.loc[index, 'politeness'] = row.politeness    \n","\n","    def get_offensive(self,df,index,row):\n","        df.loc[index, 'offensive'] = row.offensive  \n","\n","    def get_irony(self,df,index,row):\n","        output = self.irony_model(**row.cardiff_tokens.to(self.device))\n","        scores = output[0][0].detach().cpu().numpy()\n","        scores = softmax(scores)\n","\n","        ranking = np.argsort(scores)\n","        ranking = ranking[::-1]\n","\n","        df.loc[index, 'irony'] = ranking[0]\n","\n","    def get_emoji(self,df,index,row):\n","        output = self.emoji_model(**row.cardiff_tokens.to(self.device))\n","        scores = output[0][0].detach().cpu().numpy()\n","        scores = softmax(scores)\n","\n","        ranking = np.argsort(scores)\n","        ranking = ranking[::-1]\n","\n","        df.loc[index, 'emoji'] = ranking[0] \n","\n","    def get_hate(self,df,index,row):\n","        for i in range(3):\n","            df.loc[index, self.hate_labels[i]] = row.hate_output.probas[self.hate_labels[i]]\n","\n","    def get_emotion(self,df,index,row):\n","        for i in range(6):\n","            df.loc[index, self.emo_labels[i]] = row.emo_output.probas[self.emo_labels[i]]\n","\n","    @staticmethod\n","    def get_gender_model(df):\n","        path = 'user_gender_class/model/logistic_gender'\n","        mod = joblib.load(path)\n","        predictions = mod.predict(df)\n","        return predictions\n","\n","\n","####################################################################################################\n","############################                 LOAD MODELS            ################################\n","####################################################################################################\n","\n","    def load_gender_model(self,tweet_ids,user_ids,feed_ids):\n","        from nltk.corpus import stopwords\n","        stop_words = stopwords.words('english')\n","        stop_words.extend(['u', 'wa', 'ha', 'would', 'com'])\n","        print('starting user gender classification')\n","        remove_rt = lambda x: re.sub('RT @\\w+: ',\" \",x)\n","        rt = lambda x: re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",x)\n","        print('now cleaning')\n","        p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION, p.OPT.NUMBER)\n","        self.user_feeds['cln_description'] = self.user_feeds.description.map(remove_rt).map(rt).map(p.clean).str.lower()\n","        self.user_feeds['cln_text'] = self.user_feeds.text.map(remove_rt).map(rt).map(p.clean).str.lower()\n","        n = len(self.user_feeds)\n","\n","        # call the user feeds df to df just for ease\n","        df = self.user_feeds.copy()\n","\n","        print(df.columns)\n","        df['sep'] = ['.' for i in range(n)]\n","        df['txt'] = df['cln_description'] + df['sep'] + df['cln_text']\n","        df['txt'] = df['txt'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n","        user_info = df.txt\n","        print('finished cleaning')\n","\n","        print('now predicting the gender of each tweet and descrption')\n","        text_predictions = self.get_gender_model(user_info)\n","        print('model finished predicting gender')\n","\n","        df['gender'] = text_predictions\n","        male_txt = df[df['gender']==1]\n","        female_txt = df[df['gender']!=1]\n","        print(df.columns)\n","        print('male tweets')\n","        print(len(male_txt))\n","        print('female tweets')\n","        print(len(female_txt))\n","        # both these contain the users feeds and their feeds\n","\n","        self.df.reset_index(drop=False)\n","        self.df.set_index('user_id',inplace=True)\n","\n","        self.df['gender'] = np.nan\n","        self.df['num_male'] = np.nan\n","        self.df['num_female'] = np.nan\n","\n","        for id in user_ids:\n","            info_user = df[df['user_id']==id]\n","            rows = info_user['user_id'].astype(str).tolist()\n","            if info_user.empty:\n","                print('dont have useres feeds')\n","            else:\n","                gen = info_user['gender'].mode().values[0]\n","\n","                # self.df.loc[id,'gender'] = gen\n","                # self.df.loc[id,'num_male'] = len(info_user[info_user['gender']==1])\n","                # self.df.loc[id,'num_female'] = len(info_user[info_user['gender']!=1])\n","\n","                self.df.loc[rows,'gender'] = gen\n","                self.df.loc[rows,'num_male'] = len(info_user[info_user['gender']==1])\n","                self.df.loc[rows,'num_female'] = len(info_user[info_user['gender']!=1])\n","                \n","\n","        self.df.reset_index(drop=False)\n","\n","        try:\n","            self.df.set_index('tweet_id',inplace=True)\n","            print('SELF.DF INDEX HAS NOW BEEN RESET BACK TO TWEET IDS')\n","        except:\n","            print('tweet_id is already the index')\n","\n","        if 'user_id' not in self.df:\n","            self.df['user_id'] = user_ids\n","            print('\\n\\n\\n added the user ideas \\n\\n\\n')\n","\n","        if 'tweet_id' not in self.df:\n","            self.df['tweet_id'] = tweet_ids\n","\n","        del df\n","        print('gender results')\n","        male_usr = self.df[self.df['gender']==1]\n","        female_usr = self.df[self.df['gender']!=1]\n","        print('male users')\n","        print(len(male_usr))\n","        print('female users')\n","        print(len(female_usr))\n","        print('finished gender')\n","        print('---------\\n---------\\n')\n","        \n","\n","    def load_topic_model(self):\n","\n","        MODEL = f\"cardiffnlp/tweet-topic-21-multi\"\n","        tokenizer = AutoTokenizer.from_pretrained(MODEL)\n","\n","        tokens = self.tweet_df.text.apply(lambda row: tokenizer(row, return_tensors='pt'))\n","        with torch.no_grad():\n","            self.topic_model = AutoModelForSequenceClassification.from_pretrained(MODEL).to(self.device)\n","\n","        self.topic_classes = self.topic_model.config.id2label\n","\n","        s_MODEL = f\"cardiffnlp/tweet-topic-21-single\"\n","        with torch.no_grad():\n","            self.topic_model_single = AutoModelForSequenceClassification.from_pretrained(s_MODEL).to(self.device)\n","        self.topic_classes_single = self.topic_model_single.config.id2label\n","\n","        self.tweet_df['topic_tokens'] = tokens\n","        print('loaded topic model')\n","        \n","        \n","\n","    @staticmethod\n","    def run_offensive_model(test):\n","        current = os.getcwd()\n","        new_dir = current+'/offensiveness'\n","\n","        os.chdir(new_dir)\n","        df_scraped = pd.read_csv('labeled_tweets.csv')\n","        df_public = pd.read_csv('public_data_labeled.csv')\n","        df_scraped.drop_duplicates(inplace = True)\n","        df_scraped.drop('id', axis = 'columns', inplace = True)\n","        df_public.drop_duplicates(inplace = True)\n","        df = pd.concat([df_scraped, df_public])\n","        df['label'] = df.label.map({'Offensive': 1, 'Non-offensive': 0})\n","        X_train, X_test, y_train, y_test = train_test_split(df['full_text'], \n","                                                    df['label'], \n","                                                    random_state=42)\n","\n","        os.chdir(current)\n","        # Instantiate the CountVectorizer method\n","        count_vector = CountVectorizer(stop_words = 'english', lowercase = True)\n","\n","        # Fit the training data and then return the matrix\n","        training_data = count_vector.fit_transform(X_train)\n","        testing_data = count_vector.transform(test)\n","        model = SGDClassifier()\n","        model.fit(training_data, y_train)\n","        preds = model.predict(testing_data)\n","        return preds\n","    \n","\n","    def load_offensive_model(self):\n","        test_data = self.tweet_df.text\n","        preds = self.run_offensive_model(test_data)\n","        self.tweet_df['offensive'] = preds\n","        print('loaded offensive model')\n","\n","\n","    def load_irony_model(self):\n","        task='irony'\n","        MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n","        tokenizer = AutoTokenizer.from_pretrained(MODEL)\n","        tokens = self.tweet_df.text.apply(lambda row: tokenizer(row, return_tensors='pt'))\n","        # download label mapping\n","        mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n","        with urllib.request.urlopen(mapping_link) as f:\n","            html = f.read().decode('utf-8').split(\"\\n\")\n","            csvreader = csv.reader(html, delimiter='\\t')\n","        self.irony_labels = [row[1] for row in csvreader if len(row) > 1]\n","        # PT\n","        with torch.no_grad():\n","            self.irony_model = AutoModelForSequenceClassification.from_pretrained(MODEL).to(self.device)\n","        self.tweet_df['cardiff_tokens'] = tokens\n","        print('loaded irony model for tweets')\n","\n","\n","\n","    def load_emoji_model(self):\n","        task='emoji'\n","        MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n","        tokenizer = AutoTokenizer.from_pretrained(MODEL)\n","        # download label mapping\n","        mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n","        with urllib.request.urlopen(mapping_link) as f:\n","            html = f.read().decode('utf-8').split(\"\\n\")\n","            csvreader = csv.reader(html, delimiter='\\t')\n","        self.emoji_labels = [row[1] for row in csvreader if len(row) > 1]\n","        with torch.no_grad():\n","            self.emoji_model = AutoModelForSequenceClassification.from_pretrained(MODEL).to(self.device)\n","        print('loaded emoji model')\n","        print('---------\\n---------\\n')\n","\n","    def load_politeness(self):\n","        ## Scoring each tweet based on politeness \n","        class SimpleDataset:\n","            def __init__(self, tokenized_texts):\n","                self.tokenized_texts = tokenized_texts\n","            \n","            def __len__(self):\n","                return len(self.tokenized_texts[\"input_ids\"])\n","            \n","            def __getitem__(self, idx):\n","                return {k: v[idx] for k, v in self.tokenized_texts.items()}\n","        from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n","\n","\n","        # Tokenize same way as training data\n","        model_name = 'roberta-base'\n","        path = f'politeness{os.path.sep}results/checkpoint-52500/'\n","        print('loaded politeness')\n","        tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","        print('politeness per tweet')\n","        with torch.no_grad():\n","            model = AutoModelForSequenceClassification.from_pretrained(path).to(self.device)\n","        tweets = self.tweet_df.grammartext.tolist()\n","        test_encodings = tokenizer(tweets , truncation=True, padding=True, max_length=256)\n","        test_dataset = SimpleDataset(test_encodings)       \n","\n","\n","        trainer = Trainer(model=model)\n","        predictions = trainer.predict(test_dataset)\n","        self.tweet_df['politeness'] = predictions[0]\n","        print('loaded politeness model for tweets')\n","        print('---------\\n---------\\n')\n","\n","        \n","\n","    def load_psysentimento_model(self):\n","            \n","        tweets = self.tweet_df.text.to_list()\n","\n","        # hateful\n","        analyzer = create_analyzer(task=\"hate_speech\", lang=\"en\")\n","        self.hate_labels = ['hateful', 'targeted', 'aggressive']\n","\n","        predictions = [ analyzer.predict(preprocess_tweet(txt)) for txt in tweets ]\n","        print('predicted hate of tweets')\n","        # predictions = process_(analyzer,tweets)\n","        self.tweet_df['hate_output'] = predictions\n","        \n","        print('loaded hate model')\n","\n","        #emotion\n","        e_analyzer = create_analyzer(task=\"emotion\", lang=\"en\")\n","        self.emo_labels = ['joy','sadness','others','anger','surprise','disgust','fear']     \n","\n","        # e_predictions = process_(e_analyzer,tweets)\n","        e_predictions = [ e_analyzer.predict(preprocess_tweet(txt)) for txt in tweets ]\n","        print('predicted emotion of tweets')\n","        self.tweet_df['emo_output'] = e_predictions\n","\n","        print('loaded emotion model')\n","\n","\n","#######################################\n","# MAIN FUNCTION TO RUN THE ANALYSIS\n","########################################\n","\n","\n","    def tweet_analysis(self):\n","\n","        informer_db = self.load_informer_data()\n","        self.get_device()\n","\n","        print('loaded informer data')\n","\n","        #####################\n","        # LOAD IN THE DATA!!!!!!!\n","        #####################\n","\n","        # ALL THE TWEETS IN THE MULTI-SOURCE CASE - store by tweet id\n","        all_tweets = self.get_the_tweets(informer_db)\n","        self.df = pd.DataFrame.from_dict(all_tweets, orient='index')\n","        self.df.drop_duplicates()\n","        user_ids = self.df.user_id.copy().tolist()\n","\n","        print('loaded in ms tweets')\n","\n","        # ALL THE USER FEEDS!!!!! STORED BY THE USER ID !!!!!\n","        all_users = self.get_users(informer_db)\n","        all_users,tweet_ids_feeds = self.add_feeds(all_users)\n","        ## ALL USER FEEDS!! STORED BY THE TWEET ID!! WILL!!!\n","        feeds = [ {'user_id':key, 'description':value['description'],'text':tweet['tweet-text'], 'tweet_id':tweet['id']  }  for key,value in all_users.items() for tweet in value['feed']  ]\n","        self.user_feeds = pd.DataFrame(feeds)\n","        self.user_feeds['tweet_ids'] = tweet_ids_feeds\n","        self.user_feeds.set_index('tweet_ids', inplace=True)\n","        print(f'\\n\\n\\n raw feed len {len(self.user_feeds)}')\n","        self.user_feeds.drop_duplicates()\n","        print(f'\\n\\n\\n feed len after drop duplibats {len(self.user_feeds)}' )\n","        self.user_feeds = self.user_feeds.loc[self.user_feeds['user_id'].isin(user_ids)]\n","        print(f'\\n\\n\\n feed len after not considering the feeds we didnt pull {len(self.user_feeds)}' )\n","\n","        print('loaded in necessary data')\n","\n","        #####################\n","        #####################\n","        ##############################################################################################################################\n","        ####################################################################################\n","        ############################################################################################################################################################################################################################################################\n","        # TO TEST FOR A SUBSET OF TWEETS\n","\n","        # # remove existing\n","\n","        # done_ids, done_users, og_df = self.load_existing()\n","        # print(f'\\n\\n\\nalready done {len(done_ids)} tweets')\n","        # self.df = self.df[~self.df.index.isin(done_ids)]\n","        # print(f'\\n\\n\\ now have {len(self.df)} tweets left to analyse')\n","\n","\n","        # yusers = self.df['user_id'].tolist()\n","        \n","        # user_ids =yusers\n","\n","\n","        # print(f'\\n\\n\\ reduced user feeds from {len(self.user_feeds)} ')\n","        # self.user_feeds = self.user_feeds.copy()[self.user_feeds.user_id.isin(user_ids)]\n","        # print(f'to {len(self.user_feeds)} ')\n","\n","\n","        ##################\n","        # DROPPING DUPLICATE TWEETS FROM BOTH DATAFRAMES....\n","        \n","        print(f'have {len(self.df)} tweets loaded in')\n","        self.df = self.df.copy()[~self.df.index.duplicated(keep='first')]\n","        print(f'now considering {len(self.df)} tweets')\n","\n","\n","        print(f'have {len(self.user_feeds)} tweets loaded in')\n","        self.user_feeds = self.user_feeds.copy()[~self.user_feeds.index.duplicated(keep='first')]\n","        print(f'now considering {len(self.user_feeds)} tweets')\n","\n","\n","        print('loading test data')\n","        ## END\n","        ##########################################\n","\n","        # load in ids\n","        feed_tweet_ids = list(self.user_feeds.index.values)\n","        tweet_ids = list(self.df.index.values)\n","        feed_user_ids = list(self.user_feeds['user_id'].astype(str))\n","\n","        self.df[['polarity', 'subjectivity']] = self.df['text'].apply(lambda Text: pd.Series(TextBlob(Text).sentiment))\n","        self.user_feeds[['polarity', 'subjectivity']] = self.user_feeds['text'].apply(lambda Text: pd.Series(TextBlob(Text).sentiment))\n","        self.user_feeds = self.tweet_cleaner(self.user_feeds)\n","\n","        self.tweet_df = self.tweet_cleaner(self.df.copy())\n","\n","        ##########################################################################\n","        ##########################################################################\n","        ### LOADING IN NECESSARY MODELS\n","\n","        \n","        if 'tweet_id' not in self.df:\n","            self.df['tweet_id'] = tweet_ids\n","\n","        if 'tweet_id' not in self.user_feeds:\n","            self.df['tweet_id'] = feed_tweet_ids\n","\n","        print('put tweet ids back in')\n","\n","        self.load_gender_model(tweet_ids,user_ids,feed_tweet_ids)\n","\n","        print('loaded gender ')\n","\n","\n","        loaders = [self.load_politeness, self.load_topic_model, self.load_irony_model, self.load_offensive_model, self.load_emoji_model, self.load_psysentimento_model]\n","\n","        [load() for load in loaders]\n","\n","        self.df = self.tweet_df.copy()\n","        del self.tweet_df\n","\n","        ########################################################################\n","        ### SAVE!!!! ################\n","\n","        save_path_t = f'tweets/{self.hashtag}/{self.hashtag}_TWEETS_scores.csv'\n","        # save_path_t = f'tweet_analysis/output/{self.hashtag}_tweets_classified2'\n","\n","        ########################################################################\n","\n","        del self.user_feeds # delete so code runs smooth\n","\n","        ########################################################################\n","        ########################################################################\n","        # CLASSIFY EACH TWEET IN MS\n","        ########################################################################\n","\n","        score_funcs = [ self.get_sentiment, self.get_grammar, self.get_readability, self.get_politeness, self.get_offensive, self.get_topic, self.get_topic_single, self.get_irony, self.get_emoji, self.get_emotion, self.get_hate ]\n","\n","        # SCORE EACH TWEET IN THE MULTI-SOURCE EVENT\n","\n","        tweet_df = self.df.copy()\n","\n","        t1 = time.time()\n","\n","        [ func(self.df,index,row) for func in score_funcs for index, row in tweet_df.iterrows() ]\n","\n","        t2 = time.time()\n","        print(f'finshed scoring tweets in {(t2-t1)/3600} hours')\n","        print('-------')\n","        print('---------------------')\n","        print('-------------------------------------------------')\n","        print('finished classifying all the tweets')\n","        print('-------------------------------------------------')\n","        print('---------------------')\n","        print('-------')\n","        #################################\n","\n","        ### SAVE AFTER CLASSIFYING ALL TWEETS\n","        ################################\n","        joblib.dump(self.df, save_path_t+'.pkl')\n","        \n","        print(f'DONE WITH {self.hashtag}')\n"]},{"cell_type":"markdown","metadata":{"id":"x27JS-gN8X-o"},"source":["## RUN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bbwJ9GEpucoO"},"outputs":[],"source":["# hashtags = ['avengers','blm','borisjohnson','brexit','climatechange','covid','gaza','loveisland','monkeypox','nhs','olivianewtonjohn','supercup','UkraineWar']\n","\n","hashtags = ['olivianewtonjohn','supercup']\n","\n","for hashtag in hashtags:\n","\n","    a = Analyzer(hashtag)\n","\n","    a.tweet_analysis()\n","\n","    print(f'----------------\\nDONE \\n{hashtag}hashtag  \\n----------------')"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyNkhuJInTCUM2wcEMqiIMLY"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}