{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"colab-analsis.ipynb","provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyNAl+JHbxmFV2ZXGsCdIXKv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd /content/drive/My Drive/MSc_project/hashtag-crawler"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"waCbTke7um7D","executionInfo":{"status":"ok","timestamp":1660656155568,"user_tz":-60,"elapsed":1559,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"}},"outputId":"910aa205-f5a2-4ca3-a75e-67b621ba73a7"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/My Drive/MSc_project/hashtag-crawler\n"]}]},{"cell_type":"code","source":["# !pip install language_tool_python\n","# !pip install pycountry\n","# !pip install langdetect\n","# !pip install nltk\n","!pip install tweet-preprocessor"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":190},"id":"6LO7JQlbt-Ih","executionInfo":{"status":"ok","timestamp":1660656141159,"user_tz":-60,"elapsed":3652,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"}},"outputId":"aa16d7ba-724c-4ae6-d8f5-c5bd3f582c8c"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tweet-preprocessor\n","  Downloading tweet_preprocessor-0.6.0-py3-none-any.whl (27 kB)\n","Installing collected packages: tweet-preprocessor\n","Successfully installed tweet-preprocessor-0.6.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["preprocessor"]}}},"metadata":{}}]},{"cell_type":"code","execution_count":13,"metadata":{"id":"7zqfjeT2SwRe","executionInfo":{"status":"ok","timestamp":1660662476539,"user_tz":-60,"elapsed":233,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"}}},"outputs":[],"source":["from enum import unique\n","from textblob import TextBlob\n","import sys\n","import tweepy\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","import os\n","import os.path\n","import nltk\n","import nltk.data\n","import time\n","import string\n","\n","# nltk.download('vader_lexicon')\n","# nltk.download('punkt')\n","# nltk.download('stopwords')\n","import json\n","\n","import language_tool_python\n","import preprocessor as p\n","\n","import pycountry\n","import re\n","import string\n","from wordcloud import WordCloud, STOPWORDS\n","from PIL import Image\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","from nltk import tokenize\n","from nltk import word_tokenize\n","from langdetect import detect\n","from nltk.stem import SnowballStemmer\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","class Analyzer(object):\n","    def __init__(self, hashtag):\n","\n","        self.hashtag = hashtag\n","        self.save_path = f'informer_results{os.path.sep}{hashtag}'\n","\n","        self.tool = language_tool_python.LanguageTool('en-US')\n","        self.alphabets= \"([A-Za-z])\"\n","        self.prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n","        self.suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n","        self.starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n","        self.acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n","        self.websites = \"[.](com|net|org|io|gov)\"\n","        # self.informer_db = informer_db\n","\n","    def load_informer_data(self):\n","        path = f'{self.hashtag}{os.path.sep}informer_database.json'\n","        with open(path) as jf:\n","            data = json.load(jf)\n","        return data\n","\n","    @staticmethod\n","    def get_the_tweets(database):\n","        all_tweets = {}\n","        for key,value in database.items():\n","            all_tweets.update( {key:value['tweet-text']})\n","            for informer in value['informers-data']:\n","                all_tweets.update({informer['id']:informer['tweet-text']})\n","        return all_tweets\n","\n","    @staticmethod\n","    def store_by_tweets(database):\n","        all_tweets = {}\n","        for key,value in database.items():\n","            if value in all_tweets:\n","                new = all_tweets[value].append(key)\n","                all_tweets[value] = new\n","            else:\n","                all_tweets[value] = [key]\n","\n","        return all_tweets\n","\n","        \n","    def tweet_cleaner(self,tw_list):\n","        remove_rt = lambda x: re.sub('RT @\\w+: ',\" \",x)\n","        rt = lambda x: re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",x)\n","        hash = lambda x: re.sub(r'#', \"\", x)\n","        amp = lambda x: re.sub(r'&amp', \"\", x)\n","\n","\n","        tw_list['grammartext'] = tw_list.text.map(remove_rt).map(rt)\n","        tw_list['clean_text'] = tw_list.text.map(remove_rt).map(rt)\n","        p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION, p.OPT.HASHTAG)\n","        tw_list[\"grammartext\"] = tw_list.grammartext.map(p.clean)\n","        p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION)\n","        tw_list[\"clean_text\"] = tw_list.clean_text.map(p.clean).map(hash).map(amp)\n","        tw_list[\"clean_text\"] = tw_list.clean_text.str.lower()\n","\n","        #Calculating tweet's lenght and word count\n","        tw_list['text_len'] = tw_list['clean_text'].astype(str).apply(len)\n","        tw_list['text_word_count'] = tw_list['clean_text'].apply(lambda x: len(str(x).split()))\n","        tw_list['punct'] = tw_list['clean_text'].apply(lambda x: self.remove_punct(x))\n","        tw_list['tokenized'] = tw_list['punct'].apply(lambda x: self.tokenization(x.lower()))\n","        tw_list['nonstop'] = tw_list['tokenized'].apply(lambda x: self.remove_stopwords(x))\n","        tw_list['stemmed'] = tw_list['nonstop'].apply(lambda x: self.stemming(x))\n","        return tw_list\n","\n","\n","    def get_sentiment(self,index,row):\n","        score = SentimentIntensityAnalyzer().polarity_scores(row.clean_text)\n","        neg = score['neg']\n","        neu = score['neu']\n","        pos = score['pos']\n","        comp = score['compound']\n","        if neg > pos:\n","            self.df.loc[index, 'sentiment'] = \"negative\"\n","        elif pos > neg:\n","            self.df.loc[index, 'sentiment'] = \"positive\"\n","        else:\n","            self.df.loc[index, 'sentiment'] = \"neutral\"\n","        self.df.loc[index, 'neg'] = neg\n","        self.df.loc[index, 'neu'] = neu\n","        self.df.loc[index, 'pos'] = pos\n","        self.df.loc[index, 'compound'] = comp\n","\n","\n","    def get_grammar(self,index, row):\n","        # https://michaeljanz-data.science/deepllearning/natural-language-processing/scoring-texts-by-their-grammar-in-python/\n","        scores_word_based_sentence = []\n","        scores_sentence_based_sentence = []\n","        s1 = time.perf_counter()\n","        sentences = nltk.tokenize.sent_tokenize(row.grammartext)\n","        e1 = time.perf_counter()\n","        # print(f'tokenizer took {e1-s1}s to complete')\n","        # print(sentences)\n","        # sentences = self.split_into_sentences(row)\n","        for sentence in sentences:\n","        # for sentence in helpers.text_to_sentences(text):\n","            s2 = time.perf_counter()\n","            matches = self.tool.check(sentence)\n","            e2 = time.perf_counter()\n","            # print(f'language tool took {e2-s2}s ')\n","            count_errors = len(matches)\n","            # only check if the sentence is correct or not\n","            scores_sentence_based_sentence.append(np.min([count_errors, 1]))\n","            scores_word_based_sentence.append(count_errors)\n","            \n","        word_count = len(nltk.tokenize.word_tokenize(row.grammartext))\n","        sum_count_errors_word_based = np.sum(scores_word_based_sentence)\n","        score_word_based = 1 - (sum_count_errors_word_based / word_count)\n","        \n","        sentence_count = len(sentences)       \n","        sum_count_errors_sentence_based = np.sum(scores_sentence_based_sentence)\n","        score_sentence_based = 1 - np.sum(sum_count_errors_sentence_based / sentence_count)\n","\n","        self.df.loc[index, 'grammar-word-scoure'] = score_word_based\n","        self.df.loc[index, 'grammar-sentence-score'] = score_sentence_based\n","\n","\n","    @staticmethod\n","    def count_values_in_column(data,feature):\n","        total=data.loc[:,feature].value_counts(dropna=False)\n","        percentage=round(data.loc[:,feature].value_counts(dropna=False,normalize=True)*100,2)\n","        return pd.concat([total,percentage],axis=1,keys=['Total','Percentage'])\n","\n","\n","    def make_sentiment_pi_chart(self,tw_list):\n","        # create data for Pie Chart\n","        pichart = self.count_values_in_column(tw_list,\"sentiment\")\n","        names= pichart.index\n","        size=pichart[\"Percentage\"]\n","        \n","        # Create a circle for the center of the plot\n","        my_circle=plt.Circle( (0,0), 0.7, color='white')\n","        plt.pie(size, labels=names, colors=['green','blue','red'])\n","        p=plt.gcf()\n","        p.gca().add_artist(my_circle)\n","        # plt.show()\n","        plt.savefig(f'{self.save_path}_sentiment_summarisation_pichart.png')\n","\n","    def create_wordcloud(self,text,label):\n","        path = f'{self.save_path}_wordcloud_{label}.png'\n","        stopwords = set(STOPWORDS)\n","        wordcloud = WordCloud(width=1600, height=800,max_font_size=200,max_words=1000,stopwords=stopwords, background_color='white', repeat=False).generate(str(text))\n","        plt.figure(figsize=(12,10))\n","        plt.imshow(wordcloud, interpolation=\"bilinear\")\n","        plt.axis(\"off\")\n","        plt.savefig(path)\n","        # plt.show()\n","\n","\n","    def get_wordclouds(self,tw_list):\n","        \n","        tw_list_negative = tw_list[tw_list[\"sentiment\"]==\"negative\"]['clean_text']\n","        tw_list_positive = tw_list[tw_list[\"sentiment\"]==\"positive\"]['clean_text']\n","        tw_list_neutral = tw_list[tw_list[\"sentiment\"]==\"neutral\"]['clean_text']\n","\n","        #Creating wordcloud for all tweets\n","        self.create_wordcloud(tw_list[\"clean_text\"].values,label='all')\n","\n","        #Creating wordcloud for positive sentiment\n","        self.create_wordcloud(tw_list_positive.values,label ='positive')\n","\n","        #Creating wordcloud for negative sentiment\n","        self.create_wordcloud(tw_list_negative.values,label = 'negative')\n","\n","        #Creating wordcloud for neutral sentiment\n","        self.create_wordcloud(tw_list_neutral.values,label = 'neutral')\n","\n","    def remove_punct(self,text):\n","        text = \"\".join([char for char in text if char not in string.punctuation])\n","        text = re.sub('[0â€“9]+', '', text)\n","        text = re.sub( \"'\", \"\",text)\n","        text = re.sub( '\"', '',text)\n","\n","        return text\n","\n","\n","    def remove_stopwords(self,text):\n","        self.stopword = nltk.corpus.stopwords.words('english')\n","        text = [word for word in text if word not in self.stopword]\n","        return text\n","\n","    def stemming(self,text):\n","        self.ps = nltk.PorterStemmer()\n","        text = [self.ps.stem(word) for word in text]\n","        return text\n","\n","    def clean_text(self,text):\n","        text_lc = \"\".join([word.lower() for word in text if word not in string.punctuation]) # remove puntuation\n","        text_rc = re.sub('[0-9]+', '', text_lc)\n","        tokens = re.split('\\W+', text_rc)    # tokenization\n","        text = [self.ps.stem(word) for word in tokens if word not in self.stopword]  # remove stopwords and stemming\n","        return text\n","\n","    @staticmethod\n","    def tokenization(text):\n","        text = re.split('\\W+', text)\n","        return text\n","\n","\n","    def get_density_of_words(self,tw_list):\n","\n","        #Appliyng Countvectorizer\n","        countVectorizer = CountVectorizer(analyzer=self.clean_text) \n","        countVector = countVectorizer.fit_transform(tw_list['stemmed'])\n","        print('{} Number of reviews has {} words'.format(countVector.shape[0], countVector.shape[1]))\n","        #print(countVectorizer.get_feature_names())\n","        count_vect_df = pd.DataFrame(countVector.toarray(), columns=countVectorizer.get_feature_names())\n","\n","        # Most Used Words\n","        count = pd.DataFrame(count_vect_df.sum())\n","        countdf = pd.DataFrame(count.sort_values(0,ascending=False))\n","        countdf.to_csv(f'{self.save_path}_most_used_words.csv')\n","\n","    @staticmethod\n","    def sort_by_tweet(all_tweets): \n","\n","        df = pd.DataFrame.from_dict(all_tweets, orient='index', columns= ['text'])\n","        sorted_tweets = {}\n","        for row,index in df.groupby('text').groups.items():\n","            key = tuple(index.values.tolist())\n","            sorted_tweets.update({key:row})\n","\n","        new_df = pd.DataFrame.from_dict(sorted_tweets, orient='index', columns= ['text'])\n","        \n","        return new_df\n","\n","\n","    def tweet_analysis(self):\n","\n","        print(f'Performing analysis on {self.hashtag}')\n","\n","        self.informer_db = self.load_informer_data()\n","\n","        all_tweets = self.get_the_tweets(self.informer_db)\n","        self.df = pd.DataFrame.from_dict(all_tweets, orient='index', columns= ['text'])\n","\n","        tweet_df = self.sort_by_tweet(all_tweets)\n","\n","        tweet_df = self.tweet_cleaner(tweet_df)\n","        self.df = self.tweet_cleaner(self.df)\n","\n","        self.df[['polarity', 'subjectivity']] = self.df['text'].apply(lambda Text: pd.Series(TextBlob(Text).sentiment))\n","\n","        # index the rows of the database that contains the unique tweets\n","        for index, row in tweet_df.iterrows():\n","\n","            self.get_sentiment(index,row)\n","            self.get_grammar(index,row)\n","\n","        print('finished getting the metrics!!!!!!!')\n","\n","        df_path = f'{self.save_path}_tweets_classification.csv'\n","        self.df.to_csv(df_path)\n","        \n","        # refilter the database to only contain the unique tweets\n","        unique_df = self.df.copy()\n","        unique_df.drop_duplicates('clean_text', inplace=True)\n","\n","        sentiment_count = self.count_values_in_column(unique_df,\"sentiment\")\n","        sentiment_count.to_csv(f'{self.save_path}_sentiment_summarisation.csv')\n","\n","        # make a pi chart of the result of sentiments\n","        self.make_sentiment_pi_chart(unique_df)\n","\n","        ### Create word clouds of all \n","        self.get_wordclouds(unique_df)\n","\n","        # get thedensity of the words used\n","        self.get_density_of_words(unique_df)\n"]},{"cell_type":"code","source":["hashtags = ['avengers','blm','borisjohnson','brexit','climatechange','covid','gaza','loveisland','monkeypox','nhs','olivianewtonjohn','supercup','UkraineWar']\n","# hashtag = 'gaza'\n","\n","informer_db_path = f'informer_results{os.path.sep}'\n","\n","for hashtag in hashtags:\n","\n","    a = Analyzer(hashtag)\n","\n","    a.tweet_analysis()"],"metadata":{"id":"XZQUHWlOuO3Z","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1MTbjxmAbgZCLqtbYgW6OGwhKHKs4KGxr"},"executionInfo":{"status":"ok","timestamp":1660666703405,"user_tz":-60,"elapsed":4220394,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"}},"outputId":"3c277949-fb5a-4dab-ef04-ea54a9ae54ed"},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}