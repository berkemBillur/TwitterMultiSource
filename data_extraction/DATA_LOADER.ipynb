{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":43303,"status":"ok","timestamp":1661075464822,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"},"user_tz":-60},"id":"8YS3hbMmfdOb","outputId":"012fb511-bd91-4ae3-ee20-7cf4892e0de2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting preprocessor\n","  Downloading preprocessor-1.1.3.tar.gz (4.2 kB)\n","Building wheels for collected packages: preprocessor\n","  Building wheel for preprocessor (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for preprocessor: filename=preprocessor-1.1.3-py3-none-any.whl size=4477 sha256=0964d64acee853c2d9af8bc11158320c025ced42fdd5abf0056c2cfb3bd0d5fe\n","  Stored in directory: /root/.cache/pip/wheels/0e/b7/36/aa37256db62b4bfd35a6f1b5536e9ba843f257b79dcbf3d5f1\n","Successfully built preprocessor\n","Installing collected packages: preprocessor\n","Successfully installed preprocessor-1.1.3\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tweepy==4.10.0\n","  Downloading tweepy-4.10.0-py3-none-any.whl (94 kB)\n","\u001b[K     |████████████████████████████████| 94 kB 2.6 MB/s \n","\u001b[?25hRequirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from tweepy==4.10.0) (3.2.0)\n","Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from tweepy==4.10.0) (1.3.1)\n","Collecting requests<3,>=2.27.0\n","  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n","\u001b[K     |████████████████████████████████| 62 kB 1.6 MB/s \n","\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy==4.10.0) (2022.6.15)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy==4.10.0) (2.10)\n","Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy==4.10.0) (2.1.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy==4.10.0) (1.24.3)\n","Installing collected packages: requests, tweepy\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.23.0\n","    Uninstalling requests-2.23.0:\n","      Successfully uninstalled requests-2.23.0\n","  Attempting uninstall: tweepy\n","    Found existing installation: tweepy 3.10.0\n","    Uninstalling tweepy-3.10.0:\n","      Successfully uninstalled tweepy-3.10.0\n","Successfully installed requests-2.28.1 tweepy-4.10.0\n","Mounted at /content/drive\n","/content/drive/My Drive/MSc_project/.MAIN\n"]}],"source":["!pip install preprocessor\n","!pip install tweepy==4.10.0\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd /content/drive/My Drive/MSc_project/.MAIN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xAsROycLB8Jt"},"outputs":[],"source":["import os\n","\n","hashtags = ['AJvsUsyk']\n","friend_path  = f'user-friends{os.path.sep}'\n","maxTweets = 10000\n","job = ['search']\n","key_paths = ['apikeys.txt', 'apikeys2.txt']\n","num_of_keys = [45,83]\n","storage_save_freq = 30\n","# num_of_keys = [1,1]\n","\n","\n","for i,hashtag in enumerate(hashtags):\n","\n","    c = crawler( job[i] , maxTweets, friend_path, hashtag, key_paths, num_of_keys,storage_save_freq)\n","\n","    # c.get_api_list('apikeys.txt', 45)\n","    \n","    c.stage1_extraction()\n"]},{"cell_type":"markdown","source":["## CRAWLER "],"metadata":{"id":"DzY8fjKNOxz6"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"qowW1xO7qyij"},"outputs":[],"source":["from matplotlib import collections\n","from psutil import users\n","import tweepy\n","from tweepy import OAuthHandler\n","import pandas as pd\n","import numpy as np\n","import datetime\n","from datetime import timedelta\n","from datetime import datetime\n","import json\n","import time\n","import glob\n","import os\n","from collections import defaultdict\n","import configparser\n","from copy import deepcopy\n","import multiprocessing\n","from joblib import Parallel, delayed\n","import time\n","import os.path\n","\n","\n","class crawler(object):\n","    def __init__(self, job, maxTweets,friend_path,hashtag,file_names,num_keys,storage_save_freq): \n","\n","        # consumer_key = str(consumer_key)\n","        # consumer_secret = str(consumer_secret)\n","        # access_token = str(access_token)\n","        # access_secret = str(access_secret)\n","\n","        self.maxTweets = maxTweets\n","        self.friends_path = friend_path\n","        self.hashtag = hashtag\n","        self.sep = os.path.sep  \n","        self.file_names = file_names\n","        self.num_keys = num_keys\n","        self.job = job\n","        self.storage_save_freq = storage_save_freq\n","\n","\n","################################################################################################################\n","################################################################################################################\n","################################################################################################################\n","\n","    @staticmethod\n","    def clean_date(dic):\n","        for key in dic:\n","            del dic[key]['check-idx']\n","        return dic\n","\n","    @staticmethod\n","    def process_searched_targets(tweet):\n","        d = {'id':tweet.id_str, 'tweet-text': tweet.full_text,'created_at': tweet.created_at, 'user-id':tweet.author.id,\n","        'check-idx':None,'informers-data':None,'friend-ids':None, 'location': tweet.author.location,\n","        'num-followers': tweet.author.followers_count, 'num-following':  tweet.author.friends_count, 'description': tweet.author.description,\n","        'retweet_count': tweet.retweet_count, 'favourite_count':tweet.favorite_count}\n","        return d\n","\n","    @staticmethod\n","    def process_loaded_targets(tweet):\n","        d = {'id':tweet['id_str'], 'tweet-text': tweet['full_text'],'created_at': tweet['created_at'], 'user-id':tweet['user']['id_str'],\n","        'check-idx':None,'informers-data':None,'friend-ids':None, 'location': tweet['user']['location'],\n","        'num-followers': tweet['user']['followers_count'], 'num-following':  tweet['user']['friends_count'], 'description': tweet['user']['description'],\n","        'retweet_count': tweet['retweet_count'], 'favourite_count':tweet['favorite_count']}\n","        return d\n","\n","    @staticmethod\n","    def process_searched_all(tweet):\n","        d = {'id':tweet.id_str, 'tweet-text': tweet.full_text,'created_at': tweet.created_at, 'user-id':tweet.author.id,\n","        'location': tweet.author.location,\n","        'num-followers': tweet.author.followers_count, 'num-following':  tweet.author.friends_count, 'description': tweet.author.description,\n","        'retweet_count': tweet.retweet_count, 'favourite_count':tweet.favorite_count}\n","        return d\n","\n","    @staticmethod\n","    def process_loaded_all(tweet):\n","        d = {'id':tweet['id_str'], 'tweet-text': tweet['full_text'],'created_at': tweet['created_at'], 'user-id':tweet['user']['id_str'],\n","        'location': tweet['user']['location'],\n","        'num-followers': tweet['user']['followers_count'], 'num-following':  tweet['user']['friends_count'], 'description': tweet['user']['description'],\n","        'retweet_count': tweet['retweet_count'], 'favourite_count':tweet['favorite_count']}\n","        return d\n","\n","\n","\n","    ##### FUCNTIONS TO WRITE\n","\n","\n","    def write_stats(self):\n","\n","        file1 = open(f\"{self.hashtag}{self.sep}tweets_description.txt\",\"w\")\n","        n = len(self.tweet_df)\n","        all_users = self.tweet_df['user-id'].to_dict()\n","        num_users = len(set(all_users))\n","\n","        last_date = self.tweet_df.iloc[0]['created_at']\n","        first_date =  self.tweet_df.iloc[n-1]['created_at']\n","\n","        file1.write(f'For the hashtag #{self.hashtag}\\nTotal Tweets Extracted: {n} \\nTotal Retweets Extracted: {len(self.all_retweets)}')\n","        file1.write(f'\\nCrawled tweets from {num_users} different users')\n","        file1.write(f'\\nAll tweets were found in a region of {first_date-last_date}\\nTime first tweet was posted {first_date} \\nTime Last tweets was posted {last_date}')\n","\n","        file1.close() #to change file access modes\n","\n","    def get_friends_db(self):\n","        jsons = [pos_json for pos_json in os.listdir(f'user-friends{self.sep}') if pos_json.endswith('.json')]\n","        all_js = {}\n","        for file in jsons:\n","            with open(os.path.join(f'user-friends{self.sep}' + file)) as jf:\n","                all_js = { **all_js, **json.load(jf) }\n","        print(f'pulled data on {len(all_js)} users')\n","        return all_js\n","\n","###########################################################\n","###########################################################\n","# API KEY FUNCTIONS\n","###########################################################\n","###########################################################\n","\n","\n","    def get_api_list(self,filename,num_keys):\n","        settings_file = f\"apikeys{self.sep}{filename}\"\n","        # Read config settings\n","        config = configparser.ConfigParser()\n","        config = configparser.ConfigParser(interpolation=None)\n","        config.readfp(open(settings_file))\n","\n","        # Create API objects for each of the API keys\n","        # 1-based indexing of config file\n","        start_idx = 1\n","        end_idx = num_keys\n","        num_api_keys = end_idx - start_idx + 1\n","\n","        apis = []\n","\n","        print(\"Creating api objects for {} API keys\".format(num_api_keys))\n","        for api_idx in range(start_idx, end_idx + 1):\n","            consumer_key = config.get('API Keys ' + str(api_idx), 'consumer_key')\n","            consumer_secret = config.get('API Keys ' + str(api_idx), 'consumer_secret')\n","            access_token_key = config.get('API Keys ' + str(api_idx),\n","                                        'access_token')\n","            access_token_secret = config.get('API Keys ' + str(api_idx),\n","                                            'access_secret')\n","\n","            # Connect to Twitter API\n","            try:\n","                auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n","                auth.set_access_token(access_token_key, access_token_secret)\n","                api = tweepy.API(auth, wait_on_rate_limit=True)\n","            except Exception as e:\n","                print(\"Error while creating API object: \" + str(e))\n","                continue\n","            else:\n","                apis.append(api)\n","        print(f'found {len(apis)} apis from {filename}')\n","        idx = []\n","        for i, apii in enumerate(apis):\n","            try: \n","                a = apii.verify_credentials()\n","            except tweepy.TweepyException as e:\n","                idx.append(i)\n","                # print(i+1)\n","                # print(e)\n","        \n","        working_api = np.delete(apis,idx)\n","        print(f'{len(working_api)} api work')\n","        return working_api.tolist()\n","\n","    def make_api(self):\n","        consumer_key = '3rJOl1ODzm9yZy63FACdg'\n","        consumer_secret = '5jPoQ5kQvMJFDYRNE8bQ4rHuds4xJqhvgNJM4awaE8'\n","\n","        tokens=\"1273621486201450497-2KsGK9JgSSdabGYxFlDNB214MXwi35:kbLULiG4k7SvUlwFkr0p6ESTdvARIThfmI1lQ5GuQmH8s,\"\n","        # tokens+=\"1186873796982128641-RHYpdyKJIfSX6KtRfA6k73emHMOhBY:Zth72rrtcZGfqJ4B9TK8dH1RhS8HC6KOlTDqA3bJnMnGL,\" DOES NOT WORK\n","        tokens+=\"1186715756437868546-U9LOFzfSEk4tTOYHoKn7X08vWFm3An:kWWnOy6JLbHI9nG8GihGIjB1SeH5nJE8oqi4KiRIiHB2B,\"\n","        tokens+=\"1186707518799650816-KOvTi7HOrZ1KiVRXyAvNjpJofF3lY8:EsXAVrtyqkekdmtr0GcZCiVk9RwAJC4v9Kj3fo0RWPpgp,\"\n","        tokens+=\"1186873046726041600-rNJoDJulul6m5GQUbXUG7zNfpnJywj:OsR7WIHZ9PEdlmIE7K36V1CrM3OiMKIzAboNXMdEJm1UJ,\"\n","        tokens+=\"1186872285782794240-X4x7t8KRaf3Ce3oNI2ka7DO7rapVys:VUyNnBIRr6oRpBr10lbEF0MzlGz6eJJW3STUWOGyXgVbr,\"\n","        tokens+=\"1261898258739126273-Dw4HYHPkKj82V4RcgYhpxLsnJdMALV:384AYTeZqRykw5ADdVXhi1BXZadOuflhzOiKljh1Pt0bi,\"\n","        tokens+=\"1261898258739126273-Dw4HYHPkKj82V4RcgYhpxLsnJdMALV:384AYTeZqRykw5ADdVXhi1BXZadOuflhzOiKljh1Pt0bi,\"\n","        tokens+=\"1261901714564583424-odKz6Xonlzg1QVg32o6hdqXbGAZ6nt:lx7hysS64y7kyc0pr4IMdqXNJr8yqbgY9vZ7qhnL6ldGY,\"\n","        tokens+=\"1261905369485373440-mxYEQlU9tEjFvFClg3YTZU2Krh4ezO:mcvotgeJvuOgkJb5pXejP9qzjDBcALZSHqGJesH7FzOOs,\"\n","        tokens+=\"1261908827961585664-YkhHP2z6XczyTwBt8pT7IYmYw4XFTN:OQJhcBCQiHWCOS1lcUHlZp8Minc7drmpW861tFNwweABC,\"\n","        tokens+=\"1261911771528364032-uFSYzCtvZeQPLvSIR2NZM3MwYnPDx9:gjLJUDRkiC6qVhh6zN8czDLzcE3Bnhge9Sgq7QIQsRfZp,\"\n","        tokens+=\"1261913654967992321-dq90nrKd8OTIAiBwK4rmDPR1hvBwTb:6lL8f15qO9DOMczUzImxwUjMmDvPHlQcd3eJBtUzQ53ck,\"\n","        tokens+=\"1261918659942871040-ThZsmIIeIWLfTuhNB8uD3HORhunMVS:D3QNGphJ1R870yzBV41QSvezqS6rysRVgEG1RU3c7tH30,\"\n","        tokens+=\"1261929369691283456-0CdW3UgD4ONxFCshwmBqroq3m8BFG7:YpeYZ3hWRNkFOzfoJGiIPXnW65vzul5e7ofDN1q3zZErV,\"\n","        tokens+=\"1261930395513192448-14EJHzMXDGzDN65NAx9HLXaz8Y1OHf:97qLbZ53tPQl5xwmNPDBlADBrSlZZeXd659N5XXDG2O5E,\"\n","        tokens+=\"1261931615837581317-Bdgkok64OnA0HWB9q3NZfFF12FpBjD:qeYMydCMxt2ctLOkfNV3yix6erJoxti7NiOScNCQ0zHf6,\"\n","        tokens+=\"1261932804046114816-5s1MVYfHsfwjxz1pZ2jhLE3jgO0enO:8XmjhdkKTakxF9a3ZKCMYJzlNUb9ARChEZ74XH91qZVqg,\"\n","        tokens+=\"1259732232433856512-knm0EqhoG7tyEjTsgIQc6974cEy976:lavQjBjsOdZgUy5U64ZtK8YYdfwQeknl9Oy2CP7nJ3uE9,\"\n","        tokens+=\"1262372803174531072-uyPlAvb0BHwSuB1kgPOroGMyjFPtzj:Bsnfo8WTYNqEzr5S9fW9gzfWFc9E2ZqM0wXwUGIUgSwPo,\"\n","        tokens+=\"1262374702003044353-GCVqUDfNPkRGnBzmKHTPeZmmsBHjxx:pzqueROwD89WH3hZyr7qDV4ftnu8OPZM3OfhyNSM4pi9T,\"\n","        tokens+=\"1261903040946171905-mLBFRyOpShKunFrxk5o5DDoJERTEE1:Osvx77SI7qS0qIIYqlI43eLOCOS6UL1e3XSYcUyrYpCHz\"\n","\n","        connections=[]\n","\n","        for tk in tokens.split(\",\"):\n","            token=tk.split(\":\")\n","            auth = tweepy.OAuth1UserHandler(consumer_key, consumer_secret,token[0], token[1])\n","            api = tweepy.API(auth, wait_on_rate_limit=True)\n","            connections.append(api) \n","\n","        print(f'\\nfound {len(connections)} apis from juans stuff')\n","        idx = []\n","        for i, apii in enumerate(connections):\n","            try: \n","                a = apii.verify_credentials()\n","            except tweepy.TweepyException as e:\n","                idx.append(i)\n","                # print(e)\n","        \n","        working_api = np.delete(connections,idx)\n","        print(f'{len(working_api)} api work')\n","        return working_api.tolist()\n","\n","\n","####################################################################################\n","\n","# FUNCTINOS INVOLVING GETTING THE TWEETS WITH HASHTAGS\n","\n","####################################################################################\n","\n","    def load_in_tweets(self):\n","        path = f'{self.hashtag}{os.path.sep}{self.hashtag}_all_tweets.json'\n","        with open(path) as jf:\n","            data = json.load(jf)\n","\n","        return data\n","\n","    def search_for_tweets(self):\n","\n","        api_counter = 1\n","        start = time.perf_counter()\n","        print(f'\\nSearching for tweets with hashtag {self.hashtag}...')\n","\n","        tweets = tweepy.Cursor(self.all_api[api_counter].search_tweets,\n","                                q=f'#{self.hashtag}',\n","                                tweet_mode='extended', \n","                                count = 100,\n","                                lang='en').items(self.maxTweets)\n","\n","        all_tweets = []\n","        tweet_counter = 0\n","        for tweet in tweets:\n","            tweet_counter +=1 \n","            if tweet_counter == 179:\n","                api_counter +=1 \n","            \n","            all_tweets.append(tweet)\n","\n","        fin = time.perf_counter()\n","        total = fin - start\n","        total /= 60\n","        print(f'search took {total} mins to complete')\n","        ### Save all extracted tweets to directory\n","        json_save = [ tweet._json for tweet in all_tweets ]\n","        directory = f'{self.hashtag}{self.sep}'\n","        if not os.path.exists(directory):\n","            os.makedirs(directory)\n","        with open(f'{self.hashtag}{self.sep}{self.hashtag}_all_tweets.json', 'w') as fp:\n","            json.dump(json_save,fp)\n","        return all_tweets\n","\n","\n","    ##############################################################################################################################\n","    ##############################################################################################################################\n","    ##############################################################################################################################\n","        '''\n","\n","        THE MAIN FUNCTION!!!!!!!!!!!!!!!!!!!!!!!!\n","        \n","        '''\n","    ##############################################################################################################################\n","    ##############################################################################################################################\n","    ##############################################################################################################################\n","\n","    def stage1_extraction(self):\n","\n","        # juan_api = self.make_api()\n","\n","        everyone_api = self.get_api_list(self.file_names[0],self.num_keys[0])\n","        just_my_api = self.get_api_list(self.file_names[1],self.num_keys[1])\n","\n","        all_api = just_my_api + everyone_api #+ juan_api\n","        self.all_api = all_api\n","        num_api = len(all_api)\n","        print(f'total working api is {num_api}')\n","        #  we will keep track of the api that we use\n","\n","        ################################################################################################################\n","        ########################################################\n","        ### PROCESS INTO NECESSARY FORMAT\n","\n","        if self.job == 'search':\n","            # WHEN SEARCHING FOR TWEETS\n","            ########################################################\n","            tweet_list = self.search_for_tweets()\n","\n","            self.all_tweets = {tweet.id: self.process_searched_all(tweet) for tweet in tweet_list}\n","\n","            self.tweet_df = pd.DataFrame.from_dict(self.all_tweets, orient='index')\n","            self.tweet_arr = np.array(list(self.all_tweets.items()))\n","            self.all_retweets = {tweet.id:self.process_searched_targets(tweet) for tweet in tweet_list if \"RT @\" in tweet.full_text }\n","            print(f'found {len(self.tweet_df)} tweets all together\\nfound {len(self.all_retweets)} retweets')\n","\n","            ##### FIND POTENTIAL INFORMERS AMONGST THE DATA\n","            # print(f'we have {num_cores} at our disposal')\n","            print('\\nnow finding tweets that are posted 24 hours before, for each tweet')\n","            print('please work ya whoooore')\n","\n","            # change the string format to datetime\n","            self.tweet_df['created_at'] = pd.to_datetime( self.tweet_df['created_at'], format =  '%a %b %d %H:%M:%S +%f %Y' )\n","            self.write_stats()\n","\n","            self.find_tweets_posted_before()\n","\n","            # with open(f'{self.hashtag}{self.sep}{self.hashtag}_all_retweets.json', 'w') as fp:\n","            #     json.dump(self.all_retweets,fp)\n","\n","        else:\n","            # WHEN LOADING TWEETS IN\n","            tweet_list = self.load_in_tweets()\n","            ########################################################\n","\n","            self.all_tweets = {tweet['id']: self.process_loaded_all(tweet) for tweet in tweet_list}\n","\n","            self.tweet_df = pd.DataFrame.from_dict(self.all_tweets, orient='index')\n","            self.tweet_df['created_at'] = pd.to_datetime( self.tweet_df['created_at'], format =  '%a %b %d %H:%M:%S +%f %Y' )\n","\n","            self.tweet_arr = np.array(list(self.all_tweets.items()))\n","\n","            self.all_retweets = {tweet['id']:self.process_loaded_targets(tweet) for tweet in tweet_list if \"RT @\" in tweet['full_text'] }\n","\n","            self.write_stats()\n","\n","            self.find_tweets_posted_before()\n","\n","        \n","        ################################################################################################################\n","        ###### \n","        ##### CHECK IF WE HAVE DATA ON USERS WE HAVE ALREADY DONE!!!! \n","        ######\n","        ################################################################################################################\n","\n","        print(len(self.all_retweets))\n","        users_done_data = self.get_friends_db()\n","\n","        users_done_ids = list( users_done_data.keys() )\n","\n","        current_ids = [ value['user-id'] for _,value in self.all_retweets.items() ]\n","\n","        already_done = list(set(users_done_ids).intersection(current_ids))\n","\n","        targets_with_friends = {}\n","\n","        if already_done: # have users info in our database!!!!\n","\n","            print(f'we already have data for {len(already_done)} users!! Removing them from API search')\n","\n","            targets_without_friends = deepcopy(self.all_retweets)\n","\n","            all_retweets_df = pd.DataFrame.from_dict(self.all_retweets, orient='index')\n","\n","            for user_id in already_done:\n","\n","                # entries with those user ids\n","                entries = all_retweets_df.loc[ all_retweets_df['user-id'] == user_id].to_dict(orient='index')\n","                for key in entries:\n","                    del targets_without_friends[key]\n","\n","                for key in entries:\n","                    entries[key]['friend-ids'] = users_done_data[user_id]['friend-ids']\n","\n","                targets_with_friends.update(entries)\n","\n","            print(f'num of tweets w friends {len(targets_with_friends)}')\n","            print(f'num of tweets w no friends {len(targets_without_friends)}')\n","\n","        else:\n","            print(f'oops we got no data on this batch')\n","            targets_without_friends = self.all_retweets\n","\n","\n","\n","        #################################################################\n","        \n","        ### NOW GET THE LIST OF THE ACCOUNTS EACH TWEET AUTHOR FOLLOWS.\n","        \n","        #################################################################\n","\n","        \n","\n","        print(f'Getting the accounts that user of each target tweet follows')\n","        start = time.perf_counter()\n","        storage = {}\n","        count_users_pulled = 1 # counter count the number of accounts we request to pull in this batch\n","        num_api = len(all_api)\n","        api_idx = 0\n","        unique_users = {}\n","\n","        for tweet_id, data in list(targets_without_friends.copy().items()):\n","\n","            if count_users_pulled % 3: # can only make 15 requests per API. So after 15 requests, move onto next API\n","                print('using next api')\n","                api_idx +=1     \n","\n","            if count_users_pulled % self.storage_save_freq == 0: # once we have collected the information of 500 users. Dump the information to a json\n","                print(f'\\n\\n HAVE FOUND {count_users_pulled} users friends!! The code still works!! Wow!!\\n\\n')\n","                stamp = datetime.today().strftime('%Y%m%d_%H%M%S')\n","                with open(f'{self.friends_path}{self.sep}{self.hashtag}_dump_{stamp}.json', 'w') as fp:\n","                    json.dump(storage,fp)\n","                storage.clear()\n","\n","            if api_idx == num_api-1: # we have made one cycle through all the api\n","                api_idx = 0     \n","                print('gone through all api... should now reach a limit')\n","\n","            followings = [] # initialise the list of friends each user will have\n","            user = data['user-id']\n","\n","            if  user in unique_users: # if we have this users friends, then pull it\n","                targets_without_friends[tweet_id]['friend-ids'] = unique_users[user]['friend-ids']\n","                print('done this mug')\n","                continue\n","\n","            else: # the user has not already been found\n","                try:\n","                    # api_idx = np.random.randint(0,num_api)\n","                    print(f'using api number {api_idx}')\n","                    for friend in tweepy.Cursor(all_api[api_idx].get_friend_ids, user_id=user, count = 4999).items():\n","                        followings.append(friend)\n","                        # update the unique users file\n","\n","                    unique_users.update({user:{'friend-ids':followings}} )\n","                    targets_without_friends[tweet_id]['friend-ids'] = followings\n","                    print(count_users_pulled)\n","                    count_users_pulled+=1 # have collected data on one more user\n","                    # store this users friends in our storgae dictionary. this will later be dumped into \n","                    storage.update( {user: {'friend-ids':followings}} )\n","                    if not followings: # if the user has no following accounts\n","                        del targets_without_friends[tweet_id]\n","                except tweepy.TweepyException as e: # we've got an error with the tweepy API\n","                    print(e) \n","                    print(f'get rid of api number {api_idx}')\n","                    del all_api[api_idx]\n","                    num_api -=1 # working with one less api\n","                    api_idx += 1 # move onto the next\n","\n","        fin = time.perf_counter()\n","        print(f'finished extracting friends in {(fin-start)/3600} hours')\n","\n","        ########################################################################\n","        ########################################################################\n","        ########################\n","        ##### FINDING THE FRIENDS OF INFORMERS\n","        ########################\n","        ########################################################################\n","        ########################################################################        \n","\n","\n","        self.multi_source_cases = {**targets_without_friends,  **targets_with_friends }\n","\n","        found_multisource_cases = self.check_formulti_source()\n","\n","        if not found_multisource_cases:\n","            print('Found no informers in this search. Sorry Berkem.')\n","        else: # WE ACTUALLY FOUND SOME DATA!!! WOOHOOOOOO\n","            print(f'Found all informers for this batch of tweets with hashtag {self.hashtag}')\n","            print(f'In total found {len(found_multisource_cases)} multi-source cases \\nOnto the next')\n","\n","            file1 = open(f\"{self.hashtag}{os.path.sep}tweets_description.txt\",\"a\")\n","            file1.write(f'\\n\\nFOUND {len(found_multisource_cases)} MULTI-SOURCE CASES')\n","            file1.close()\n","\n","            # SAVE THIS TO JSON DERULO\n","            save_path = f'{self.hashtag}{os.path.sep}informer_database.json'\n","            with open(save_path, 'w') as fp:\n","                json.dump(found_multisource_cases, fp)\n","\n","\n","\n","    def check_formulti_source(self):\n","\n","        # multi_source_df = pd.DataFrame.from_dict(self.multi_source_cases, orient = 'index' )\n","\n","        found_multi_source = {}\n","\n","        ### Check the target tweets for users that follow them in this database.\n","        for key, value in list(self.multi_source_cases.copy().items()):\n","\n","            if value['friend-ids'] is not None:\n","\n","                users_friends = value['friend-ids']\n","                users_friends = [str(user) for user in users_friends]\n","                set_friend = set(users_friends)\n","\n","                check_tweets = value['check-idx']\n","\n","                if check_tweets: \n","\n","                    check_tweets = [ int(c) for c in check_tweets]\n","                    \n","                    check_users = [ self.all_tweets[tweet_id]['user-id'] for tweet_id in  check_tweets]\n","\n","                    checks_dic = { k:v for k,v in zip (check_users,check_tweets) }\n","\n","                    check = set_friend.intersection(check_users)\n","\n","                    if check: # if we've found a friend of the current user\n","\n","                        print(f'FOUND {len(check)} informers here')\n","\n","                        informer_sources = [ self.all_tweets[ checks_dic[key] ] for key in check ]\n","\n","                        value['informers-data'] = informer_sources\n","\n","                        found_multi_source.update( {key:value}  )\n","                        \n","        for key in found_multi_source:\n","            del found_multi_source[key]['check-idx']\n","\n","        return found_multi_source\n","\n","\n","    def find_tweets_posted_before(self):\n","\n","        for key, value in list(self.all_retweets.copy().items()):\n","\n","            if self.job == 'load':\n","                tweet_time =  datetime.strptime( value['created_at'] , '%a %b %d %H:%M:%S +%f %Y')\n","            else: \n","                tweet_time =  value['created_at']\n","\n","            tweets_before = self.tweet_df.loc[(self.tweet_df['created_at'] < tweet_time) \\\n","                & (tweet_time - self.tweet_df['created_at'] <  timedelta(days=1) ) ].index.values.tolist()\n","\n","            # c1 = ( tweet_time - all_tweet_times < timedelta(days=1)).values.astype(int)\n","            # c2 = ( all_tweet_times < tweet_time ).values.astype(int)\n","\n","            if not tweets_before:\n","                self.all_retweets[key]['check-idx'] = []\n","                print(f'dropped tweet {key} from consideration. Has no tweets that were posted 24 hours before')\n","            else:\n","                self.all_retweets[key]['check-idx'] = [ str(tweet_id) for tweet_id in tweets_before ] \n","        print('tweets potential informers found. target tweet contains indices of the potential informers that are posted 24 hrs before')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SHTmC-7LekkS"},"outputs":[],"source":["# CHECK THE TWITTER API\n","\n","from matplotlib import collections\n","from psutil import users\n","import tweepy\n","from tweepy import OAuthHandler\n","import pandas as pd\n","import numpy as np\n","import datetime\n","from datetime import timedelta\n","from datetime import datetime\n","import json\n","import time\n","import glob\n","import os\n","from collections import defaultdict\n","import configparser\n","from copy import deepcopy\n","import multiprocessing\n","from joblib import Parallel, delayed\n","import time\n","import os.path"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nqkvl0Woe-Y3"},"outputs":[],"source":["def get_api_list(filename,num_keys):\n","    settings_file = f\"apikeys{os.path.sep}{filename}\"\n","    # Read config settings\n","    config = configparser.ConfigParser()\n","    config = configparser.ConfigParser(interpolation=None)\n","    config.readfp(open(settings_file))\n","\n","    # Create API objects for each of the API keys\n","    # 1-based indexing of config file\n","    start_idx = 1\n","    end_idx = num_keys\n","    num_api_keys = end_idx - start_idx + 1\n","\n","    apis = []\n","\n","    print(\"Creating api objects for {} API keys\".format(num_api_keys))\n","    for api_idx in range(start_idx, end_idx + 1):\n","        consumer_key = config.get('API Keys ' + str(api_idx), 'consumer_key')\n","        consumer_secret = config.get('API Keys ' + str(api_idx), 'consumer_secret')\n","        access_token_key = config.get('API Keys ' + str(api_idx),\n","                                    'access_token')\n","        access_token_secret = config.get('API Keys ' + str(api_idx),\n","                                        'access_secret')\n","\n","        # Connect to Twitter API\n","        try:\n","            auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n","            auth.set_access_token(access_token_key, access_token_secret)\n","            api = tweepy.API(auth, wait_on_rate_limit=True)\n","        except Exception as e:\n","            print(\"Error while creating API object: \" + str(e))\n","            continue\n","        else:\n","            apis.append(api)\n","    print(f'found {len(apis)} apis from {filename}')\n","    idx = []\n","    for i, apii in enumerate(apis):\n","        try: \n","            a = apii.verify_credentials()\n","        except tweepy.TweepyException as e:\n","            idx.append(i)\n","            # print(i+1)\n","            # print(e)\n","    \n","    working_api = np.delete(apis,idx)\n","    print(f'{len(working_api)} api work')\n","    return working_api.tolist()"]}],"metadata":{"colab":{"background_execution":"on","collapsed_sections":[],"machine_shape":"hm","name":"DATA_LOADER.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.9.7 ('msc-disso')","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.7"},"vscode":{"interpreter":{"hash":"b0ae7f71d21f34de1471e5367b3add59dad8c05025ace26ae0bb1f219f8110f4"}}},"nbformat":4,"nbformat_minor":0}