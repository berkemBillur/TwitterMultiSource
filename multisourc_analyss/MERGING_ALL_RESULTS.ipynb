{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28098,"status":"ok","timestamp":1663720994072,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"},"user_tz":-60},"id":"njrEJjvc8DnR","outputId":"a981d9ed-cd51-4c63-e467-a0c088c03a36"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/My Drive/MSc_project/.MAIN\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd /content/drive/My Drive/MSc_project/.MAIN"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":5721,"status":"ok","timestamp":1663720999775,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"},"user_tz":-60},"id":"oMVpbM5wfrL8"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\berke\\anaconda3\\envs\\env-pytorch\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n","c:\\Users\\berke\\anaconda3\\envs\\env-pytorch\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n","c:\\Users\\berke\\anaconda3\\envs\\env-pytorch\\lib\\site-packages\\numpy\\.libs\\libopenblas.JPIJNSWNNAN3CE6LLI5FWSPHUT2VXMTH.gfortran-win_amd64.dll\n","  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"]}],"source":["import pandas as pd \n","import pickle\n","import json\n","import os\n","import os.path\n","import numpy as np\n","import seaborn as sns\n","from collections import defaultdict\n","import joblib\n","import matplotlib.pyplot as plt\n","from matplotlib.ticker import (\n","                               AutoMinorLocator,\n","                               FuncFormatter,\n","                               )\n","import matplotlib.dates as mdates\n","from matplotlib.dates import DateFormatter\n","import torch\n","import re\n","%matplotlib inline\n","\n","import os\n","\n","os.chdir('G:\\My Drive\\MSc_project\\.MAIN')"]},{"cell_type":"markdown","metadata":{"id":"Q8KdWQ6JcjrV"},"source":["## analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uKxlXpKKco1c","outputId":"8b1f1601-8b0d-4d88-aee5-2b0b30a887dd"},"outputs":[{"name":"stdout","output_type":"stream","text":["loaded the scores\n","loaded the MS cases\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\berke\\AppData\\Local\\Temp\\ipykernel_17944\\3910615519.py:150: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n","  scores = scores_df.T.to_dict('list')\n"]},{"name":"stdout","output_type":"stream","text":["Out of 3862 MS cases \n","We found. We got the scores for 3862 of them.\n","target count: 3862\n","infector count: 3862\n","informer count: 25355\n"]}],"source":["class Analyzer(object):\n","    def __init__(self):\n","\n","        # self.hashtags = ['avengers','gaza','borisjohnson','brexit','climatechange','covid','gaza','loveisland','monkeypox','nhs','olivianewtonjohn','supercup','UkraineWar']\n","        self.hashtags = ['blm']\n","\n","    @staticmethod\n","    def load_ms_cases(hashtag):\n","        path = f'tweets{os.path.sep}{hashtag}{os.path.sep}{hashtag}_ms_cases.json'\n","        with open(path) as jf:\n","            data = json.load(jf)\n","        return data\n","\n","\n","    @staticmethod\n","    def sort_cols(df):\n","        emos = ['others','joy','surprise','disgust','anger','fear','sadness']\n","        for emo in emos:\n","            df[emo] = df.apply( lambda row: re.search(f'{emo}: (\\d*\\.?\\d+)', row.emo_output).group(1), axis=1)\n","            \n","        hates = ['hateful','targeted','aggressive']\n","        for hate in hates:\n","            df[hate] = df.apply( lambda row: re.search(f'{hate}: (\\d*\\.?\\d+)', row.hate_output).group(1), axis=1)\n","        \n","        return df\n","\n","\n","\n","    def load_scores_df(self,hashtag):\n","        save_path_all = f'tweets/{hashtag}/{hashtag}_TWEETS_scores.csv'\n","        df =  pd.read_csv(save_path_all)\n","        df.tweet_id.drop_duplicates(keep='first',inplace=True)\n","        df.set_index('tweet_id', inplace = True)\n","\n","\n","        save_path = f'tweets/{hashtag}/{hashtag}_infector_scores.csv'\n","        i_df =  pd.read_csv(save_path)\n","        i_df.tweet_id.drop_duplicates(keep='first',inplace=True)\n","        i_df.set_index('tweet_id', inplace = True)\n","\n","\n","        save_gender = f'tweets/{hashtag}/{hashtag}_gender_scores.csv'\n","        g_df =  pd.read_csv(save_gender)\n","        g_df.tweet_id.drop_duplicates(keep='first',inplace=True)\n","        g_df.drop(['text'], axis=1)\n","        g_df.set_index('tweet_id', inplace = True)\n","\n","        fear_path = f'tweets/{hashtag}/{hashtag}_fear_scores.csv'\n","        fear_df =  pd.read_csv(fear_path)\n","        fear_df.tweet_id.drop_duplicates(keep='first',inplace=True)\n","        fear_df.drop(['text','user_id'], axis=1,inplace=True)\n","        fear_df.set_index('tweet_id', inplace = True)\n","\n","        sub_all =pd.concat( [df,i_df],axis=0 ) # adding the infector scores to the database. so new entries\n","\n","        all = pd.concat( [sub_all,g_df, fear_df], axis = 1) # adding the gender scores to all types\n","        \n","\n","        d = [ 'Unnamed: 0','clean_text','punct','tokenized','nonstop','stemmed','topic_tokens','cardiff_tokens','hate_output','emo_output','grammartext']\n","\n","        \n","        all.drop(d,inplace=True,axis=1)\n","\n","        all = all.loc[:,~all.columns.duplicated()].copy()\n","\n","        print('loaded the scores')\n","        \n","        return all\n","\n","\n","    def get_the_tweets(self,hashtag):\n","        database = self.load_ms_cases(hashtag)\n","        cases = {}\n","        for key,value in database.items():\n","\n","            inf = value['infector-info']\n","            k = list(inf.keys())[0]\n","            infector = inf[k]\n","\n","\n","            informers = [ int(inf['id']) for inf in value['informers-data']]\n","\n","            dum =  {   int(key): { 'target':int(key) , 'infector': int(infector['id']), 'informers':informers } } # get the case breakdown of target infector informer\n","            cases.update( dum  )\n","            \n","        print('loaded the MS cases')\n","        return cases \n","                   \n","\n","    @staticmethod\n","    def arange_case(scores,cases):\n","        all = []\n","        target_count = 0 \n","        infector_count = 0 \n","        informer_count = 0\n","\n","        count=0\n","\n","        for key,value in cases.items():\n","            # each row is organised as follows\n","            # target id.    is target,    is infector, is informer, score\n","\n","            target = key\n","            infector = value['infector']\n","            all_informers = [informer for informer in value['informers'] ]\n","\n","            if target in scores:\n","                target_count += 1\n","\n","            if infector in scores:\n","                infector_count += 1\n","\n","            for informer in all_informers:\n","                if informer  in scores:\n","                    informer_count += 1\n","                \n","\n","            # if (all(target, and infector in scores) and (any(informers in scores)) :\n","            if ( target and infector in scores ) and any( informer in scores.keys() for informer in all_informers ):\n","                count+=1\n","                \n","                all.append( [ key, 1, 0 ,0] + scores[key]  )                 # target row\n","\n","                all.append( [ key, 0, 1 ,0] + scores[infector]  )         # infector row\n","\n","                informers = [informer for informer in all_informers if informer in scores]\n","\n","                for informer in informers:\n","                    all.append( [ key, 0, 0, 1] +  scores[informer]  )\n","\n","        print(f'Out of {len(cases)} MS cases \\nWe found. We got the scores for {count} of them.')\n","        print(f'target count: {target_count}')\n","        print(f'infector count: {infector_count}')\n","        print(f'informer count: {informer_count}')\n","\n","        return all   \n","\n","\n","\n","    def create_ms(self):\n","\n","        for i, hashtag in enumerate(self.hashtags):\n","\n","            #load scores\n","            scores_df = self.load_scores_df(hashtag) # these are indexed by integer number 1,2,3,4. Tweet ids here are integers.\n","            cases = self.get_the_tweets(hashtag) # indexed by tweet ids. INTEGER\n","            \n","            # index the df with the tweet id\n","            scores_df.index.drop_duplicates(keep='first')\n","            scores = scores_df.T.to_dict('list')\n","\n","            h_list = self.arange_case(scores,cases)\n","            n = len(h_list)\n","\n","            old_cols = scores_df.copy().columns.tolist()\n","            cols = ['hashtag','target','is-target','is-infector','is-informer'] + old_cols\n","\n","            hdf = pd.DataFrame(h_list)\n","\n","            hdf.insert(0,column = 'hashtag', value= [hashtag]*n )\n","\n","            hdf.columns = cols\n","\n","            if i== 0:\n","                df  = hdf.copy()\n","            else:\n","                df = pd.concat( [df,hdf], ignore_index=True, axis = 0)\n","                print(f'added {hashtag} to database\\n')\n","\n","\n","            hdf.to_csv(f'multisource_analysis/data/{hashtag}_scored_tweets.csv')\n","\n","        # df.to_csv('multisource_analysis/multisource_database.csv')\n","\n","\n","        return df\n","\n","a = Analyzer()\n","\n","df = a.create_ms()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0f0Dd997frMF","outputId":"4a07f219-638e-463f-83f4-8f013ecb5e59"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>hashtag</th>\n","      <th>target</th>\n","      <th>is-target</th>\n","      <th>is-infector</th>\n","      <th>is-informer</th>\n","      <th>text</th>\n","      <th>user_id</th>\n","      <th>polarity</th>\n","      <th>subjectivity</th>\n","      <th>text_len</th>\n","      <th>...</th>\n","      <th>surprise</th>\n","      <th>disgust</th>\n","      <th>hateful</th>\n","      <th>targeted</th>\n","      <th>aggressive</th>\n","      <th>grammar-sentence-score</th>\n","      <th>gender</th>\n","      <th>num_male</th>\n","      <th>num_female</th>\n","      <th>fear</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>avengers</td>\n","      <td>1555684437077430272</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>RT @Ickypoo82: \"Accept defeat!\"\\n\\n@PlayAvenge...</td>\n","      <td>1241298027731542019</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>97</td>\n","      <td>...</td>\n","      <td>0.003388</td>\n","      <td>0.003023</td>\n","      <td>0.005790</td>\n","      <td>0.003569</td>\n","      <td>0.003724</td>\n","      <td>NaN</td>\n","      <td>1.0</td>\n","      <td>100.0</td>\n","      <td>0.0</td>\n","      <td>0.001475</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>avengers</td>\n","      <td>1555684437077430272</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>\"Accept defeat!\"\\n\\n@PlayAvengers #PS5Share #M...</td>\n","      <td>1397755845446877184</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>172</td>\n","      <td>...</td>\n","      <td>0.003122</td>\n","      <td>0.003760</td>\n","      <td>0.006412</td>\n","      <td>0.003332</td>\n","      <td>0.003664</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>100.0</td>\n","      <td>0.0</td>\n","      <td>0.001484</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>avengers</td>\n","      <td>1555684437077430272</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>Yes, I would call that a party foul!\\nGame: @P...</td>\n","      <td>1253824887166296065</td>\n","      <td>-0.4</td>\n","      <td>0.4</td>\n","      <td>159</td>\n","      <td>...</td>\n","      <td>0.003085</td>\n","      <td>0.002510</td>\n","      <td>0.009454</td>\n","      <td>0.002653</td>\n","      <td>0.003465</td>\n","      <td>NaN</td>\n","      <td>1.0</td>\n","      <td>97.0</td>\n","      <td>3.0</td>\n","      <td>0.001490</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>avengers</td>\n","      <td>1555684437077430272</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>RT @PlayAvengers616: “Shakespeare in The Park?...</td>\n","      <td>1526161221770174464</td>\n","      <td>0.2</td>\n","      <td>0.7</td>\n","      <td>91</td>\n","      <td>...</td>\n","      <td>0.003606</td>\n","      <td>0.002124</td>\n","      <td>0.008906</td>\n","      <td>0.004560</td>\n","      <td>0.002507</td>\n","      <td>NaN</td>\n","      <td>1.0</td>\n","      <td>89.0</td>\n","      <td>11.0</td>\n","      <td>0.001406</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>avengers</td>\n","      <td>1555684437077430272</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>RT @swingwithspidey: Miles Morales 🕸\\n.\\n🎮: Sp...</td>\n","      <td>1419712116425101318</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>86</td>\n","      <td>...</td>\n","      <td>0.002849</td>\n","      <td>0.002674</td>\n","      <td>0.005349</td>\n","      <td>0.004129</td>\n","      <td>0.003674</td>\n","      <td>NaN</td>\n","      <td>1.0</td>\n","      <td>99.0</td>\n","      <td>1.0</td>\n","      <td>0.001372</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 70 columns</p>\n","</div>"],"text/plain":["    hashtag               target  is-target  is-infector  is-informer  \\\n","0  avengers  1555684437077430272          1            0            0   \n","1  avengers  1555684437077430272          0            1            0   \n","2  avengers  1555684437077430272          0            0            1   \n","3  avengers  1555684437077430272          0            0            1   \n","4  avengers  1555684437077430272          0            0            1   \n","\n","                                                text              user_id  \\\n","0  RT @Ickypoo82: \"Accept defeat!\"\\n\\n@PlayAvenge...  1241298027731542019   \n","1  \"Accept defeat!\"\\n\\n@PlayAvengers #PS5Share #M...  1397755845446877184   \n","2  Yes, I would call that a party foul!\\nGame: @P...  1253824887166296065   \n","3  RT @PlayAvengers616: “Shakespeare in The Park?...  1526161221770174464   \n","4  RT @swingwithspidey: Miles Morales 🕸\\n.\\n🎮: Sp...  1419712116425101318   \n","\n","   polarity  subjectivity  text_len  ...  surprise   disgust   hateful  \\\n","0       0.0           0.0        97  ...  0.003388  0.003023  0.005790   \n","1       0.0           0.0       172  ...  0.003122  0.003760  0.006412   \n","2      -0.4           0.4       159  ...  0.003085  0.002510  0.009454   \n","3       0.2           0.7        91  ...  0.003606  0.002124  0.008906   \n","4       0.0           0.0        86  ...  0.002849  0.002674  0.005349   \n","\n","   targeted  aggressive  grammar-sentence-score  gender  num_male  num_female  \\\n","0  0.003569    0.003724                     NaN     1.0     100.0         0.0   \n","1  0.003332    0.003664                     0.0     1.0     100.0         0.0   \n","2  0.002653    0.003465                     NaN     1.0      97.0         3.0   \n","3  0.004560    0.002507                     NaN     1.0      89.0        11.0   \n","4  0.004129    0.003674                     NaN     1.0      99.0         1.0   \n","\n","       fear  \n","0  0.001475  \n","1  0.001484  \n","2  0.001490  \n","3  0.001406  \n","4  0.001372  \n","\n","[5 rows x 70 columns]"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["df.head(5)"]},{"cell_type":"markdown","metadata":{"id":"4Z8oCv8UfrMG"},"source":["# USER SCORES"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":345},"executionInfo":{"elapsed":6139,"status":"error","timestamp":1663721167288,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"},"user_tz":-60},"id":"wxqlHMv_frMH","outputId":"8e395498-0804-492f-8dfe-1e64efdbc04b"},"outputs":[],"source":["class Analyzer(object):\n","    def __init__(self):\n","\n","        self.hashtags = ['avengers','gaza', 'blm','brexit','climatechange','covid','gaza','loveisland','monkeypox','nhs','olivianewtonjohn','supercup','UkraineWar']\n","        # self.hashtags = ['blm']\n","\n","    @staticmethod\n","    def load_ms_cases(hashtag):\n","        path = f'tweets{os.path.sep}{hashtag}{os.path.sep}{hashtag}_ms_cases.json'\n","        with open(path) as jf:\n","            data = json.load(jf)\n","        return data\n","\n","\n","    def load_scores_df(self,hashtag):\n","        save_path_all = f'tweets/{hashtag}/{hashtag}_TWEETS_scores.csv'\n","        df =  pd.read_csv(save_path_all)\n","        df.tweet_id.drop_duplicates(keep='first',inplace=True)\n","        df.set_index('tweet_id', inplace = True)\n","\n","\n","        save_path = f'tweets/{hashtag}/{hashtag}_infector_scores.csv'\n","        i_df =  pd.read_csv(save_path)\n","        i_df.tweet_id.drop_duplicates(keep='first',inplace=True)\n","        i_df.set_index('tweet_id', inplace = True)\n","\n","\n","        save_gender = f'tweets/{hashtag}/{hashtag}_gender_scores.csv'\n","        g_df =  pd.read_csv(save_gender)\n","        g_df.tweet_id.drop_duplicates(keep='first',inplace=True)\n","        g_df.drop(['text'], axis=1)\n","        g_df.set_index('tweet_id', inplace = True)\n","\n","        fear_path = f'tweets/{hashtag}/{hashtag}_fear_scores.csv'\n","        fear_df =  pd.read_csv(fear_path)\n","        fear_df.tweet_id.drop_duplicates(keep='first',inplace=True)\n","        fear_df.drop(['text','user_id'], axis=1,inplace=True)\n","        fear_df.set_index('tweet_id', inplace = True)\n","\n","        save_path = f'tweets/{hashtag}/{hashtag}_USER_scores.csv'\n","        u_df =  pd.read_csv(save_path)\n","        u_df.tweet_id.drop_duplicates(keep='first',inplace=True)\n","        u_df.set_index('tweet_id', inplace = True)\n","\n","        p_path = f'tweets/{hashtag}/{hashtag}_USER_POLITE_scores.csv'\n","        p_df =  pd.read_csv(p_path)\n","        p_df.tweet_id.drop_duplicates(keep='first',inplace=True)\n","        p_df.set_index('tweet_id', inplace = True)     \n","\n","        r_path = f'tweets/{hashtag}/{hashtag}_USER_READ_scores.csv'\n","        r_df =  pd.read_csv(r_path)\n","        r_df.tweet_id.drop_duplicates(keep='first',inplace=True)\n","        r_df.set_index('tweet_id', inplace = True)        \n","\n","        sub_all =pd.concat( [df,i_df],axis=0 ) # adding the infector scores to the database. so new entries\n","\n","        all = pd.concat( [sub_all,g_df, fear_df, u_df, p_df, r_df], axis = 1) # adding the gender scores to all types\n","        \n","\n","        d = [ 'Unnamed: 0','clean_text','punct','tokenized','nonstop','stemmed','topic_tokens','cardiff_tokens','hate_output','emo_output','grammartext']\n","\n","        \n","        all.drop(d,inplace=True,axis=1)\n","\n","        all = all.loc[:,~all.columns.duplicated()].copy()\n","\n","        user_all = all[~all['user_ARI_mean'].isnull() ]\n","\n","        print('loaded the scores')\n","        \n","        return user_all\n","\n","\n","    def get_the_tweets(self,hashtag):\n","        database = self.load_ms_cases(hashtag)\n","        cases = {}\n","        for key,value in database.items():\n","\n","            inf = value['infector-info']\n","            k = list(inf.keys())[0]\n","            infector = inf[k]\n","\n","\n","            informers = [ int(inf['id']) for inf in value['informers-data']]\n","\n","            dum =  {   int(key): { 'target':int(key) , 'infector': int(infector['id']), 'informers':informers } } # get the case breakdown of target infector informer\n","            cases.update( dum  )\n","            \n","        print('loaded the MS cases')\n","        return cases \n","                   \n","\n","    @staticmethod\n","    def arange_case(scores,cases):\n","        all = []\n","        target_count = 0 \n","        infector_count = 0 \n","        informer_count = 0\n","\n","        count=0\n","\n","        for key,value in cases.items():\n","            # each row is organised as follows\n","            # target id.    is target,    is infector, is informer, score\n","\n","            target = key\n","            infector = value['infector']\n","            all_informers = [informer for informer in value['informers'] ]\n","\n","            if target in scores:\n","                target_count += 1\n","\n","            if infector in scores:\n","                infector_count += 1\n","\n","            for informer in all_informers:\n","                if informer  in scores:\n","                    informer_count += 1\n","                \n","\n","            # if (all(target, and infector in scores) and (any(informers in scores)) :\n","            if ( target in scores ) and ( infector in scores) and any( informer in scores.keys() for informer in all_informers ):\n","                count+=1\n","                \n","                all.append( [ key, 1, 0 ,0] + scores[key]  )                 # target row\n","\n","                all.append( [ key, 0, 1 ,0] + scores[infector]  )         # infector row\n","\n","                informers = [informer for informer in all_informers if informer in scores]\n","\n","                for informer in informers:\n","                    all.append( [ key, 0, 0, 1] +  scores[informer]  )\n","\n","        print(f'Out of {len(cases)} MS cases \\nWe found. We got the scores for {count} of them.')\n","        print(f'target count: {target_count}')\n","        print(f'infector count: {infector_count}')\n","        print(f'informer count: {informer_count}')\n","\n","        return all   \n","\n","\n","\n","    def create_ms(self):\n","\n","        for i, hashtag in enumerate(self.hashtags):\n","\n","            #load scores\n","            scores_df = self.load_scores_df(hashtag) # these are indexed by integer number 1,2,3,4. Tweet ids here are integers.\n","            cases = self.get_the_tweets(hashtag) # indexed by tweet ids. INTEGER\n","            \n","            # index the df with the tweet id\n","            scores_df.index.drop_duplicates(keep='first')\n","            scores = scores_df.T.to_dict('list')\n","\n","            h_list = self.arange_case(scores,cases)\n","            n = len(h_list)\n","\n","            old_cols = scores_df.copy().columns.tolist()\n","            cols = ['hashtag','target','is-target','is-infector','is-informer'] + old_cols\n","\n","            hdf = pd.DataFrame(h_list)\n","\n","            hdf.insert(0,column = 'hashtag', value= [hashtag]*n )\n","\n","            hdf.columns = cols\n","\n","            if i== 0:\n","                df  = hdf.copy()\n","            else:\n","                df = pd.concat( [df,hdf], ignore_index=True, axis = 0)\n","                print(f'added {hashtag} to database\\n')\n","\n","\n","            hdf.to_csv(f'multisource_analysis/user_ft_data/{hashtag}_scored_tweets2.csv')\n","\n","        # df.to_csv('multisource_analysis/multisource_database.csv')\n","\n","\n","        return df\n","\n","a = Analyzer()\n","\n","df = a.create_ms()"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["loaded the scores\n","loaded the MS cases\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\berke\\AppData\\Local\\Temp\\ipykernel_29076\\3049170631.py:143: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n","  scores = scores_df.T.to_dict('list')\n"]},{"name":"stdout","output_type":"stream","text":["Out of 1362 MS cases \n","We found. We got the scores for 1345 of them.\n","target count: 1351\n","infector count: 1356\n","informer count: 7955\n","loaded the scores\n","loaded the MS cases\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\berke\\AppData\\Local\\Temp\\ipykernel_29076\\3049170631.py:143: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n","  scores = scores_df.T.to_dict('list')\n"]},{"name":"stdout","output_type":"stream","text":["Out of 1555 MS cases \n","We found. We got the scores for 1447 of them.\n","target count: 1455\n","infector count: 1545\n","informer count: 9160\n","added gaza to database\n","\n","loaded the scores\n","loaded the MS cases\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\berke\\AppData\\Local\\Temp\\ipykernel_29076\\3049170631.py:143: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n","  scores = scores_df.T.to_dict('list')\n"]},{"name":"stdout","output_type":"stream","text":["Out of 3862 MS cases \n","We found. We got the scores for 2131 of them.\n","target count: 2205\n","infector count: 3525\n","informer count: 21761\n","added blm to database\n","\n","loaded the scores\n","loaded the MS cases\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\berke\\AppData\\Local\\Temp\\ipykernel_29076\\3049170631.py:143: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n","  scores = scores_df.T.to_dict('list')\n"]},{"name":"stdout","output_type":"stream","text":["Out of 3379 MS cases \n","We found. We got the scores for 1987 of them.\n","target count: 2062\n","infector count: 3051\n","informer count: 26065\n","added brexit to database\n","\n","loaded the scores\n","loaded the MS cases\n","Out of 1939 MS cases \n","We found. We got the scores for 1571 of them.\n","target count: 1585\n","infector count: 1853\n","informer count: 11618\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\berke\\AppData\\Local\\Temp\\ipykernel_29076\\3049170631.py:143: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n","  scores = scores_df.T.to_dict('list')\n"]},{"name":"stdout","output_type":"stream","text":["added climatechange to database\n","\n","loaded the scores\n","loaded the MS cases\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\berke\\AppData\\Local\\Temp\\ipykernel_29076\\3049170631.py:143: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n","  scores = scores_df.T.to_dict('list')\n"]},{"name":"stdout","output_type":"stream","text":["Out of 1831 MS cases \n","We found. We got the scores for 1564 of them.\n","target count: 1579\n","infector count: 1767\n","informer count: 10450\n","added covid to database\n","\n","loaded the scores\n","loaded the MS cases\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\berke\\AppData\\Local\\Temp\\ipykernel_29076\\3049170631.py:143: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n","  scores = scores_df.T.to_dict('list')\n"]},{"name":"stdout","output_type":"stream","text":["Out of 1555 MS cases \n","We found. We got the scores for 1447 of them.\n","target count: 1455\n","infector count: 1545\n","informer count: 9160\n","added gaza to database\n","\n","loaded the scores\n","loaded the MS cases\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\berke\\AppData\\Local\\Temp\\ipykernel_29076\\3049170631.py:143: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n","  scores = scores_df.T.to_dict('list')\n"]},{"name":"stdout","output_type":"stream","text":["Out of 1489 MS cases \n","We found. We got the scores for 1465 of them.\n","target count: 1467\n","infector count: 1487\n","informer count: 8580\n","added loveisland to database\n","\n","loaded the scores\n","loaded the MS cases\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\berke\\AppData\\Local\\Temp\\ipykernel_29076\\3049170631.py:143: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n","  scores = scores_df.T.to_dict('list')\n"]},{"name":"stdout","output_type":"stream","text":["Out of 4094 MS cases \n","We found. We got the scores for 1673 of them.\n","target count: 1715\n","infector count: 3227\n","informer count: 22078\n","added monkeypox to database\n","\n","loaded the scores\n","loaded the MS cases\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\berke\\AppData\\Local\\Temp\\ipykernel_29076\\3049170631.py:143: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n","  scores = scores_df.T.to_dict('list')\n"]},{"name":"stdout","output_type":"stream","text":["Out of 5577 MS cases \n","We found. We got the scores for 2329 of them.\n","target count: 2519\n","infector count: 4750\n","informer count: 34550\n","added nhs to database\n","\n","loaded the scores\n","loaded the MS cases\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\berke\\AppData\\Local\\Temp\\ipykernel_29076\\3049170631.py:143: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n","  scores = scores_df.T.to_dict('list')\n"]},{"name":"stdout","output_type":"stream","text":["Out of 1327 MS cases \n","We found. We got the scores for 1304 of them.\n","target count: 1316\n","infector count: 1315\n","informer count: 6888\n","added olivianewtonjohn to database\n","\n","loaded the scores\n","loaded the MS cases\n","Out of 3385 MS cases \n","We found. We got the scores for 1688 of them.\n","target count: 1714\n","infector count: 3106\n","informer count: 14204\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\berke\\AppData\\Local\\Temp\\ipykernel_29076\\3049170631.py:143: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n","  scores = scores_df.T.to_dict('list')\n"]},{"name":"stdout","output_type":"stream","text":["added supercup to database\n","\n","loaded the scores\n","loaded the MS cases\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\berke\\AppData\\Local\\Temp\\ipykernel_29076\\3049170631.py:143: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n","  scores = scores_df.T.to_dict('list')\n"]},{"name":"stdout","output_type":"stream","text":["Out of 2499 MS cases \n","We found. We got the scores for 1586 of them.\n","target count: 1610\n","infector count: 2365\n","informer count: 14290\n","added UkraineWar to database\n","\n"]}],"source":["class Analyzer(object):\n","    def __init__(self):\n","\n","        self.hashtags = ['avengers','gaza', 'blm','brexit','climatechange','covid','gaza','loveisland','monkeypox','nhs','olivianewtonjohn','supercup','UkraineWar']\n","        # self.hashtags = ['blm']\n","\n","    @staticmethod\n","    def load_ms_cases(hashtag):\n","        path = f'tweets{os.path.sep}{hashtag}{os.path.sep}{hashtag}_ms_cases.json'\n","        with open(path) as jf:\n","            data = json.load(jf)\n","        return data\n","\n","\n","    def load_scores_df(self,hashtag):\n","        save_path_all = f'tweets/{hashtag}/{hashtag}_TWEETS_scores.csv'\n","        df =  pd.read_csv(save_path_all)\n","        df.tweet_id.drop_duplicates(keep='first',inplace=True)\n","        df.set_index('tweet_id', inplace = True)\n","\n","\n","        save_path = f'tweets/{hashtag}/{hashtag}_infector_scores.csv'\n","        i_df =  pd.read_csv(save_path)\n","        i_df.tweet_id.drop_duplicates(keep='first',inplace=True)\n","        i_df.set_index('tweet_id', inplace = True)\n","\n","\n","        save_gender = f'tweets/{hashtag}/{hashtag}_gender_scores.csv'\n","        g_df =  pd.read_csv(save_gender)\n","        g_df.tweet_id.drop_duplicates(keep='first',inplace=True)\n","        g_df.drop(['text'], axis=1)\n","        g_df.set_index('tweet_id', inplace = True)\n","\n","        fear_path = f'tweets/{hashtag}/{hashtag}_fear_scores.csv'\n","        fear_df =  pd.read_csv(fear_path)\n","        fear_df.tweet_id.drop_duplicates(keep='first',inplace=True)\n","        fear_df.drop(['text','user_id'], axis=1,inplace=True)\n","        fear_df.set_index('tweet_id', inplace = True)\n","\n","        save_path = f'tweets/{hashtag}/{hashtag}_USER_scores_10_feeds.csv'\n","        u_df =  pd.read_csv(save_path)\n","        u_df.tweet_id.drop_duplicates(keep='first',inplace=True)\n","        u_df.set_index('tweet_id', inplace = True)\n","\n","        sub_all =pd.concat( [df,i_df],axis=0 ) # adding the infector scores to the database. so new entries\n","\n","        all = pd.concat( [sub_all,g_df, fear_df, u_df], axis = 1) # adding the gender scores to all types\n","        \n","\n","        d = [ 'Unnamed: 0','clean_text','punct','tokenized','nonstop','stemmed','topic_tokens','cardiff_tokens','hate_output','emo_output','grammartext']\n","\n","        \n","        all.drop(d,inplace=True,axis=1)\n","\n","        all = all.loc[:,~all.columns.duplicated()].copy()\n","\n","        user_all = all[~all['user_ARI_mean'].isnull() ]\n","\n","        print('loaded the scores')\n","        \n","        return user_all\n","\n","\n","    def get_the_tweets(self,hashtag):\n","        database = self.load_ms_cases(hashtag)\n","        cases = {}\n","        for key,value in database.items():\n","\n","            inf = value['infector-info']\n","            k = list(inf.keys())[0]\n","            infector = inf[k]\n","\n","\n","            informers = [ int(inf['id']) for inf in value['informers-data']]\n","\n","            dum =  {   int(key): { 'target':int(key) , 'infector': int(infector['id']), 'informers':informers } } # get the case breakdown of target infector informer\n","            cases.update( dum  )\n","            \n","        print('loaded the MS cases')\n","        return cases \n","                   \n","\n","    @staticmethod\n","    def arange_case(scores,cases):\n","        all = []\n","        target_count = 0 \n","        infector_count = 0 \n","        informer_count = 0\n","\n","        count=0\n","\n","        for key,value in cases.items():\n","            # each row is organised as follows\n","            # target id.    is target,    is infector, is informer, score\n","\n","            target = key\n","            infector = value['infector']\n","            all_informers = [informer for informer in value['informers'] ]\n","\n","            if target in scores:\n","                target_count += 1\n","\n","            if infector in scores:\n","                infector_count += 1\n","\n","            for informer in all_informers:\n","                if informer  in scores:\n","                    informer_count += 1\n","                \n","\n","            # if (all(target, and infector in scores) and (any(informers in scores)) :\n","            if ( target in scores ) and ( infector in scores) and any( informer in scores.keys() for informer in all_informers ):\n","                count+=1\n","                \n","                all.append( [ key, 1, 0 ,0] + scores[key]  )                 # target row\n","\n","                all.append( [ key, 0, 1 ,0] + scores[infector]  )         # infector row\n","\n","                informers = [informer for informer in all_informers if informer in scores]\n","\n","                for informer in informers:\n","                    all.append( [ key, 0, 0, 1] +  scores[informer]  )\n","\n","        print(f'Out of {len(cases)} MS cases \\nWe found. We got the scores for {count} of them.')\n","        print(f'target count: {target_count}')\n","        print(f'infector count: {infector_count}')\n","        print(f'informer count: {informer_count}')\n","\n","        return all   \n","\n","\n","\n","    def create_ms(self):\n","\n","        for i, hashtag in enumerate(self.hashtags):\n","\n","            #load scores\n","            scores_df = self.load_scores_df(hashtag) # these are indexed by integer number 1,2,3,4. Tweet ids here are integers.\n","            cases = self.get_the_tweets(hashtag) # indexed by tweet ids. INTEGER\n","            \n","            # index the df with the tweet id\n","            scores_df.index.drop_duplicates(keep='first')\n","            scores = scores_df.T.to_dict('list')\n","\n","            h_list = self.arange_case(scores,cases)\n","            n = len(h_list)\n","\n","            old_cols = scores_df.copy().columns.tolist()\n","            cols = ['hashtag','target','is-target','is-infector','is-informer'] + old_cols\n","\n","            hdf = pd.DataFrame(h_list)\n","\n","            hdf.insert(0,column = 'hashtag', value= [hashtag]*n )\n","\n","            hdf.columns = cols\n","\n","            if i== 0:\n","                df  = hdf.copy()\n","            else:\n","                df = pd.concat( [df,hdf], ignore_index=True, axis = 0)\n","                print(f'added {hashtag} to database\\n')\n","\n","\n","            hdf.to_csv(f'multisource_analysis/user_ft_data/{hashtag}_scored_tweets_10_feeds.csv')\n","\n","        # df.to_csv('multisource_analysis/multisource_database.csv')\n","\n","\n","        return df\n","\n","a = Analyzer()\n","\n","df = a.create_ms()"]}],"metadata":{"colab":{"collapsed_sections":[],"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3.9.7 ('env-pytorch')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"59556fecfb9e1650310a175ca5cd58c0b942341d9e7f74cbfe1ccb958d377f3d"}}},"nbformat":4,"nbformat_minor":0}
