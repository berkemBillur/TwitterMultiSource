{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["FMiPphSbp48l","oQDW0t3op8cv","ock8vM_pqAgl","ta9e_cEJWu1t","oBw6cTkP9r0m","yYW8hTKH0ONs","rXGH7qfsas67","iT5au7CzH3I8","8GVleE3uOII6","x4A_wqMeOII8","fZIRZd2VOII_","1vrc0fAQOIJD"],"machine_shape":"hm","authorship_tag":"ABX9TyMoH9vpE4S+w/oTUWk/wDFJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"ea5850a405b547e5ac8101a5ffd0ffa5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_66c2509e6f25455b95410c495ce58beb","IPY_MODEL_ca64fd38c9c046c4a41a6918e88ae81f","IPY_MODEL_f06635248a624531b87d37b3dd2af7c9"],"layout":"IPY_MODEL_8658488916d5418288155a1c040995c9"}},"66c2509e6f25455b95410c495ce58beb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b28954cbc3cf4a3882cd0263579b024a","placeholder":"​","style":"IPY_MODEL_ef94255e948742afaf4b9c5269d71269","value":"Downloading vocab.txt: 100%"}},"ca64fd38c9c046c4a41a6918e88ae81f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_915c80902a224f0a9e2500bc01c3d1ea","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fb9350f2664d4fdfac112aee0d310351","value":231508}},"f06635248a624531b87d37b3dd2af7c9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_151e512cef07489fbaddffec4f6404a2","placeholder":"​","style":"IPY_MODEL_2c2ff89b4bb340c7a7f8d2acedc4a69a","value":" 226k/226k [00:00&lt;00:00, 1.44MB/s]"}},"8658488916d5418288155a1c040995c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b28954cbc3cf4a3882cd0263579b024a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef94255e948742afaf4b9c5269d71269":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"915c80902a224f0a9e2500bc01c3d1ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb9350f2664d4fdfac112aee0d310351":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"151e512cef07489fbaddffec4f6404a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c2ff89b4bb340c7a7f8d2acedc4a69a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b8d3c5bb29a14e139a7d639394b451dc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b75095c6f4e441b29a4cf3cca0cb5a67","IPY_MODEL_b834f938383544e88e2f5457a5dc8562","IPY_MODEL_782b1e9126094ea5bfa65f9f833eb2bb"],"layout":"IPY_MODEL_8100d02f44a340f1aa4fda90db5a3eff"}},"b75095c6f4e441b29a4cf3cca0cb5a67":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_553fc15d1d11494cb5db4a5fe28491a7","placeholder":"​","style":"IPY_MODEL_faedbb7dbf25420ab8d65b8de2d38c75","value":"Downloading tokenizer_config.json: 100%"}},"b834f938383544e88e2f5457a5dc8562":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_825f31c95f30462ca5f7af7876b8850d","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_193469e8180b48f58939946d4acc4639","value":28}},"782b1e9126094ea5bfa65f9f833eb2bb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_12c3bd0491f544378ef62b4f45b35918","placeholder":"​","style":"IPY_MODEL_94acbeca43794a3eb5483cc242bf71cb","value":" 28.0/28.0 [00:00&lt;00:00, 892B/s]"}},"8100d02f44a340f1aa4fda90db5a3eff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"553fc15d1d11494cb5db4a5fe28491a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"faedbb7dbf25420ab8d65b8de2d38c75":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"825f31c95f30462ca5f7af7876b8850d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"193469e8180b48f58939946d4acc4639":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"12c3bd0491f544378ef62b4f45b35918":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"94acbeca43794a3eb5483cc242bf71cb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"90544ad565ba492ead6e7d7bd36a8784":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_998454837a6b405d84d278ba39abcc96","IPY_MODEL_5832350f663c4cbdabd46e96afd5f270","IPY_MODEL_62db7eba427846f5922a099597a00754"],"layout":"IPY_MODEL_cb637623f9c448089910a036a7f41e78"}},"998454837a6b405d84d278ba39abcc96":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7ed0647e7a1e46658ed589d91e364c11","placeholder":"​","style":"IPY_MODEL_766ead1e5f0e4bb9b733b4ff8aaed71e","value":"Downloading config.json: 100%"}},"5832350f663c4cbdabd46e96afd5f270":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_475857f8eb194db991024545a2673564","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a9bbc1c1478945c4af229f22e1c40f88","value":570}},"62db7eba427846f5922a099597a00754":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6b7e752aa274ba3a217f2fad88ad63d","placeholder":"​","style":"IPY_MODEL_5b1cba2271ad4ea2a8512b3e285cdd90","value":" 570/570 [00:00&lt;00:00, 13.9kB/s]"}},"cb637623f9c448089910a036a7f41e78":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ed0647e7a1e46658ed589d91e364c11":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"766ead1e5f0e4bb9b733b4ff8aaed71e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"475857f8eb194db991024545a2673564":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9bbc1c1478945c4af229f22e1c40f88":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c6b7e752aa274ba3a217f2fad88ad63d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b1cba2271ad4ea2a8512b3e285cdd90":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"61bd743386614b37a24b8422a10e2b55":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_12f4138401144204afcec2b5fb2385fc","IPY_MODEL_a6b16dabaf04469cbd66737cb8c023a4","IPY_MODEL_949104470407496a98bfe853644e62cf"],"layout":"IPY_MODEL_8998316fe30f488d945bceeb1f7e3c7b"}},"12f4138401144204afcec2b5fb2385fc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_48df61292d1e4196b04faaaeb9ba64a6","placeholder":"​","style":"IPY_MODEL_4fa9f8a23c0149a0be6c071dd1a62cb7","value":"Downloading pytorch_model.bin: 100%"}},"a6b16dabaf04469cbd66737cb8c023a4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8a943ba1764e4d64acf55a562f47fd36","max":440473133,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0e633e199f6f454a922f458b3080f889","value":440473133}},"949104470407496a98bfe853644e62cf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_540982e4c8894f7586f5ebd2e31827cc","placeholder":"​","style":"IPY_MODEL_8c077ff1396140e68065369e4ff5c46f","value":" 420M/420M [00:09&lt;00:00, 46.0MB/s]"}},"8998316fe30f488d945bceeb1f7e3c7b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48df61292d1e4196b04faaaeb9ba64a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4fa9f8a23c0149a0be6c071dd1a62cb7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8a943ba1764e4d64acf55a562f47fd36":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e633e199f6f454a922f458b3080f889":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"540982e4c8894f7586f5ebd2e31827cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c077ff1396140e68065369e4ff5c46f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ec24826b21b64ce1a6cadfde92a328ac":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a41670c7dd27427097c4c3d8b373bb9c","IPY_MODEL_cfe131061d634fba9c93788bf9aaca4b","IPY_MODEL_2b90d7f617894a3f9b5e26027a919b81"],"layout":"IPY_MODEL_86a802086f2d4cba901ed85108edd044"}},"a41670c7dd27427097c4c3d8b373bb9c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_51b3591ce9c4459eb3e243beb5741bba","placeholder":"​","style":"IPY_MODEL_c53c0c9010d840a2a0ffebb383f6f01c","value":"Downloading vocab.txt: 100%"}},"cfe131061d634fba9c93788bf9aaca4b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b2183ebdd2414a2e8412f9ac21e2d78f","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7404dca381d04b3f820b60ca517c94bf","value":231508}},"2b90d7f617894a3f9b5e26027a919b81":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_db836544ff1b4656a9db21c0dba820a0","placeholder":"​","style":"IPY_MODEL_385eef8684cc43f88df0743ce7449a3e","value":" 226k/226k [00:00&lt;00:00, 2.78MB/s]"}},"86a802086f2d4cba901ed85108edd044":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51b3591ce9c4459eb3e243beb5741bba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c53c0c9010d840a2a0ffebb383f6f01c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b2183ebdd2414a2e8412f9ac21e2d78f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7404dca381d04b3f820b60ca517c94bf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"db836544ff1b4656a9db21c0dba820a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"385eef8684cc43f88df0743ce7449a3e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8cf2e856c9e24aa195a57539d5563b65":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c76589d4995a482cab0239df2dc45558","IPY_MODEL_d889c9c64b8943b9818af59ffdae9177","IPY_MODEL_a53c83494c37412ca7cf1765553abafd"],"layout":"IPY_MODEL_8ae41388ff794630afa7a7c9c23c37d6"}},"c76589d4995a482cab0239df2dc45558":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f48cf4737c094d628cd3f14c56926c61","placeholder":"​","style":"IPY_MODEL_90e6cd7176f7478c82816e19dc51691d","value":"Downloading tokenizer_config.json: 100%"}},"d889c9c64b8943b9818af59ffdae9177":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_de12f90d3b984df08e6427d6304e7d16","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7f6d4d2e55ca4023993ed9a3b983c41f","value":28}},"a53c83494c37412ca7cf1765553abafd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_52e22800efdb47b5a645a96ba7b56cbb","placeholder":"​","style":"IPY_MODEL_b5d3745315f44a57af96b13891529f98","value":" 28.0/28.0 [00:00&lt;00:00, 850B/s]"}},"8ae41388ff794630afa7a7c9c23c37d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f48cf4737c094d628cd3f14c56926c61":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90e6cd7176f7478c82816e19dc51691d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"de12f90d3b984df08e6427d6304e7d16":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f6d4d2e55ca4023993ed9a3b983c41f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"52e22800efdb47b5a645a96ba7b56cbb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b5d3745315f44a57af96b13891529f98":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ce81e398b63a4cc3b9805d56c4965465":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4ac324cb07934f2e98a118ad72d9d906","IPY_MODEL_350c598bca7e4fa59694efe3a8d711b1","IPY_MODEL_85bb3b3087e24b129f3355f6e6f67951"],"layout":"IPY_MODEL_544db15263a8418c96cb52fbb8a5318f"}},"4ac324cb07934f2e98a118ad72d9d906":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd0fdfceaed84c12b686bd32be2c5631","placeholder":"​","style":"IPY_MODEL_391a898d1f9549e9b8d8cf8accf79c97","value":"Downloading config.json: 100%"}},"350c598bca7e4fa59694efe3a8d711b1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dce2ceab291a4297930e8a55f5e6b944","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_989c6725846b46638d58ca9d99aa353e","value":570}},"85bb3b3087e24b129f3355f6e6f67951":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ee80c8bd9c44425b8fb4af70ec15aa36","placeholder":"​","style":"IPY_MODEL_70e3e6e784bd42c1a7dec381f2fae2d6","value":" 570/570 [00:00&lt;00:00, 16.7kB/s]"}},"544db15263a8418c96cb52fbb8a5318f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd0fdfceaed84c12b686bd32be2c5631":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"391a898d1f9549e9b8d8cf8accf79c97":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dce2ceab291a4297930e8a55f5e6b944":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"989c6725846b46638d58ca9d99aa353e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ee80c8bd9c44425b8fb4af70ec15aa36":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70e3e6e784bd42c1a7dec381f2fae2d6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8698e4da3bed48718e9272154941b453":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c1c6e7d876624d379bb32f2bb902f90b","IPY_MODEL_ae4e87a2c950493cac93816aad91e89b","IPY_MODEL_bf6dec09851f4b31921d43da1db6b6b7"],"layout":"IPY_MODEL_1bcb8642ca6849dd800d2d7bbed16b8e"}},"c1c6e7d876624d379bb32f2bb902f90b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_895150e0df1a45b38e1222d33d1ef6c4","placeholder":"​","style":"IPY_MODEL_b3eebf897a8e45f6a1affbb555bf6c7e","value":"Downloading pytorch_model.bin: 100%"}},"ae4e87a2c950493cac93816aad91e89b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b0a604d06fa14619b730209468ca13ef","max":440473133,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3b94642f23fe41d991fb210e4917267d","value":440473133}},"bf6dec09851f4b31921d43da1db6b6b7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_64e3334b4fc04a4e9b26deffbe7e26b9","placeholder":"​","style":"IPY_MODEL_ce3a256bbd014c429152bb620582c8d3","value":" 420M/420M [00:10&lt;00:00, 44.1MB/s]"}},"1bcb8642ca6849dd800d2d7bbed16b8e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"895150e0df1a45b38e1222d33d1ef6c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b3eebf897a8e45f6a1affbb555bf6c7e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b0a604d06fa14619b730209468ca13ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b94642f23fe41d991fb210e4917267d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"64e3334b4fc04a4e9b26deffbe7e26b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce3a256bbd014c429152bb620582c8d3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["## MOUNT"],"metadata":{"id":"FMiPphSbp48l"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd /content/drive/My Drive/MSc_project/.MAIN"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wjB04glDz9GM","executionInfo":{"status":"ok","timestamp":1662458297884,"user_tz":-60,"elapsed":17177,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"}},"outputId":"2572af85-521f-446f-cb92-3afa0b05582e"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/My Drive/MSc_project/.MAIN\n"]}]},{"cell_type":"markdown","source":["## PIPS"],"metadata":{"id":"oQDW0t3op8cv"}},{"cell_type":"code","source":["!pip install pandas==1.2.4\n","!pip install xgboost\n","!pip install lightgbm\n","!pip install tweet-preprocessor\n","!pip install transformers"],"metadata":{"id":"nX70KVtb4G0i","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662458328603,"user_tz":-60,"elapsed":29336,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"}},"outputId":"a5b4475e-98f4-442d-bf77-25dcb4107c09"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pandas==1.2.4\n","  Downloading pandas-1.2.4-cp37-cp37m-manylinux1_x86_64.whl (9.9 MB)\n","\u001b[K     |████████████████████████████████| 9.9 MB 22.2 MB/s \n","\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.4) (2022.2.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.4) (2.8.2)\n","Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.4) (1.21.6)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas==1.2.4) (1.15.0)\n","Installing collected packages: pandas\n","  Attempting uninstall: pandas\n","    Found existing installation: pandas 1.3.5\n","    Uninstalling pandas-1.3.5:\n","      Successfully uninstalled pandas-1.3.5\n","Successfully installed pandas-1.2.4\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (0.90)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.7.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.21.6)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: lightgbm in /usr/local/lib/python3.7/dist-packages (2.2.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from lightgbm) (1.7.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from lightgbm) (1.21.6)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from lightgbm) (1.0.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->lightgbm) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->lightgbm) (1.1.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tweet-preprocessor\n","  Downloading tweet_preprocessor-0.6.0-py3-none-any.whl (27 kB)\n","Installing collected packages: tweet-preprocessor\n","Successfully installed tweet-preprocessor-0.6.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.21.3-py3-none-any.whl (4.7 MB)\n","\u001b[K     |████████████████████████████████| 4.7 MB 30.5 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 49.8 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n","\u001b[K     |████████████████████████████████| 120 kB 68.3 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.9.1 tokenizers-0.12.1 transformers-4.21.3\n"]}]},{"cell_type":"markdown","source":["## DATA LOAD AND CLEAN"],"metadata":{"id":"ock8vM_pqAgl"}},{"cell_type":"code","source":["import pandas as pd\n","import os.path\n","\n","import nltk\n","from nltk.stem import PorterStemmer # for stemming\n","from nltk.stem import WordNetLemmatizer # for lemmatization\n","from nltk.corpus import stopwords\n","import re\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","from xgboost import XGBClassifier, Booster\n","from lightgbm import LGBMClassifier\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import confusion_matrix\n","from sklearn.model_selection import GridSearchCV\n","\n","import numpy as np\n","from sklearn.model_selection import cross_validate\n","from sklearn.model_selection import train_test_split\n","\n","import preprocessor as p\n","import joblib"],"metadata":{"id":"V9Sz024N4MOp","executionInfo":{"status":"ok","timestamp":1662458406767,"user_tz":-60,"elapsed":1763,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":4,"metadata":{"id":"POjBgP6uLqHd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662458408127,"user_tz":-60,"elapsed":1362,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"}},"outputId":"396316fa-5669-4503-b6ac-6084f23123c6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["female    5725\n","male      5469\n","Name: gender, dtype: int64"]},"metadata":{},"execution_count":4}],"source":["df=pd.read_csv('user_gender_class/data/gender-classifier-DFE-791531.csv',encoding='latin1')\n","\n","df.drop(['_unit_id','_last_judgment_at','created','fav_number','profileimage','retweet_count','tweet_coord',\n","         '_trusted_judgments', 'tweet_count', 'tweet_created', 'tweet_id', 'tweet_location', 'user_timezone', \n","         '_golden','_unit_state', 'gender_gold', 'link_color', 'name', 'profile_yn_gold', 'sidebar_color', \n","         'profile_yn', 'profile_yn:confidence','gender:confidence'], axis=1, inplace=True)\n","\n","df.isna().sum()\n","df.dropna(axis=0,inplace=True)\n","df['gender'].value_counts()\n","df['gender'] = df[(df['gender'] == 'female') | (df['gender'] == 'male')]\n","df['gender'].value_counts()"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"IflS0bJhSbw1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662458411241,"user_tz":-60,"elapsed":3117,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"}},"outputId":"6187edb7-af68-44a1-ab8a-1c966962a06c"},"outputs":[{"output_type":"stream","name":"stdout","text":["0    5725\n","1    5469\n","Name: gender, dtype: int64\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"]},{"output_type":"stream","name":"stdout","text":["now cleaning\n"]}],"source":["for gen in df['gender']:\n","  #print(gen)\n","  if gen=='male':\n","    df['gender'].replace({'male':'1'},inplace=True)\n","  elif gen=='female':\n","    df['gender'].replace({'female':'0'},inplace=True)\n","\n","\n","\n","df = df[df['gender'].notna()]\n","print(df['gender'].value_counts())\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","nltk.download('omw-1.4')\n","remove_rt = lambda x: re.sub('RT @\\w+: ',\" \",x)\n","rt = lambda x: re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",x)\n","\n","lemma = WordNetLemmatizer()\n","stopword = nltk.corpus.stopwords.words('english')\n","\n","def remove_stopwords(text):\n","    text = [word for word in text if word not in stopword]\n","    return text\n","\n","def tokenization(text):\n","    text = re.split('\\W+', text)\n","    return text\n","\n","\n","print('now cleaning')\n","\n","p.set_options(p.OPT.URL, p.OPT.MENTION, p.OPT.NUMBER)\n","df['clean_description'] = df.description.map(remove_rt).map(rt).map(p.clean).str.lower()#.apply(lambda x: tokenization(x)).apply(lambda x: remove_stopwords(x))\n","df['clean_text'] = df.text.map(remove_rt).map(rt).map(p.clean).str.lower()#.apply(lambda x: tokenization(x)).apply(lambda x: remove_stopwords(x))\n","\n","# df['clean_description'] = df.clean_description.apply(lambda lst: [lemma.lemmatize(word) for word in lst]).str.join(\" \")\n","# df['clean_text'] = df.clean_text.apply(lambda lst: [lemma.lemmatize(word) for word in lst]).str.join(\" \")\n","\n","df['texty'] = df.text.map(remove_rt).map(rt).map(p.clean).str.lower()\n","df['desc'] = df.description.map(remove_rt).map(rt).map(p.clean).str.lower()\n","\n","n = len(df)\n","df['sep'] = ['. \\n ' for i in range(n)]\n","\n","df['txt'] = df['desc'] + df['sep'] + df['texty']\n","\n","df['gender'] = (df['gender']).astype(int)  "]},{"cell_type":"markdown","source":["# BERT for DESCRIPTION"],"metadata":{"id":"ta9e_cEJWu1t"}},{"cell_type":"markdown","source":["### IMPORTS"],"metadata":{"id":"oBw6cTkP9r0m"}},{"cell_type":"code","source":["import tensorflow as tf\n","import torch\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"metadata":{"id":"Rr4eOUsXYcFg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662413239227,"user_tz":-60,"elapsed":5386,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"}},"outputId":"864f64d1-b0ac-4713-d486-70124bc12729"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla P100-PCIE-16GB\n"]}]},{"cell_type":"code","source":["import nltk\n","from nltk.corpus import stopwords\n","from wordcloud import WordCloud\n","\n","import pandas as pd\n","import random, time\n","from babel.dates import format_date, format_datetime, format_time\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.pipeline import Pipeline\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.metrics import classification_report, accuracy_score\n","\n","\n","import torch\n","from torch import Tensor\n","from torch import nn, optim\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","import torch.nn.functional as F\n","\n","import transformers, os\n","from transformers import BertModel, AutoModel, AdamW, get_linear_schedule_with_warmup, BertTokenizer, BertForSequenceClassification\n","from transformers import RobertaModel, AutoModel, AdamW, get_linear_schedule_with_warmup, RobertaTokenizer, RobertaForSequenceClassification\n","from transformers import AutoTokenizer, RobertaConfig"],"metadata":{"id":"ZCs0qIAlY2H0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### BERT MODEL"],"metadata":{"id":"yYW8hTKH0ONs"}},{"cell_type":"code","source":["df['gender'] = (df['gender']).astype(int)\n","\n","\n","# Split test and train data using 25% of the dataset for validation purposes\n","x_train, x_test, y_train, y_test = train_test_split(df['clean_description'], \n","                                                      df['gender'], test_size=0.2, shuffle=True, random_state=42)\n","\n","# Obtain a 10% test set from train set\n","X_train_Transformer, X_val_Transformer, y_train_Transformer, y_val_Transformer = train_test_split(\n","                                                    x_train, y_train, test_size=0.20, random_state=42)\n","\n","model_name = 'bert-base-uncased'\n","SEQ_LEN = 128\n","batch_size = 16\n","epochs = 3\n","learning_rate = 1e-5 # Controls how large a step is taken when updating model weights during training.\n","steps_per_epoch = int(len(df)/16)\n","num_workers = 3\n","\n","def get_split(text1):\n","    '''Get split of the text with 200 char lenght'''\n","    l_total = []\n","    l_parcial = []\n","    if len(text1.split())//150 >0:\n","        n = len(text1.split())//150\n","    else: \n","        n = 1\n","    for w in range(n):\n","        if w == 0:\n","            l_parcial = text1.split()[:200]\n","            l_total.append(\" \".join(l_parcial))\n","        else:\n","            l_parcial = text1.split()[w*150:w*150 + 200]\n","            l_total.append(\" \".join(l_parcial))\n","    return str(l_total)\n","\n","# Splits train and validation sets to be feed to the transformer which only accepts 512 tokens maximum\n","split_train_text = [get_split(t) for t in X_train_Transformer]\n","split_valid_text = [get_split(t) for t in X_val_Transformer]\n","split_test_text = [get_split(t) for t in x_test]\n","\n","\n","# Load the RoBERTa tokenizer and tokenize the data\n","print('Loading BERT tokenizer...')\n","tokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case=True)\n","\n","trencoding = tokenizer.batch_encode_plus(\n","  list(split_train_text),\n","  max_length=SEQ_LEN,\n","  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n","  return_token_type_ids=True,\n","  truncation=True,\n","  padding='longest',\n","  return_attention_mask=True,\n",")\n","\n","valencoding = tokenizer.batch_encode_plus(\n","  list(split_valid_text),\n","  max_length=SEQ_LEN,\n","  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n","  return_token_type_ids=True,\n","  truncation=True,\n","  padding='longest',\n","  return_attention_mask=True,\n",")\n","\n","\n","testencoding = tokenizer.batch_encode_plus(\n","  list(split_test_text),\n","  max_length=SEQ_LEN,\n","  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n","  return_token_type_ids=True,\n","  truncation=True,\n","  padding='longest',\n","  return_attention_mask=True,\n",")\n","\n","#compute the class weights\n","class_wts = compute_class_weight(class_weight='balanced', classes = np.unique(df['gender'].values.tolist()), \n","                                 y = df['gender'])\n","\n","#print(class_wts)\n","\n","# convert class weights to tensor\n","weights= torch.tensor(class_wts,dtype=torch.float)\n","weights = weights.to(device)\n","\n","# loss function\n","cross_entropy  = nn.NLLLoss(weight=weights) \n","cross_entropy  = nn.CrossEntropyLoss(weight=weights)\n","\n","def loadData(prep_df, batch_size, num_workers, sampler):\n","    \n","    return  DataLoader(\n","            prep_df,\n","            batch_size=batch_size,\n","            num_workers=num_workers,\n","            sampler=sampler,\n","            pin_memory=True\n","        )\n","\n","## convert lists to tensors\n","train_seq = torch.tensor(trencoding['input_ids'])\n","train_mask = torch.tensor(trencoding['attention_mask'])\n","train_token_ids = torch.tensor(trencoding['token_type_ids'])\n","train_y = torch.tensor(y_train_Transformer.tolist())\n","\n","val_seq = torch.tensor(valencoding['input_ids'])\n","val_mask = torch.tensor(valencoding['attention_mask'])\n","val_token_ids = torch.tensor(valencoding['token_type_ids'])\n","val_y = torch.tensor(y_val_Transformer.tolist())\n","\n","test_seq = torch.tensor(testencoding['input_ids'])\n","test_mask = torch.tensor(testencoding['attention_mask'])\n","test_token_ids = torch.tensor(testencoding['token_type_ids'])\n","test_y = torch.tensor(y_test.tolist())\n","\n","# wrap tensors\n","train_data = TensorDataset(train_seq, train_mask, train_token_ids, train_y)\n","# sampler for sampling the data during training\n","train_sampler = RandomSampler(train_data)\n","# Train Data Loader\n","traindata = loadData(train_data, batch_size, num_workers, train_sampler)\n","\n","# wrap tensors\n","val_data = TensorDataset(val_seq, val_mask, val_token_ids, val_y)\n","# sampler for sampling the data during training\n","val_sampler = SequentialSampler(val_data)\n","# Val Data Loader\n","valdata = loadData(val_data, batch_size, num_workers, val_sampler)\n","\n","# wrap tensors\n","test_data = TensorDataset(test_seq, test_mask, test_token_ids, test_y)\n","# sampler for sampling the data during training\n","test_sampler = SequentialSampler(test_data)\n","# Val Data Loader\n","testdata = loadData(test_data, batch_size, num_workers, test_sampler)\n","\n","\n","print('Number of data in the train set', len(traindata))\n","print('Number of data in the validation set', len(valdata))\n","print('Number of data in the test set', len(testdata))\n","\n","class BERT_Arch(nn.Module):\n","    \n","    def __init__(self, n_classes, freeze_bert=False):\n","        \n","        super(BERT_Arch,self).__init__()\n","        # Instantiating BERT model object\n","        self.bert = BertModel.from_pretrained(model_name, return_dict=False)\n","        \n","        # Freeze bert layers\n","        if freeze_bert:\n","            for p in self.bert.parameters():\n","                p.requires_grad = False\n","                \n","        self.bert_drop_1 = nn.Dropout(0.3)\n","        self.fc = nn.Linear(self.bert.config.hidden_size, self.bert.config.hidden_size) # (768, 64)\n","        self.bn = nn.BatchNorm1d(768) # (768)\n","        self.bert_drop_2 = nn.Dropout(0.25)\n","        self.out = nn.Linear(self.bert.config.hidden_size, 2) # (768,2)\n","\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids):\n","        _, output = self.bert(\n","            input_ids = input_ids,\n","            attention_mask = attention_mask,\n","            token_type_ids = token_type_ids\n","        )\n","        output = self.bert_drop_1(output)\n","        output = self.fc(output)\n","        output = self.bn(output)\n","        output = self.bert_drop_2(output)\n","        output = self.out(output)        \n","        return output\n","\n","class_names = np.unique(df['gender'])\n","print('Downloading the BERT custom model...')\n","model = BERT_Arch(len(class_names))\n","model.to(device) # Model to GPU.\n","\n","#optimizer parameters\n","param_optimizer = list(model.named_parameters())\n","no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","optimizer_parameters = [{'params': [p for n, p in param_optimizer \n","                                    if not any(nd in n for nd in no_decay)],'weight_decay':0.001},\n","                        {'params': [p for n, p in param_optimizer \n","                                    if any(nd in n for nd in no_decay)],'weight_decay':0.0}]\n","\n","print('Preparing the optimizer...')\n","#optimizer \n","optimizer = AdamW(optimizer_parameters, lr=learning_rate)\n","steps = steps_per_epoch\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps = 0,\n","    num_training_steps = steps\n",")\n"],"metadata":{"id":"R2Ap835KYd2g","colab":{"base_uri":"https://localhost:8080/","height":366,"referenced_widgets":["ea5850a405b547e5ac8101a5ffd0ffa5","66c2509e6f25455b95410c495ce58beb","ca64fd38c9c046c4a41a6918e88ae81f","f06635248a624531b87d37b3dd2af7c9","8658488916d5418288155a1c040995c9","b28954cbc3cf4a3882cd0263579b024a","ef94255e948742afaf4b9c5269d71269","915c80902a224f0a9e2500bc01c3d1ea","fb9350f2664d4fdfac112aee0d310351","151e512cef07489fbaddffec4f6404a2","2c2ff89b4bb340c7a7f8d2acedc4a69a","b8d3c5bb29a14e139a7d639394b451dc","b75095c6f4e441b29a4cf3cca0cb5a67","b834f938383544e88e2f5457a5dc8562","782b1e9126094ea5bfa65f9f833eb2bb","8100d02f44a340f1aa4fda90db5a3eff","553fc15d1d11494cb5db4a5fe28491a7","faedbb7dbf25420ab8d65b8de2d38c75","825f31c95f30462ca5f7af7876b8850d","193469e8180b48f58939946d4acc4639","12c3bd0491f544378ef62b4f45b35918","94acbeca43794a3eb5483cc242bf71cb","90544ad565ba492ead6e7d7bd36a8784","998454837a6b405d84d278ba39abcc96","5832350f663c4cbdabd46e96afd5f270","62db7eba427846f5922a099597a00754","cb637623f9c448089910a036a7f41e78","7ed0647e7a1e46658ed589d91e364c11","766ead1e5f0e4bb9b733b4ff8aaed71e","475857f8eb194db991024545a2673564","a9bbc1c1478945c4af229f22e1c40f88","c6b7e752aa274ba3a217f2fad88ad63d","5b1cba2271ad4ea2a8512b3e285cdd90","61bd743386614b37a24b8422a10e2b55","12f4138401144204afcec2b5fb2385fc","a6b16dabaf04469cbd66737cb8c023a4","949104470407496a98bfe853644e62cf","8998316fe30f488d945bceeb1f7e3c7b","48df61292d1e4196b04faaaeb9ba64a6","4fa9f8a23c0149a0be6c071dd1a62cb7","8a943ba1764e4d64acf55a562f47fd36","0e633e199f6f454a922f458b3080f889","540982e4c8894f7586f5ebd2e31827cc","8c077ff1396140e68065369e4ff5c46f"]},"executionInfo":{"status":"ok","timestamp":1662413263248,"user_tz":-60,"elapsed":23213,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"}},"outputId":"d01efa38-0def-42d3-b96d-8f2039d3517f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading BERT tokenizer...\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea5850a405b547e5ac8101a5ffd0ffa5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8d3c5bb29a14e139a7d639394b451dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90544ad565ba492ead6e7d7bd36a8784"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Number of data in the train set 448\n","Number of data in the validation set 112\n","Number of data in the test set 140\n","Downloading the BERT custom model...\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/420M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61bd743386614b37a24b8422a10e2b55"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Preparing the optimizer...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n"]}]},{"cell_type":"markdown","source":["### TRAINING AND EVAL FUNCS"],"metadata":{"id":"rXGH7qfsas67"}},{"cell_type":"code","source":["# function to train the bert model\n","def trainMODEL():\n","  \n","    print('Training...')\n","    model.train()\n","    total_loss, total_accuracy = 0, 0\n","\n","    # empty list to save model predictions\n","    total_preds=[]\n","\n","    # iterate over batches\n","    for step, batch in enumerate(traindata):\n","    \n","        # progress update after every 50 batches.\n","        if step % 50 == 0 and not step == 0:\n","            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(traindata)))\n","\n","        if torch.cuda.is_available():\n","            # push the batch to gpu\n","            batch = [r.to(device) for r in batch]\n","\n","        sent_id, mask, token_type_ids, labels = batch\n","        # clear previously calculated gradients \n","        model.zero_grad()        \n","        # get model predictions for the current batch\n","        preds = model(sent_id, mask, token_type_ids)\n","        # compute the loss between actual and predicted values\n","        loss = cross_entropy(preds, labels)\n","        # add on to the total loss\n","        total_loss = total_loss + loss.item()\n","        # backward pass to calculate the gradients\n","        loss.backward()\n","        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        # update parameters\n","        optimizer.step()\n","        # model predictions are stored on GPU. So, push it to CPU\n","        preds=preds.detach().cpu().numpy()\n","        # append the model predictions\n","        total_preds.append(preds)\n","        \n","        torch.cuda.empty_cache()\n","\n","    # compute the training loss of the epoch\n","    avg_loss = total_loss / len(traindata)\n","\n","    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n","    # reshape the predictions in form of (number of samples, no. of classes)\n","    total_preds  = np.concatenate(total_preds, axis=0)\n","\n","    #returns the loss and predictions\n","    return avg_loss, total_preds\n","\n","# function for evaluating the model\n","def evaluate():\n","  \n","    print(\"\\nEvaluating...\")\n","    t0 = time.time()\n","    \n","    model.eval() # deactivate dropout layers\n","    total_loss, total_accuracy = 0, 0\n","    \n","    # empty list to save the model predictions\n","    total_preds = []\n","\n","    # iterate over batches\n","    for step, batch in enumerate(valdata):\n","        # Progress update every 50 batches.\n","        if step % 50 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(valdata)))\n","\n","        if torch.cuda.is_available():\n","            # push the batch to gpu\n","            batch = [t.to(device) for t in batch]\n","\n","        sent_id, mask, token_type_ids, labels = batch\n","\n","        # deactivate autograd\n","        with torch.no_grad(): # Dont store any previous computations, thus freeing GPU space\n","\n","            # model predictions\n","            preds = model(sent_id, mask, token_type_ids)\n","            # compute the validation loss between actual and predicted values\n","            loss = cross_entropy(preds, labels)\n","            total_loss = total_loss + loss.item()\n","            preds = preds.detach().cpu().numpy()\n","            total_preds.append(preds)\n","\n","        torch.cuda.empty_cache()\n","    # compute the validation loss of the epoch\n","    avg_loss = total_loss / len(valdata) \n","    # reshape the predictions in form of (number of samples, no. of classes)\n","    total_preds  = np.concatenate(total_preds, axis=0)\n","\n","    return avg_loss, total_preds"],"metadata":{"id":"yYVrmY1Caufd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## TRAINING"],"metadata":{"id":"iT5au7CzH3I8"}},{"cell_type":"code","source":["# set initial loss to infinite\n","best_valid_loss = float('inf')\n","\n","# Empty lists to store training and validation loss of each epoch\n","train_losses=[]\n","valid_losses=[]\n","\n","# for each epoch perform training and evaluation\n","for epoch in range(epochs):\n","     \n","    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n","    \n","    #train model\n","    train_loss, _ = trainMODEL()\n","    \n","    #evaluate model\n","    valid_loss, _ = evaluate()\n","    \n","    print('Evaluation done for epoch {}'.format(epoch + 1))\n","    #save the best model\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        print('Saving model...')\n","        torch.save(model.state_dict(), 'user_gender_class/desc_gender.pt') # Save model weight's (you can also save it in .bin format)\n","    \n","    # append training and validation loss\n","    train_losses.append(train_loss)\n","    valid_losses.append(valid_loss)\n","    \n","    print(f'\\nTraining Loss: {train_loss:.3f}')\n","    print(f'Validation Loss: {valid_loss:.3f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X-plzp1XHtSk","executionInfo":{"status":"ok","timestamp":1662413487392,"user_tz":-60,"elapsed":224156,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"}},"outputId":"8801774e-60d9-49c5-c7c2-e326bc76e7a3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," Epoch 1 / 3\n","Training...\n","  Batch    50  of    448.\n","  Batch   100  of    448.\n","  Batch   150  of    448.\n","  Batch   200  of    448.\n","  Batch   250  of    448.\n","  Batch   300  of    448.\n","  Batch   350  of    448.\n","  Batch   400  of    448.\n","\n","Evaluating...\n","  Batch    50  of    112.\n","  Batch   100  of    112.\n","Evaluation done for epoch 1\n","Saving model...\n","\n","Training Loss: 0.690\n","Validation Loss: 0.598\n","\n"," Epoch 2 / 3\n","Training...\n","  Batch    50  of    448.\n","  Batch   100  of    448.\n","  Batch   150  of    448.\n","  Batch   200  of    448.\n","  Batch   250  of    448.\n","  Batch   300  of    448.\n","  Batch   350  of    448.\n","  Batch   400  of    448.\n","\n","Evaluating...\n","  Batch    50  of    112.\n","  Batch   100  of    112.\n","Evaluation done for epoch 2\n","Saving model...\n","\n","Training Loss: 0.586\n","Validation Loss: 0.595\n","\n"," Epoch 3 / 3\n","Training...\n","  Batch    50  of    448.\n","  Batch   100  of    448.\n","  Batch   150  of    448.\n","  Batch   200  of    448.\n","  Batch   250  of    448.\n","  Batch   300  of    448.\n","  Batch   350  of    448.\n","  Batch   400  of    448.\n","\n","Evaluating...\n","  Batch    50  of    112.\n","  Batch   100  of    112.\n","Evaluation done for epoch 3\n","\n","Training Loss: 0.500\n","Validation Loss: 0.662\n"]}]},{"cell_type":"markdown","source":["## TEST set"],"metadata":{"id":"-Sze3YlJHwdR"}},{"cell_type":"code","source":["print('\\nTest Set...')\n","\n","test_preds = []\n","\n","print('Total batches:', len(testdata))\n","\n","for fold_index in range(0, 3):\n","    \n","    print('\\nFold Model', fold_index)\n","    \n","    # Load the fold model\n","    path_model = 'user_gender_class/desc_gender.pt'\n","    model.load_state_dict(torch.load(path_model))\n","\n","    # Send the model to the GPU\n","    model.to(device)\n","\n","    stacked_val_labels = []\n","    \n","    # Put the model in evaluation mode.\n","    model.eval()\n","\n","    # Turn off the gradient calculations.\n","    # This tells the model not to compute or store gradients.\n","    # This step saves memory and speeds up validation.\n","    torch.set_grad_enabled(False)\n","\n","\n","    # Reset the total loss for this epoch.\n","    total_val_loss = 0\n","\n","    for j, test_batch in enumerate(testdata):\n","\n","        inference_status = 'Batch ' + str(j + 1)\n","\n","        print(inference_status, end='\\r')\n","\n","        b_input_ids = test_batch[0].to(device)\n","        b_input_mask = test_batch[1].to(device)\n","        b_token_type_ids = test_batch[2].to(device)\n","        b_test_y = test_batch[3].to(device)\n","\n","\n","        outputs = model(b_input_ids, \n","                        attention_mask=b_input_mask,\n","                        token_type_ids=b_token_type_ids)\n","\n","        # Get the preds\n","        preds = outputs[0]\n","\n","        # Move preds to the CPU\n","        val_preds = preds.detach().cpu().numpy()\n","        \n","        #true_labels.append(b_test_y.to('cpu').numpy().flatten())\n","        \n","        # Stack the predictions.\n","        if j == 0:  # first batch\n","            stacked_val_preds = val_preds\n","            \n","        else:\n","            stacked_val_preds = np.vstack((stacked_val_preds, val_preds))\n","            \n","    test_preds.append(stacked_val_preds)\n","    \n","            \n","print('\\nPrediction complete.')"],"metadata":{"id":"pOGH5-EYa-8w","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662413504084,"user_tz":-60,"elapsed":16709,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"}},"outputId":"5e699460-9635-42b4-8283-a7310745e516"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Test Set...\n","Total batches: 140\n","\n","Fold Model 0\n","Batch 140\n","Fold Model 1\n","Batch 140\n","Fold Model 2\n","Batch 140\n","Prediction complete.\n"]}]},{"cell_type":"code","source":["# Sum the predictions of all fold models\n","for i, item in enumerate(test_preds):\n","    if i == 0:\n","        preds = item\n","    else:\n","        # Sum the matrices\n","        preds = item + preds\n","\n","# Average the predictions\n","avg_preds = preds/(len(test_preds))\n","\n","#print(preds)\n","#print()\n","#print(avg_preds)\n","\n","# Take the argmax. \n","# This returns the column index of the max value in each row.\n","test_predictions = np.argmax(avg_preds, axis=1)\n","\n","true_y = []\n","for j, test_batch in enumerate(testdata):\n","    true_y.append(int(test_batch[3][0].numpy().flatten()))\n","print(true_y)\n","\n","# Accuracy and classification report \n","target_names = ['true_y', 'predicted_y']\n","\n","data = {'true_y': true_y,\n","       'predicted_y': test_predictions}\n","\n","df_pred_BERT = pd.DataFrame(data, columns=['true_y','predicted_y'])\n","\n","confusion_matrix = pd.crosstab(df_pred_BERT['true_y'], df_pred_BERT['predicted_y'], rownames=['True'], colnames=['Predicted'])\n","\n","sns.heatmap(confusion_matrix, annot=True)\n","plt.show()\n","\n","print('----')\n","print('Accuracy of BERT model', accuracy_score(true_y, test_predictions))\n","print('----')\n","\n","print(classification_report(true_y, test_predictions, target_names=target_names))"],"metadata":{"id":"1Z7RCNb3bFcQ","colab":{"base_uri":"https://localhost:8080/","height":536},"executionInfo":{"status":"ok","timestamp":1662413504862,"user_tz":-60,"elapsed":791,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"}},"outputId":"26feab0c-d05e-47b0-fe47-f9f9369051fe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0]\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 2 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"stream","name":"stdout","text":["----\n","Accuracy of BERT model 0.6357142857142857\n","----\n","              precision    recall  f1-score   support\n","\n","      true_y       0.63      0.59      0.61        68\n"," predicted_y       0.64      0.68      0.66        72\n","\n","    accuracy                           0.64       140\n","   macro avg       0.64      0.63      0.63       140\n","weighted avg       0.64      0.64      0.63       140\n","\n"]}]},{"cell_type":"markdown","source":["# BERT for TWEET AND DESCRIPTION"],"metadata":{"id":"8GVleE3uOII6"}},{"cell_type":"markdown","source":["### IMPORTS"],"metadata":{"id":"x4A_wqMeOII8"}},{"cell_type":"code","source":["import tensorflow as tf\n","import torch\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662411884368,"user_tz":-60,"elapsed":5870,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"}},"outputId":"eca144b5-87a7-437f-a6ee-04dbbd6a6d4b","id":"OdgYPQ40OII8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla P100-PCIE-16GB\n"]}]},{"cell_type":"code","source":["import nltk\n","from nltk.corpus import stopwords\n","from wordcloud import WordCloud\n","\n","import pandas as pd\n","import random, time\n","from babel.dates import format_date, format_datetime, format_time\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.pipeline import Pipeline\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.metrics import classification_report, accuracy_score\n","\n","\n","import torch\n","from torch import Tensor\n","from torch import nn, optim\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","import torch.nn.functional as F\n","\n","import transformers, os\n","from transformers import BertModel, AutoModel, AdamW, get_linear_schedule_with_warmup, BertTokenizer, BertForSequenceClassification\n","from transformers import RobertaModel, AutoModel, AdamW, get_linear_schedule_with_warmup, RobertaTokenizer, RobertaForSequenceClassification\n","from transformers import AutoTokenizer, RobertaConfig"],"metadata":{"id":"M0H_NKX9OII-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### BERT MODEL"],"metadata":{"id":"fZIRZd2VOII_"}},{"cell_type":"code","source":["df['gender'] = (df['gender']).astype(int)\n","\n","\n","# Split test and train data using 25% of the dataset for validation purposes\n","x_train, x_test, y_train, y_test = train_test_split(df['txt'], \n","                                                      df['gender'], test_size=0.2, shuffle=True, random_state=42)\n","\n","# Obtain a 10% test set from train set\n","X_train_Transformer, X_val_Transformer, y_train_Transformer, y_val_Transformer = train_test_split(\n","                                                    x_train, y_train, test_size=0.20, random_state=42)\n","\n","model_name = 'bert-base-uncased'\n","SEQ_LEN = 256\n","batch_size = 16\n","epochs = 3\n","learning_rate = 1e-5 # Controls how large a step is taken when updating model weights during training.\n","steps_per_epoch = int(len(df)/16)\n","num_workers = 3\n","\n","def get_split(text1):\n","    '''Get split of the text with 200 char lenght'''\n","    l_total = []\n","    l_parcial = []\n","    if len(text1.split())//150 >0:\n","        n = len(text1.split())//150\n","    else: \n","        n = 1\n","    for w in range(n):\n","        if w == 0:\n","            l_parcial = text1.split()[:200]\n","            l_total.append(\" \".join(l_parcial))\n","        else:\n","            l_parcial = text1.split()[w*150:w*150 + 200]\n","            l_total.append(\" \".join(l_parcial))\n","    return str(l_total)\n","\n","# Splits train and validation sets to be feed to the transformer which only accepts 512 tokens maximum\n","split_train_text = [get_split(t) for t in X_train_Transformer]\n","split_valid_text = [get_split(t) for t in X_val_Transformer]\n","split_test_text = [get_split(t) for t in x_test]\n","\n","\n","# Load the RoBERTa tokenizer and tokenize the data\n","print('Loading BERT tokenizer...')\n","tokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case=True)\n","\n","trencoding = tokenizer.batch_encode_plus(\n","  list(split_train_text),\n","  max_length=SEQ_LEN,\n","  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n","  return_token_type_ids=True,\n","  truncation=True,\n","  padding='longest',\n","  return_attention_mask=True,\n",")\n","\n","valencoding = tokenizer.batch_encode_plus(\n","  list(split_valid_text),\n","  max_length=SEQ_LEN,\n","  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n","  return_token_type_ids=True,\n","  truncation=True,\n","  padding='longest',\n","  return_attention_mask=True,\n",")\n","\n","\n","testencoding = tokenizer.batch_encode_plus(\n","  list(split_test_text),\n","  max_length=SEQ_LEN,\n","  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n","  return_token_type_ids=True,\n","  truncation=True,\n","  padding='longest',\n","  return_attention_mask=True,\n",")\n","\n","#compute the class weights\n","class_wts = compute_class_weight(class_weight='balanced', classes = np.unique(df['gender'].values.tolist()), \n","                                 y = df['gender'])\n","\n","#print(class_wts)\n","\n","# convert class weights to tensor\n","weights= torch.tensor(class_wts,dtype=torch.float)\n","weights = weights.to(device)\n","\n","# loss function\n","cross_entropy  = nn.NLLLoss(weight=weights) \n","cross_entropy  = nn.CrossEntropyLoss(weight=weights)\n","\n","def loadData(prep_df, batch_size, num_workers, sampler):\n","    \n","    return  DataLoader(\n","            prep_df,\n","            batch_size=batch_size,\n","            num_workers=num_workers,\n","            sampler=sampler,\n","            pin_memory=True\n","        )\n","\n","## convert lists to tensors\n","train_seq = torch.tensor(trencoding['input_ids'])\n","train_mask = torch.tensor(trencoding['attention_mask'])\n","train_token_ids = torch.tensor(trencoding['token_type_ids'])\n","train_y = torch.tensor(y_train_Transformer.tolist())\n","\n","val_seq = torch.tensor(valencoding['input_ids'])\n","val_mask = torch.tensor(valencoding['attention_mask'])\n","val_token_ids = torch.tensor(valencoding['token_type_ids'])\n","val_y = torch.tensor(y_val_Transformer.tolist())\n","\n","test_seq = torch.tensor(testencoding['input_ids'])\n","test_mask = torch.tensor(testencoding['attention_mask'])\n","test_token_ids = torch.tensor(testencoding['token_type_ids'])\n","test_y = torch.tensor(y_test.tolist())\n","\n","# wrap tensors\n","train_data = TensorDataset(train_seq, train_mask, train_token_ids, train_y)\n","# sampler for sampling the data during training\n","train_sampler = RandomSampler(train_data)\n","# Train Data Loader\n","traindata = loadData(train_data, batch_size, num_workers, train_sampler)\n","\n","# wrap tensors\n","val_data = TensorDataset(val_seq, val_mask, val_token_ids, val_y)\n","# sampler for sampling the data during training\n","val_sampler = SequentialSampler(val_data)\n","# Val Data Loader\n","valdata = loadData(val_data, batch_size, num_workers, val_sampler)\n","\n","# wrap tensors\n","test_data = TensorDataset(test_seq, test_mask, test_token_ids, test_y)\n","# sampler for sampling the data during training\n","test_sampler = SequentialSampler(test_data)\n","# Val Data Loader\n","testdata = loadData(test_data, batch_size, num_workers, test_sampler)\n","\n","\n","print('Number of data in the train set', len(traindata))\n","print('Number of data in the validation set', len(valdata))\n","print('Number of data in the test set', len(testdata))\n","\n","class BERT_Arch(nn.Module):\n","    \n","    def __init__(self, n_classes, freeze_bert=False):\n","        \n","        super(BERT_Arch,self).__init__()\n","        # Instantiating BERT model object\n","        self.bert = BertModel.from_pretrained(model_name, return_dict=False)\n","        \n","        # Freeze bert layers\n","        if freeze_bert:\n","            for p in self.bert.parameters():\n","                p.requires_grad = False\n","                \n","        self.bert_drop_1 = nn.Dropout(0.3)\n","        self.fc = nn.Linear(self.bert.config.hidden_size, self.bert.config.hidden_size) # (768, 64)\n","        self.bn = nn.BatchNorm1d(768) # (768)\n","        self.bert_drop_2 = nn.Dropout(0.25)\n","        self.out = nn.Linear(self.bert.config.hidden_size, 2) # (768,2)\n","\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids):\n","        _, output = self.bert(\n","            input_ids = input_ids,\n","            attention_mask = attention_mask,\n","            token_type_ids = token_type_ids\n","        )\n","        output = self.bert_drop_1(output)\n","        output = self.fc(output)\n","        output = self.bn(output)\n","        output = self.bert_drop_2(output)\n","        output = self.out(output)        \n","        return output\n","\n","class_names = np.unique(df['gender'])\n","print('Downloading the BERT custom model...')\n","model = BERT_Arch(len(class_names))\n","model.to(device) # Model to GPU.\n","\n","#optimizer parameters\n","param_optimizer = list(model.named_parameters())\n","no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","optimizer_parameters = [{'params': [p for n, p in param_optimizer \n","                                    if not any(nd in n for nd in no_decay)],'weight_decay':0.001},\n","                        {'params': [p for n, p in param_optimizer \n","                                    if any(nd in n for nd in no_decay)],'weight_decay':0.0}]\n","\n","print('Preparing the optimizer...')\n","#optimizer \n","optimizer = AdamW(optimizer_parameters, lr=learning_rate)\n","steps = steps_per_epoch\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps = 0,\n","    num_training_steps = steps\n",")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":366,"referenced_widgets":["ec24826b21b64ce1a6cadfde92a328ac","a41670c7dd27427097c4c3d8b373bb9c","cfe131061d634fba9c93788bf9aaca4b","2b90d7f617894a3f9b5e26027a919b81","86a802086f2d4cba901ed85108edd044","51b3591ce9c4459eb3e243beb5741bba","c53c0c9010d840a2a0ffebb383f6f01c","b2183ebdd2414a2e8412f9ac21e2d78f","7404dca381d04b3f820b60ca517c94bf","db836544ff1b4656a9db21c0dba820a0","385eef8684cc43f88df0743ce7449a3e","8cf2e856c9e24aa195a57539d5563b65","c76589d4995a482cab0239df2dc45558","d889c9c64b8943b9818af59ffdae9177","a53c83494c37412ca7cf1765553abafd","8ae41388ff794630afa7a7c9c23c37d6","f48cf4737c094d628cd3f14c56926c61","90e6cd7176f7478c82816e19dc51691d","de12f90d3b984df08e6427d6304e7d16","7f6d4d2e55ca4023993ed9a3b983c41f","52e22800efdb47b5a645a96ba7b56cbb","b5d3745315f44a57af96b13891529f98","ce81e398b63a4cc3b9805d56c4965465","4ac324cb07934f2e98a118ad72d9d906","350c598bca7e4fa59694efe3a8d711b1","85bb3b3087e24b129f3355f6e6f67951","544db15263a8418c96cb52fbb8a5318f","dd0fdfceaed84c12b686bd32be2c5631","391a898d1f9549e9b8d8cf8accf79c97","dce2ceab291a4297930e8a55f5e6b944","989c6725846b46638d58ca9d99aa353e","ee80c8bd9c44425b8fb4af70ec15aa36","70e3e6e784bd42c1a7dec381f2fae2d6","8698e4da3bed48718e9272154941b453","c1c6e7d876624d379bb32f2bb902f90b","ae4e87a2c950493cac93816aad91e89b","bf6dec09851f4b31921d43da1db6b6b7","1bcb8642ca6849dd800d2d7bbed16b8e","895150e0df1a45b38e1222d33d1ef6c4","b3eebf897a8e45f6a1affbb555bf6c7e","b0a604d06fa14619b730209468ca13ef","3b94642f23fe41d991fb210e4917267d","64e3334b4fc04a4e9b26deffbe7e26b9","ce3a256bbd014c429152bb620582c8d3"]},"executionInfo":{"status":"ok","timestamp":1662411913132,"user_tz":-60,"elapsed":28361,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"}},"outputId":"36db623c-13a8-4fb6-ef42-422f6f78e83d","id":"dVO6nQbROII_"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading BERT tokenizer...\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec24826b21b64ce1a6cadfde92a328ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cf2e856c9e24aa195a57539d5563b65"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce81e398b63a4cc3b9805d56c4965465"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Number of data in the train set 448\n","Number of data in the validation set 112\n","Number of data in the test set 140\n","Downloading the BERT custom model...\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/420M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8698e4da3bed48718e9272154941b453"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Preparing the optimizer...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n"]}]},{"cell_type":"markdown","source":["### TRAINING AND EVAL FUNCS"],"metadata":{"id":"1vrc0fAQOIJD"}},{"cell_type":"code","source":["# function to train the bert model\n","def trainMODEL():\n","  \n","    print('Training...')\n","    model.train()\n","    total_loss, total_accuracy = 0, 0\n","\n","    # empty list to save model predictions\n","    total_preds=[]\n","\n","    # iterate over batches\n","    for step, batch in enumerate(traindata):\n","    \n","        # progress update after every 50 batches.\n","        if step % 50 == 0 and not step == 0:\n","            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(traindata)))\n","\n","        if torch.cuda.is_available():\n","            # push the batch to gpu\n","            batch = [r.to(device) for r in batch]\n","\n","        sent_id, mask, token_type_ids, labels = batch\n","        # clear previously calculated gradients \n","        model.zero_grad()        \n","        # get model predictions for the current batch\n","        preds = model(sent_id, mask, token_type_ids)\n","        # compute the loss between actual and predicted values\n","        loss = cross_entropy(preds, labels)\n","        # add on to the total loss\n","        total_loss = total_loss + loss.item()\n","        # backward pass to calculate the gradients\n","        loss.backward()\n","        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        # update parameters\n","        optimizer.step()\n","        # model predictions are stored on GPU. So, push it to CPU\n","        preds=preds.detach().cpu().numpy()\n","        # append the model predictions\n","        total_preds.append(preds)\n","        \n","        torch.cuda.empty_cache()\n","\n","    # compute the training loss of the epoch\n","    avg_loss = total_loss / len(traindata)\n","\n","    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n","    # reshape the predictions in form of (number of samples, no. of classes)\n","    total_preds  = np.concatenate(total_preds, axis=0)\n","\n","    #returns the loss and predictions\n","    return avg_loss, total_preds\n","\n","# function for evaluating the model\n","def evaluate():\n","  \n","    print(\"\\nEvaluating...\")\n","    t0 = time.time()\n","    \n","    model.eval() # deactivate dropout layers\n","    total_loss, total_accuracy = 0, 0\n","    \n","    # empty list to save the model predictions\n","    total_preds = []\n","\n","    # iterate over batches\n","    for step, batch in enumerate(valdata):\n","        # Progress update every 50 batches.\n","        if step % 50 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(valdata)))\n","\n","        if torch.cuda.is_available():\n","            # push the batch to gpu\n","            batch = [t.to(device) for t in batch]\n","\n","        sent_id, mask, token_type_ids, labels = batch\n","\n","        # deactivate autograd\n","        with torch.no_grad(): # Dont store any previous computations, thus freeing GPU space\n","\n","            # model predictions\n","            preds = model(sent_id, mask, token_type_ids)\n","            # compute the validation loss between actual and predicted values\n","            loss = cross_entropy(preds, labels)\n","            total_loss = total_loss + loss.item()\n","            preds = preds.detach().cpu().numpy()\n","            total_preds.append(preds)\n","\n","        torch.cuda.empty_cache()\n","    # compute the validation loss of the epoch\n","    avg_loss = total_loss / len(valdata) \n","    # reshape the predictions in form of (number of samples, no. of classes)\n","    total_preds  = np.concatenate(total_preds, axis=0)\n","\n","    return avg_loss, total_preds"],"metadata":{"id":"krQd3ysLOIJD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## TRAINING"],"metadata":{"id":"myTuPUS4OIJF"}},{"cell_type":"code","source":["# set initial loss to infinite\n","best_valid_loss = float('inf')\n","\n","# Empty lists to store training and validation loss of each epoch\n","train_losses=[]\n","valid_losses=[]\n","\n","# for each epoch perform training and evaluation\n","for epoch in range(epochs):\n","     \n","    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n","    \n","    #train model\n","    train_loss, _ = trainMODEL()\n","    \n","    #evaluate model\n","    valid_loss, _ = evaluate()\n","    \n","    print('Evaluation done for epoch {}'.format(epoch + 1))\n","    #save the best model\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        print('Saving model...')\n","        torch.save(model.state_dict(), 'user_gender_class/gender_text.pt') # Save model weight's (you can also save it in .bin format)\n","    \n","    # append training and validation loss\n","    train_losses.append(train_loss)\n","    valid_losses.append(valid_loss)\n","    \n","    print(f'\\nTraining Loss: {train_loss:.3f}')\n","    print(f'Validation Loss: {valid_loss:.3f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662412201984,"user_tz":-60,"elapsed":288546,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"}},"outputId":"b5d69718-a30c-4120-f8b8-50d37c52f9ea","id":"MY1FCxXHOIJG"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," Epoch 1 / 3\n","Training...\n","  Batch    50  of    448.\n","  Batch   100  of    448.\n","  Batch   150  of    448.\n","  Batch   200  of    448.\n","  Batch   250  of    448.\n","  Batch   300  of    448.\n","  Batch   350  of    448.\n","  Batch   400  of    448.\n","\n","Evaluating...\n","  Batch    50  of    112.\n","  Batch   100  of    112.\n","Evaluation done for epoch 1\n","Saving model...\n","\n","Training Loss: 0.659\n","Validation Loss: 0.575\n","\n"," Epoch 2 / 3\n","Training...\n","  Batch    50  of    448.\n","  Batch   100  of    448.\n","  Batch   150  of    448.\n","  Batch   200  of    448.\n","  Batch   250  of    448.\n","  Batch   300  of    448.\n","  Batch   350  of    448.\n","  Batch   400  of    448.\n","\n","Evaluating...\n","  Batch    50  of    112.\n","  Batch   100  of    112.\n","Evaluation done for epoch 2\n","Saving model...\n","\n","Training Loss: 0.549\n","Validation Loss: 0.542\n","\n"," Epoch 3 / 3\n","Training...\n","  Batch    50  of    448.\n","  Batch   100  of    448.\n","  Batch   150  of    448.\n","  Batch   200  of    448.\n","  Batch   250  of    448.\n","  Batch   300  of    448.\n","  Batch   350  of    448.\n","  Batch   400  of    448.\n","\n","Evaluating...\n","  Batch    50  of    112.\n","  Batch   100  of    112.\n","Evaluation done for epoch 3\n","\n","Training Loss: 0.446\n","Validation Loss: 0.610\n"]}]},{"cell_type":"markdown","source":["## TEST set"],"metadata":{"id":"G6pizohUOIJH"}},{"cell_type":"code","source":["print('\\nTest Set...')\n","\n","test_preds = []\n","\n","print('Total batches:', len(testdata))\n","\n","for fold_index in range(0, 3):\n","    \n","    print('\\nFold Model', fold_index)\n","    \n","    # Load the fold model\n","    path_model = 'user_gender_class/gender_text.pt'\n","    model.load_state_dict(torch.load(path_model))\n","\n","    # Send the model to the GPU\n","    model.to(device)\n","\n","    stacked_val_labels = []\n","    \n","    # Put the model in evaluation mode.\n","    model.eval()\n","\n","    # Turn off the gradient calculations.\n","    # This tells the model not to compute or store gradients.\n","    # This step saves memory and speeds up validation.\n","    torch.set_grad_enabled(False)\n","\n","\n","    # Reset the total loss for this epoch.\n","    total_val_loss = 0\n","\n","    for j, test_batch in enumerate(testdata):\n","\n","        inference_status = 'Batch ' + str(j + 1)\n","\n","        print(inference_status, end='\\r')\n","\n","        b_input_ids = test_batch[0].to(device)\n","        b_input_mask = test_batch[1].to(device)\n","        b_token_type_ids = test_batch[2].to(device)\n","        b_test_y = test_batch[3].to(device)\n","\n","\n","        outputs = model(b_input_ids, \n","                        attention_mask=b_input_mask,\n","                        token_type_ids=b_token_type_ids)\n","\n","        # Get the preds\n","        preds = outputs[0]\n","\n","        # Move preds to the CPU\n","        val_preds = preds.detach().cpu().numpy()\n","        \n","        #true_labels.append(b_test_y.to('cpu').numpy().flatten())\n","        \n","        # Stack the predictions.\n","        if j == 0:  # first batch\n","            stacked_val_preds = val_preds\n","            \n","        else:\n","            stacked_val_preds = np.vstack((stacked_val_preds, val_preds))\n","            \n","    test_preds.append(stacked_val_preds)\n","    \n","            \n","print('\\nPrediction complete.')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662412225291,"user_tz":-60,"elapsed":23351,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"}},"outputId":"e6f8f2ff-6d91-44b2-e571-0403c2600006","id":"EFXXPI2kOIJH"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Test Set...\n","Total batches: 140\n","\n","Fold Model 0\n","\n","Fold Model 1\n","Batch 140\n","Fold Model 2\n","Batch 140\n","Prediction complete.\n"]}]},{"cell_type":"code","source":["# Sum the predictions of all fold models\n","for i, item in enumerate(test_preds):\n","    if i == 0:\n","        preds = item\n","    else:\n","        # Sum the matrices\n","        preds = item + preds\n","\n","# Average the predictions\n","avg_preds = preds/(len(test_preds))\n","\n","#print(preds)\n","#print()\n","#print(avg_preds)\n","\n","# Take the argmax. \n","# This returns the column index of the max value in each row.\n","test_predictions = np.argmax(avg_preds, axis=1)\n","\n","true_y = []\n","for j, test_batch in enumerate(testdata):\n","    true_y.append(int(test_batch[3][0].numpy().flatten()))\n","print(true_y)\n","\n","# Accuracy and classification report \n","target_names = ['true_y', 'predicted_y']\n","\n","data = {'true_y': true_y,\n","       'predicted_y': test_predictions}\n","\n","df_pred_BERT = pd.DataFrame(data, columns=['true_y','predicted_y'])\n","\n","confusion_matrix = pd.crosstab(df_pred_BERT['true_y'], df_pred_BERT['predicted_y'], rownames=['True'], colnames=['Predicted'])\n","\n","sns.heatmap(confusion_matrix, annot=True)\n","plt.show()\n","\n","print('----')\n","print('Accuracy of BERT model', accuracy_score(true_y, test_predictions))\n","print('----')\n","\n","print(classification_report(true_y, test_predictions, target_names=target_names))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":536},"executionInfo":{"status":"ok","timestamp":1662412226309,"user_tz":-60,"elapsed":1034,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"}},"outputId":"21354cd6-9443-4d51-e0be-db9517f8345a","id":"usbDHCeiOIJJ"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0]\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 2 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"stream","name":"stdout","text":["----\n","Accuracy of BERT model 0.7142857142857143\n","----\n","              precision    recall  f1-score   support\n","\n","      true_y       0.68      0.76      0.72        68\n"," predicted_y       0.75      0.67      0.71        72\n","\n","    accuracy                           0.71       140\n","   macro avg       0.72      0.72      0.71       140\n","weighted avg       0.72      0.71      0.71       140\n","\n"]}]},{"cell_type":"markdown","source":["# LOGISTIC REGRESSION"],"metadata":{"id":"eaOL3OR-GfP8"}},{"cell_type":"code","source":["nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","nltk.download('omw-1.4')\n","remove_rt = lambda x: re.sub('RT @\\w+: ',\" \",x)\n","rt = lambda x: re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",x)\n","\n","lemma = WordNetLemmatizer()\n","\n","from nltk.corpus import stopwords\n","stop_words = stopwords.words('english')\n","stop_words.extend(['u', 'wa', 'ha', 'would', 'com'])\n","\n","def remove_stopwords(text):\n","    text = [word for word in text if word not in stopword]\n","    return text\n","\n","def tokenization(text):\n","    text = re.split('\\W+', text)\n","    return text\n","\n","p.set_options(p.OPT.URL, p.OPT.MENTION, p.OPT.NUMBER)\n","df['clean_description'] = df.description.map(remove_rt).map(rt).map(p.clean)#.apply(lambda x: tokenization(x)).apply(lambda x: remove_stopwords(x))\n","df['clean_text'] = df.text.map(remove_rt).map(rt).map(p.clean)#.apply(lambda x: tokenization(x)).apply(lambda x: remove_stopwords(x))\n","\n","n = len(df)\n","df['sep'] = ['.' for i in range(n)]\n","df['txt'] = df['clean_description'] + df['sep'] + df['clean_text']\n","\n","df['txt'] = df['txt'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n","\n","\n","df['gender'] = (df['gender']).astype(int)\n","\n","# Split test and train data using 25% of the dataset for validation purposes\n","x_train, x_test, y_train, y_test = train_test_split(df['txt'], \n","                                                      df['gender'], test_size=0.2, shuffle=True, random_state=42)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LpGH-yYAMSls","executionInfo":{"status":"ok","timestamp":1662458539336,"user_tz":-60,"elapsed":2541,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"}},"outputId":"7e87e7fb-4c92-4c1f-fd3f-37b24dea83ac"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]}]},{"cell_type":"code","source":["type(x_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UcM_GYEHLY2j","executionInfo":{"status":"ok","timestamp":1662459597642,"user_tz":-60,"elapsed":310,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"}},"outputId":"8c321684-ba18-49f1-8e3c-41d518fbf909"},"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["pandas.core.series.Series"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.pipeline import Pipeline\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.metrics import classification_report, accuracy_score\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Create a Pipeline with the TfidfVectorizer and LogisticRegression model\n","LR_pipeline = Pipeline(steps = [('tf', TfidfVectorizer()), \n","                                ('lgrg', LogisticRegression())]) # initialize TfidfVectorizer and LogisticRegression\n","\n","\n","# Create Parameter Grid\n","pgrid_lgrg = {\n"," 'tf__max_features' : [1000, 2000, 3000],\n"," 'tf__ngram_range' : [(1,1),(1,2)],\n"," 'tf__use_idf' : [True, False],\n"," 'lgrg__penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n"," 'lgrg__class_weight' : ['balanced', None]\n","}\n","\n","# Apply GridSearch to Pipeline to find the best parameters\n","gs_lgrg = GridSearchCV(LR_pipeline, pgrid_lgrg, cv=2, n_jobs=-1, verbose=2)"],"metadata":{"id":"HR4W7wNPGlnk","executionInfo":{"status":"ok","timestamp":1662458549017,"user_tz":-60,"elapsed":198,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["gs_lgrg.fit(x_train, y_train) # Train LR model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Gk-p0JlG5Bk","executionInfo":{"status":"ok","timestamp":1662458566667,"user_tz":-60,"elapsed":15469,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"}},"outputId":"3f370059-06dd-4468-b996-504a380e8574"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 2 folds for each of 96 candidates, totalling 192 fits\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","96 fits failed out of a total of 192.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","48 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 394, in fit\n","    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 449, in _check_solver\n","    % (solver, penalty)\n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","--------------------------------------------------------------------------------\n","48 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 394, in fit\n","    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 449, in _check_solver\n","    % (solver, penalty)\n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n"," 0.62825228 0.629369   0.62791711 0.62858708 0.64154118 0.63886109\n"," 0.63908431 0.63573425 0.64343935 0.63953081 0.63975435 0.63953141\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n"," 0.61060818 0.61083127 0.60647645 0.6089329  0.5903959  0.59061944\n"," 0.58860939 0.58726898 0.60770528 0.60290325 0.60558372 0.60312731\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n"," 0.62936885 0.62981568 0.62869886 0.63048579 0.64422123 0.63863758\n"," 0.64120586 0.63718609 0.64567287 0.64087142 0.64455615 0.64198796\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n"," 0.61250665 0.61217123 0.60792812 0.61183646 0.59084255 0.59262914\n"," 0.58726938 0.58726888 0.60759377 0.60268016 0.60960397 0.60558364]\n","  category=UserWarning,\n"]},{"output_type":"execute_result","data":{"text/plain":["GridSearchCV(cv=2,\n","             estimator=Pipeline(steps=[('tf', TfidfVectorizer()),\n","                                       ('lgrg', LogisticRegression())]),\n","             n_jobs=-1,\n","             param_grid={'lgrg__class_weight': ['balanced', None],\n","                         'lgrg__penalty': ['l1', 'l2', 'elasticnet', 'none'],\n","                         'tf__max_features': [1000, 2000, 3000],\n","                         'tf__ngram_range': [(1, 1), (1, 2)],\n","                         'tf__use_idf': [True, False]},\n","             verbose=2)"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["gs_lgrg.best_params_"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8p1wKM1iG6yq","executionInfo":{"status":"ok","timestamp":1662458571103,"user_tz":-60,"elapsed":328,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"}},"outputId":"5bed5d55-eeee-489a-a297-4237b2a0240e"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'lgrg__class_weight': None,\n"," 'lgrg__penalty': 'l2',\n"," 'tf__max_features': 3000,\n"," 'tf__ngram_range': (1, 1),\n"," 'tf__use_idf': True}"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["import joblib \n","\n","path = 'user_gender_class/model/logistic_gender'\n","# joblib.dump(gs_lgrg, path)\n","\n","mod = joblib.load(path)\n","\n","print('Score of train set', mod.score(x_train, y_train))\n","print('Score of test set',mod.score(x_test, y_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mu4a296iQk7D","executionInfo":{"status":"ok","timestamp":1662458911857,"user_tz":-60,"elapsed":398,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"}},"outputId":"99958bf4-3854-4b35-8f27-8620b630ca71"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Score of train set 0.7884980457844779\n","Score of test set 0.6636891469405984\n"]}]},{"cell_type":"code","source":["print('Score of train set', gs_lgrg.score(x_train, y_train))\n","print('Score of test set',gs_lgrg.score(x_test, y_test))"],"metadata":{"id":"2PJDoQuCIwHt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662458572645,"user_tz":-60,"elapsed":297,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"}},"outputId":"67fe6b04-5403-4fba-cd36-46ba32e6ef31"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Score of train set 0.7884980457844779\n","Score of test set 0.6636891469405984\n"]}]},{"cell_type":"code","source":["LR_pred = gs_lgrg.predict(x_test) # Predict on validation data\n","\n","data = {'true_y': y_test,\n","       'predicted_y': LR_pred}\n","df_pred = pd.DataFrame(data, columns=['true_y','predicted_y'])\n","confusion_matrix = pd.crosstab(df_pred['true_y'], df_pred['predicted_y'], rownames=['True'], colnames=['Predicted'])\n","\n","sns.heatmap(confusion_matrix, annot=True)\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":279},"id":"cTKcFMoWG8bM","executionInfo":{"status":"ok","timestamp":1662458577907,"user_tz":-60,"elapsed":673,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"}},"outputId":"753a33ca-c7c3-4aca-b2b0-3cdb1c622024"},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 2 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAW4AAAEGCAYAAABFBX+4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de7xWY/7/8de7czoXJdUoJIcxNYlyGIOI4ithTJkhyWy+ch6MBoMIM8Mw/cYpp2GcT83kMJRMyCElMUQ/yaG2DiQlovben+8f96ruah/uXftu75X3s8d67Htda61rXfdoPvvqs651XYoIzMwsPWpVdwPMzKxyHLjNzFLGgdvMLGUcuM3MUsaB28wsZepUdwPKsvKL2R7uYutpuM3PqrsJVgMVrSjUxtZRmZhTd8vtNvp+G8M9bjOzlKmxPW4zs02qpLi6W5AzB24zM4DioupuQc4cuM3MgIiS6m5Czhy4zcwAShy4zczSxT1uM7OU8cNJM7OUcY/bzCxdwqNKzMxSxg8nzcxSxqkSM7OU8cNJM7OUcY/bzCxlUvRw0rMDmplB5uFkrls5JHWRND1rWyrpbEmXSSrMKu+Xdc1wSbMkzZR0SEVNdY/bzAyIqJocd0TMBLoBSKoNFAJjgCHA9RFxbfb5knYBBgK7AtsAz0naMcppkHvcZmaQyXHnuuWuN/BhRHxSzjn9gQcj4vuI+AiYBexZXqUO3GZmUKlUiaQCSVOztoIyah0IPJC1f7qktyXdKalFUtYOmJN1ztykrEwO3GZmUKked0SMjogeWdvodauTVA84AngkKboZ2J5MGmUecN2GNtU5bjMzgOKVVV1jX2BaRCwAWPUTQNJtwJPJbiHQIeu69klZmdzjNjODKhtVkmUQWWkSSW2zjg0A3kk+jwUGSqovqRPQGXi9vIrd4zYzgyp9AUdSI+Bg4JSs4j9J6gYE8PGqYxHxrqSHgRlAETCsvBEl4MBtZpZRhZNMRcQ3QKt1yo4v5/yRwMhc63fgNjMDzw5oZpY2UfUPJ/PGgdvMDDzJlJlZ6jhVYmaWMu5xm5mljHvcZmYp4x63mVnKFKVnIQUHbjMzcI/bzCx1nOM2M0sZ97jNzFLGPW4zs5Rxj9vMLGU8qsTMLGUiqrsFOfMKOGZmUGUr4EjqIml61rZU0tmSWkoaL+mD5GeL5HxJGiVpVrKQcPeKmurAbWYGVRa4I2JmRHSLiG7A7sC3wBjgQmBCRHQGJiT7kFmbsnOyFZBZVLhcDtxmZlCpVd4roTfwYUR8AvQH7k7K7waOTD73B+6JjNeA5uusT7keB24zM4Di4pw3SQWSpmZtBWXUOpA1Cwa3iYh5yef5QJvkcztgTtY1c5OyMvnhpJkZVGocd0SMBkaXd46kesARwPBSrg9JG/w01IHbzAzy8QJOX2BaRCxI9hdIahsR85JUyMKkvBDokHVd+6SsTE6VmJlBPnLcg1iTJgEYCwxOPg8G/pVVfkIyuqQXsCQrpVIq97jNzIAoqbpx3JIaAQcDp2QVXwM8LGko8AlwbFL+NNAPmEVmBMqQiup34DYzgypNlUTEN0CrdcoWkRllsu65AQyrTP0O3GZmkBkxkhIO3GZm4NkBzcxSx4H7h+ujT+Zy3h+uXr0/97N5nH7y8Rz/ywGry5Ys/ZpLrr6eOYXzqF+vHlf8/hw6b9dxo+67YsUKhl9xHTNmfkDzZk25dsRw2rVtwyuvT+OGW+5i5coi6tatw2+HDaXn7t026l5WefXr12fi849Rr3596tSpzeOPP8XlI64r9dwBA/rxyEO30bNXX96Y9vZG3bdjxw7cf+9NtGzZgmlv/pfBJ57JypUrOfusAk46aRBFRUV88fmXnFxwLp9+Wu4ItM2fJ5n64eq0bXseu/tGHrv7Rh6+cxQNGjSg98/3Xuuc2+55iJ06b8+Ye27mqkvO45obbsm5/sJ5Czjx9AvWK3/8yXE0bdKYfz98J8f/8kj+ctOdALRo3pS//fEyxvzjZkZe/FuGj7h2476gbZDvv/+eg/ocy+49Dmb3Hn04pM/+9Nxz/bmEGjduxJmnD2Xy5GmVqv+E44/lD5ecu1751VddxA2jbmOnXfZl8eIlnDRkEADTp79Dz1596b77wTz2+FNcc/XFG/bFNidVNFfJpuDAnUevTZ1Oh3Zt2WbrNmuVf/jxp/Ts3hWA7bbtQOG8BXzx5WIAnnj2eQaefBZHDx7G5X8aRXGOD0yef+lV+vc7CIA++/+MyW9MJyLYeccdaL1V5uH2Dp225bvvv2fFihVV9RWtEr755lsA6tatQ526dYlSeniXX3YBf772Jr777rvVZbVq1eKPV1/Mq688xbQ3xvObk3+d8z0P2H8fHnvsKQD+8Y9H6H/EIQBMfOEVli/P3GPy62/Qvl25U2P8MJRE7ls1y1vglrSTpN8l0xWOSj7vnK/71UT/nvAC/Q76+XrlXXbYjudeeBmA/86YybwFC1mw8As+/PhTnpnwAv+45Toeu/tGatWqxZPj/pPTvRZ+voitW28JQJ06tWncaAu+WrJ0rXPGT5zELl12oF69ehv5zWxD1KpVi6lTxjGv8G0mTHiR16e8udbxn3b7MR06tOXpf09Yq/ykIYNYsvRr9tr7MHrtdRhDhx5Hx44dqEirVi346qslq3/5zy2cxzbttl7vvCEnDuKZZ3P7e7ZZq8RcJdUtLzluSb8j89bQg8DrSXF74AFJD0bENWVcV0BmWkNuuu5KTj5hUD6at0msXLmSiZMmc/ap64+lP/n4X3DNDbdy9OBhdN6+Izt13p7atWoxeep0Zrw/i4FDzwIy/7xu2aI5AGcOH0HhZwtYWbSSeQs+5+jBmWGfvz62PwMO61Nhe2bN/oS/3HQno68fWYXf0iqjpKSEHnv0oVmzpjz2yB3sumsX3n13JgCSuPbPl3LSyeesd93BB/+c3XbbmaOOOgyAZk2b0HmHTixduoxxzz4EQMsWzalXry5HHHEoACcOOZN58xasV9e6jjvuKHrs3pUDeh9dVV8ztaIGpEByla+Hk0OBXSNiZXahpL8A75J5g2g92RO3rPxidvX/e2QjvPTaVHbecXu2bNlivWONGzXiyosy+ciI4JBjTqR9u6154613OKLvQZzzv+sH+1FX/wHI5LgvGnkdf//bn9Y63nqrVsxf+AVbt96KoqJiln3zLc2bNQVg/sLPOev3V3DVJefxo/bbVPVXtUpasmQpE194mUP67L86cDdp0phdd92JCeMfBWDrrbdizON3MeCoIUhw9tkXM278C+vV1WOPzC/tE44/lo4d2zPiir+sdbx582bUrl2b4uJi2rdry2eF81cf633gzxh+4Zkc2Ptop8+gRqRAcpWvVEkJUFqEaJsc2+w9PX4i/Q7ev9RjS79exsqVmd9pjz3xDLt3243GjRrRq0c3xk+cxKLFXwGZ0Sefza+41wRwwL69+NfTzwEwbuJL9Ny9K5JY+vUyTjv/Us4+dQjdf7Lrxn8x2yBbbtmSZskv0gYNGnBQ7/2YOfPD1ceXLv2arbfZjR127MUOO/Zi8uRpDDhqCG9Me5tx417glFNOoE6dTD+rc+ft2GKLhjndd+ILr3D00Zme+vHH/4KxT4wDoFu3XbnpxmsYcNQQPv98UVV+1fTKz3zceZGvHvfZwARJH7BmntkfATsAp+fpnjXGt8u/49Upb3LpBWeuLntoTOYB0S8HHMbsT+Zw0ZXXIWD7TtsyYvjZkHw+4zcnUHD2RZRECXXr1OGic09b7+FmaY46/BCGX/Fn+h57Es2aNuHPl2cW13jgsSeYM/czbrnrfm65634ARt8wklZJCsY2jbZt23DnHTdQu3YtatWqxaOPPsFTTz/HZZeex9Q33uLJJ8eXee0dd95Px44dmPL6M0jii8+/5KhjTsrpvsN/P5L7772JEZddwPS33uXOuzJzHv3x6kto3LgRDz5wKwBz5hQy4KgKp8jYvKWox63SnmxXScVSLWBP1kwIXghMiYicMvtpT5VYfjTc5mfV3QSrgYpWFGpj6/jmDwNzjjmNRjy40ffbGHl7ASciSoDX8lW/mVmVqgEpkFz5zUkzM0hVqsSB28wMDwc0M0ufFPW4/cq7mRlU6SvvkppLelTS+5Lek7SXpMskFUqanmz9ss4fLmmWpJmSDqmofve4zcygql9l/yvwTEQck6z2vgVwCHB9RKw105ukXYCBwK5k3n95TtKO5Y3Ac4/bzIzMmpO5buWR1AzYD7gDICJWRMRX5VzSH3gwIr6PiI/IrD25Z3n3cOA2M4NKpUokFUiamrUVZNXUCfgcuEvSm5JuTxYPBjhd0tuS7pS0aj6Mdqx5URFgLmvefymVA7eZGVRqPu6IGB0RPbK20Vk11QG6AzdHxE+Bb4ALgZuB7YFuwDyg9JU0cuDAbWYGVflwci4wNyImJ/uPAt0jYkFEFCcvJ97GmnRIIZA9T2/7pKxMDtxmZlBlgTsi5gNzJHVJinoDMyRlr1YxAHgn+TwWGCipvqROQGfWTIddKo8qMTMDorhKX8A5A7gvGVEyGxgCjJLUDQjgY+AUgIh4V9LDwAygCBhW0ZxODtxmZlClL+BExHSgxzrFx5dz/kgg51VOHLjNzKDCYX41iQO3mRmk6pV3B24zM0jV2lwO3GZmQBSlJ3I7cJuZgXvcZmZp44eTZmZp4x63mVm6uMdtZpY27nGbmaVLFFV3C3LnwG1mBoR73GZmKePAbWaWLu5xm5mljAO3mVnKRLGquwk58wo4ZmZkety5bhWR1FzSo5Lel/SepL0ktZQ0XtIHyc8WybmSNErSrGQh4e4V1e/AbWYGRIly3nLwV+CZiNgJ6Aq8R2bB4AkR0RmYkOwD9CWzXFlnoIDMosLlcuA2M6PqetySmgH7AXcARMSKiPgK6A/cnZx2N3Bk8rk/cE9kvAY0X2d9yvU4cJuZARHKeZNUIGlq1laQVVUn4HPgLklvSrpdUiOgTUTMS86ZD7RJPrcD5mRdPzcpK5MfTpqZUblRJRExGhhdxuE6QHfgjIiYLOmvrEmLrLo+JG3w5CjucZuZASXFynmrwFxgbkRMTvYfJRPIF6xKgSQ/FybHC4EOWde3T8rK5MBtZkbVPZyMiPnAHEldkqLewAxgLDA4KRsM/Cv5PBY4IRld0gtYkpVSKZVTJWZmkOtokVydAdwnqR4wGxhCpqP8sKShwCfAscm5TwP9gFnAt8m55XLgNjMDogqn446I6UCPUg71LuXcAIZVpn4HbjMzqrzHnVcO3GZmZIYDpoUDt5kZULw5zVWSPOn8taQ/JPs/krRn/ptmZrbpVOYFnOqWy3DAm4C9gEHJ/tfAjXlrkZlZNajiuUryKpdUSc+I6C7pTYCIWJwMcTEz22xU5aiSfMslcK+UVBsIAElbkapFfszMKlYTetK5yiVwjwLGAK0ljQSOAS7Oa6vMzDax4pL0vEheYeCOiPskvUFm4LiAIyPivby3zMxsE9qsUiWSfkTmNcwnsssi4tN8NszMbFMqqQGjRXKVS6rkKTL5bQENyMw1OxPYNY/tMjPbpGrCML9c5ZIq2S17P1kP7bS8tcjMrBpsVqmSdUXENEk989GYbL27/ibft7AU+vq246u7CbaZ2qxSJZLOzdqtRWZC8M/y1iIzs2qwWY0qAZpkfS4ik/N+LD/NMTOrHinKlJQfuJMXb5pExHmbqD1mZtWiKlMlkj4mMz1IMVAUET0kXQb8hsxCwgC/j4ink/OHA0OT88+MiGfLq7/MwC2pTkQUSdpno7+FmVkNl4dRJQdExBfrlF0fEddmF0jaBRhIZqTeNsBzknaMiOKyKi6vx/06mXz2dEljgUeAb1YdjIjHK/cdzMxqrmqcx6M/8GBEfA98JGkWsCfwalkX5JKNbwAsAg4EDgf+J/lpZrbZCJTzJqlA0tSsrWC96mCcpDfWOXa6pLcl3SmpRVLWDpiTdc7cpKxM5fW4WycjSt5hzQs42Y0yM9tsFFUiVRIRo4HR5Zyyb0QUSmoNjJf0PnAzcAWZ+HkFcB1w0oa0tbzAXRtozNoBe3W7N+RmZmY1VZQa6jawrojC5OdCSWOAPSPixVXHJd0GPJnsFgIdsi5vn5SVqbzAPS8iRmxQq83MUqaqctySGgG1IuLr5HMfYISkthExLzltAJlsBsBY4H5JfyHzcLIzmWeMZSovcKfnNSIzs41UhT3uNsAYSZCJsfdHxDOS/iGpG5mMxcfAKQAR8a6kh4EZZN6VGVbeiJJVlZal98a338wsHaqqxx0Rs4GupZSXOV9DRIwERuZ6jzIDd0R8mWslZmZpV5yiJEOlJ5kyM9scpWjlMgduMzOAEve4zczSJU1jnB24zcyo1lfeK82B28wMKJFTJWZmqVLuwOkaxoHbzAyPKjEzSx2PKjEzSxmPKjEzSxmnSszMUsbDAc3MUqbYPW4zs3Rxj9vMLGUcuM3MUqYSS05Wu1xWeTcz2+yVVGKriKSPJf1X0nRJU5OylpLGS/og+dkiKZekUZJmJSvAd6+ofgduMzMyr7znuuXogIjoFhE9kv0LgQkR0RmYkOwD9CWzzmRnoIDMavDlcuA2MyMzjjvXbQP1B+5OPt8NHJlVfk9kvAY0l9S2vIocuM3MqFyqRFKBpKlZW8E61QUwTtIbWcfaZK3yPp/MosIA7YA5WdfOTcrK5IeTZmZUblRJRIwGRpdzyr4RUSipNTBe0vvrXB+SNvgte/e4zczIdJFz3SqsK6Iw+bkQGAPsCSxYlQJJfi5MTi8EOmRd3j4pK5MDt5kZVZfjltRIUpNVn4E+wDvAWGBwctpg4F/J57HACcnokl7AkqyUSqmcKjEzo0oXUmgDjFFmRZ06wP0R8YykKcDDkoYCnwDHJuc/DfQDZgHfAkMquoEDt5kZUFJFE7tGxGygaynli4DepZQHMKwy93DgNjPDr7ybmaWOF1IwM0sZ97jNzFKmaMOHVW9yDtxmZjhVYmaWOk6VmJmlTFUNB9wUHLjNzHCqxMwsdZwqMTNLmeIU9bkduM3McI/bzCx1wj1uM7N0cY/bqFWrFqP/fRNfzF/EhYMvWutY1567ccblw9hu5+24/LQreeGpFzf6fk2aN+Gymy+hbYc2zJuzgEtPHcGyJcs4eEBvjjttIBJ8+81yrht+Ax/OmL3R97ON9/Gir7lgzJTV+4VffcP/7rczv95zhw2uc+zbn3DbyzMB+M0+XTjiJ9uyfGUR5z/+OnMXf0OtWuLnnbfmrAN+vNHt39ykaTigF1LIk2NOPopPPvi01GMLChdy1Tl/4rl/Tqh0vd326srw6y9Yr/xXwwYxbdI0jtt3MNMmTePXwwYBMG/OPM445hxOPOg33H3DvZz/x3MrfU/Lj46tmvDwyQfy8MkH8sBJB9Cgbm0O7LJNTtcOvfclCr/6Zq2yJctXcOuk97n3xP2578T9uXXS+yxdvgKAwT07889TD+ahoQcyfc6XTPpwfpV/n7SryhVw8s2BOw+2arsle/XuyVMPPF3q8flzFzD7vdlEyfp/BQaeeiy3PnUjd42/jSG/HVzK1aXb95C9eeaRcQA888g49j10HwDemTqDZUuWAfDutBls1Xaryn4d2wQmf7yQ9i0asU2zLZizeBmnPfgyg+78D0PueZGPvvg6pzpemb2AXh1b06xhPZo2rEevjq15efYCGtatwx4dM//d69auxU5bN2fB0uX5/DqpVETkvOVCUm1Jb0p6Mtn/u6SPJE1Ptm5JuSSNkjRL0tuSuldUtwN3Hpxx+TBuvnI0JaUE5vLssd/utO/UjlMOG8ZJfQro8pMd6dpzt5yubbFlCxYt/BKARQu/pMWWLdY75/CBfZn8n9cr1SbbNJ6dMZe+u7QH4Iqnp/O7Pl154KQDOLf3j7nq2ek51bHw6+/YumnD1fttmjZk4dffrXXO0u9W8OKsefTs2LrqGr+ZiEr8ydFZwHvrlJ0fEd2SbdV/2L5A52QrAG6uqOJNnuOWNCQi7irjWAGZhrNDsy60bVTuCvU10l4H9WLxF4v5///9gG57rbcIRrn2+HkP9vh5D+4YdysADbdoSPtO7Xlr8n+55Ym/Ubd+XRpu0ZCmzZusPueWkbcx5YWp61cWa//l+une3ThsUF+GDTh7w76Y5c3K4hJe+GA+Z+6/K9+uKOKtwkWc//jrWcczi2r9861PuH/KhwDMWbyMMx56lTq1a9Gu+RZcf0yvCu9TVFLC8H9OZVCP7WnfolF+vkyKVeXDSUntgcOAkUBF+cn+wD3JSjivSWouqW15605Wx8PJy4FSA3f2kvf7tetdE1JJlbZbj13Zp8/e9DqwJ/Xq16NRky24eNRwrjzz6gqvlcR9f3uAsfc+ud6xU//ndCCT4+577CFcfc6f1jq++IvFtGrdkkULv6RV65YsXvTV6mPb7bwdF/z5t5x//HCWLl66kd/QqtqkD+ez09bNadW4Acu+X0mT+nV5+OQD1zvvyK7bcmTXbYFMjnvE4d1p13xNAG7dpAFTP/li9f6Cpcvpse2Wq/evePpNftSy0UY9/NycVWY4YHYnMzE6iV+r3ABcADRZ59KRkv4ATAAujIjvgXbAnKxz5iZlZQbuvKRKkjxNadt/ySykudkafc0dHNNjIL/s9SsuP+1Kpr08PaegDfD6xCn0++WhNNyiAQBbbr0lzVs1z+nal8e9wqG/6APAob/ow6RnXwGg9TatufK2yxh51tXMnT13A76R5dsz787l0CRN0rh+XbZp3ohx7xUCEBHMXLAkp3r23q4Nr360kKXLV7B0+Qpe/Wghe2+X+b/b3ybOYNn3RZx/8E/y8yU2AyWV2CJidET0yNpWB21JhwMLI+KNdW4xHNgJ2ANoCfxuQ9uarx53G+AQYPE65QJeydM9a7STzjuRmW/N5OXxr7JT1y5cecflNGnWmL0P3ouTfjuYwQcOZcqLb7Bt5225eez/A+Dbb7/jyjOu4qus3nNZ7rvxQS6/5RIOG9SX+XMXcOmpVwBw4jnH06xFU8656iwAiouKKeh3Wv6+qFXK8hVFvPbxQi7u+9PVZVf378HIZ6Zz+8vvU1QcHLJLe7q0aVZhXc0a1qNg3y786u8TASjYdyeaNazHgqXLuf2VmXRq1ZiBd/wHgIE9tuOobh3z8ZVSqziq7B/5+wBHSOoHNACaSro3In6dHP9e0l3Aecl+IdAh6/r2SVmZFFXX2DWVSncAd0XEpFKO3R8Rx1VUR1pTJZZfz161R3U3wWqghoOv0cbWcdy2A3KOOfd/Mian+0naHzgvIg5flbeWJOB64LuIuFDSYcDpQD+gJzAqIvYsr9689LgjYmg5xyoM2mZmm9omeOX9Pklbkck8TAdOTcqfJhO0ZwHfAkMqqshvTpqZkZ9X3iNiIjAx+bz+E+dMeQDDKlOvA7eZGel65d2B28wMzw5oZpY6VTiqJO8cuM3McKrEzCx1PB+3mVnKOMdtZpYyTpWYmaVMPt4izxcHbjMzoNg9bjOzdHGqxMwsZZwqMTNLGfe4zcxSxsMBzcxSxq+8m5mlTJpSJXlZc9LMLG1KiJy3XEiqLelNSU8m+50kTZY0S9JDkuol5fWT/VnJ8Y4V1e3AbWZGZlRJrluOzgLey9r/I3B9ROxAZj3eVSuFDQUWJ+XXJ+eVy4HbzIyq7XFLag8cBtye7As4EHg0OeVu4Mjkc/9kn+R47+T8Mjlwm5mRGVWS658c3ABcwJpJB1sBX0VEUbI/F2iXfG4HzAFIji9Jzi+TA7eZGVAcJTlvkgokTc3aClbVI+lwYGFEvJGvtnpUiZkZlXtzMiJGA6PLOLwPcISkfkADoCnwV6C5pDpJr7o9UJicXwh0AOZKqgM0AxaVd3/3uM3MqLocd0QMj4j2EdERGAg8HxG/Av4DHJOcNhj4V/J5bLJPcvz5qOC3iAO3mRlVnuMuze+AcyXNIpPDviMpvwNolZSfC1xYUUVOlZiZASV5eHMyIiYCE5PPs4E9SznnO+AXlanXgdvMDM9VYmaWOsWRnuWCHbjNzMhPqiRfHLjNzHCqxMwsddzjNjNLGfe4zcxSpjiKq7sJOXPgNjPDiwWbmaVOmlbAceA2M8M9bjOz1PGoEjOzlPGoEjOzlPEr72ZmKeMct5lZyjjHbWaWMmnqcXsFHDMzqm7pMkkNJL0u6S1J70q6PCn/u6SPJE1Ptm5JuSSNkjRL0tuSulfUVve4zcyo0h7398CBEbFMUl1gkqR/J8fOj4hH1zm/L9A52XoCNyc/y+TAbWZG1Y0qSRb6XZbs1k228n4r9AfuSa57TVJzSW0jYl5ZFzhVYmZG5uFkrpukAklTs7aC7Lok1ZY0HVgIjI+IycmhkUk65HpJ9ZOydsCcrMvnJmVlco/bzIzKpUoiYjQwupzjxUA3Sc2BMZJ+DAwH5gP1kmt/B4zYkLa6x21mRubNyVz/5FxnxFfAf4BDI2JeZHwP3MWaFd8LgQ5Zl7VPysrkwG1mRqbHnetWHklbJT1tJDUEDgbel9Q2KRNwJPBOcslY4IRkdEkvYEl5+W1wqsTMDKjSF3DaAndLqk2mc/xwRDwp6XlJWwECpgOnJuc/DfQDZgHfAkMquoHSNOj8h0pSQZJTM1vNfy9+uJwqSYeCik+xHyD/vfiBcuA2M0sZB24zs5Rx4E4H5zGtNP578QPlh5NmZinjHreZWco4cJuZpYwDdw0n6VBJM5O5ei+s7vZY9ZN0p6SFkt6p+GzbHDlw12DJm1c3kpmvdxdgkKRdqrdVVgP8HTi0uhth1ceBu2bbE5gVEbMjYgXwIJm5e+0HLCJeBL6s7nZY9XHgrtkqPU+vmW3+HLjNzFLGgbtmq/Q8vWa2+XPgrtmmAJ0ldZJUDxhIZu5eM/sBc+CuwSKiCDgdeBZ4j8y8vu9Wb6usukl6AHgV6CJprqSh1d0m27T8yruZWcq4x21mljIO3GZmKePAbWaWMg7cZmYp48BtZpYyDtyWF5KKJU2X9I6kRyRtsRF1/V3SMcnn28ubaEvS/pL23oB7fCxpyw1to9mm5MBt+bI8IrpFxI+BFcCp2Qcl1dmQSiPi5IiYUc4p+wOVDtxmaeLAbZvCS8AOSW/4JUljgRmSakv6s6Qpkt6WdAqAMv6WzEP+HNB6VdMcKuMAAAHLSURBVEWSJkrqkXw+VNI0SW9JmiCpI5lfEOckvf2fSdpK0mPJPaZI2ie5tpWkcZLelXQ7oE37P4nZhtugXo9ZrpKedV/gmaSoO/DjiPhIUgGwJCL2kFQfeFnSOOCnQBcyc5C3AWYAd65T71bAbcB+SV0tI+JLSbcAyyLi2uS8+4HrI2KSpB+ReQt1Z+BSYFJEjJB0GOC3Dy01HLgtXxpKmp58fgm4g0wK4/WI+Cgp7wP8ZFX+GmgGdAb2Ax6IiGLgM0nPl1J/L+DFVXVFRFnzUx8E7CKt7lA3ldQ4ucdRybVPSVq8gd/TbJNz4LZ8WR4R3bILkuD5TXYRcEZEPLvOef2qsB21gF4R8V0pbTFLJee4rTo9C/yvpLoAknaU1Ah4EfhlkgNvCxxQyrWvAftJ6pRc2zIp/xpoknXeOOCMVTuSVv0yeRE4LinrC7Sosm9llmcO3FadbieTv56WLHx7K5l/BY4BPkiO3UNmJry1RMTnQAHwuKS3gIeSQ08AA1Y9nATOBHokDz9nsGZ0y+VkAv+7ZFImn+bpO5pVOc8OaGaWMu5xm5mljAO3mVnKOHCbmaWMA7eZWco4cJuZpYwDt5lZyjhwm5mlzP8B9p+8GSzl5lAAAAAASUVORK5CYII=\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["print('Accuracy of LR model', accuracy_score(y_test, LR_pred))"],"metadata":{"id":"P5jiA7T6JbUk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662458582458,"user_tz":-60,"elapsed":360,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"}},"outputId":"fabc489c-e381-4ac3-b390-481e9f710345"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy of LR model 0.6636891469405984\n"]}]},{"cell_type":"code","source":["# Accuracy and classification report \n","target_names = ['true_y', 'predicted_y']\n","\n","print(classification_report(y_test, LR_pred, target_names=target_names))"],"metadata":{"id":"wBd-VVXCJdQt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662458600275,"user_tz":-60,"elapsed":266,"user":{"displayName":"Berkem Billuroglu","userId":"04977643115501908037"}},"outputId":"d044a65b-b47f-4e51-e882-a2b8ba9b372b"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","      true_y       0.66      0.70      0.68      1126\n"," predicted_y       0.67      0.63      0.65      1113\n","\n","    accuracy                           0.66      2239\n","   macro avg       0.66      0.66      0.66      2239\n","weighted avg       0.66      0.66      0.66      2239\n","\n"]}]}]}